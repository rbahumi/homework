{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behavioral Cloning\n",
    "[Assignment PDF](http://rail.eecs.berkeley.edu/deeprlcourse/static/homeworks/hw1.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Set matplotlib environment\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('bmh')\n",
    "%pylab inline\n",
    "\n",
    "# Reload python module by default\n",
    "# https://iqbalnaved.wordpress.com/2013/10/18/ipython-tip-reloading-modified-code/\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def add_to_path(new_paths):\n",
    "    for new_path in paths_to_add:\n",
    "        if new_path not in sys.path:\n",
    "            sys.path.append(new_path)\n",
    "\n",
    "\n",
    "CWD = os.getcwd().replace('/notebooks', '')\n",
    "os.chdir(CWD)\n",
    "paths_to_add = [CWD]\n",
    "add_to_path(paths_to_add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set Up environment with OpenAi Roboschool instead of MuJoCo\n",
    "In my fork to this homework repository I have added instruction of how to set up the environment with virtualenv or with a prebuild docker image. First setup the environment and then you will be able to run this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Behavioral Cloning\n",
    "The idea behind Behavioral Cloning is to apply Supervised Learning methods in order to learn a control policy. This basically means that we are not going to learn a state/action value function. Instead we will record a set of trajectories/rewards ran by an expert policy and try to imitate that continues action vector policy using a regretion model (in this case, an ANN with a regresion head).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from consts import EXPERT_DIR, ROBOSCOOL_AVAILABLE_ENVS, EXPERT_DATA_DIR\n",
    "from run_expert import run_expert_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm -fr {EXPERT_DATA_DIR}\n",
    "!mkdir -p {EXPERT_DATA_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running env RoboschoolAnt-v1\n",
      "Env description: Expert policy for module RoboschoolAnt-v1\n",
      "mean return 1818.8292998516504\n",
      "std of return 381.8678179633842\n",
      "CPU times: user 40 s, sys: 1min 3s, total: 1min 43s\n",
      "Wall time: 1min 44s\n",
      "Running env RoboschoolHumanoid-v1\n",
      "Env description: Expert policy for module RoboschoolHumanoid-v1\n",
      "mean return 2848.910280488764\n",
      "std of return 1033.9963047075728\n",
      "CPU times: user 2min 55s, sys: 7min 5s, total: 10min\n",
      "Wall time: 2min 30s\n",
      "Running env RoboschoolHalfCheetah-v1\n",
      "Env description: Expert policy for module RoboschoolHalfCheetah-v1\n",
      "mean return 2271.29843854231\n",
      "std of return 909.0033810184069\n",
      "CPU times: user 25.2 s, sys: 47.3 s, total: 1min 12s\n",
      "Wall time: 1min 12s\n",
      "Running env RoboschoolReacher-v1\n",
      "Env description: Expert policy for module RoboschoolReacher-v1\n",
      "mean return 18.612237270160552\n",
      "std of return 10.257759621234174\n",
      "CPU times: user 1.49 s, sys: 3.15 s, total: 4.64 s\n",
      "Wall time: 4.67 s\n",
      "Running env RoboschoolHopper-v1\n",
      "Env description: Expert policy for module RoboschoolHopper-v1\n",
      "mean return 1541.857022448628\n",
      "std of return 687.0205519391368\n",
      "CPU times: user 17.1 s, sys: 33.7 s, total: 50.8 s\n",
      "Wall time: 50.9 s\n",
      "Running env RoboschoolWalker2d-v1\n",
      "Env description: Expert policy for module RoboschoolWalker2d-v1\n",
      "mean return 2205.412557862982\n",
      "std of return 81.9400649404929\n",
      "CPU times: user 29.8 s, sys: 58.3 s, total: 1min 28s\n",
      "Wall time: 1min 28s\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "for envname in ROBOSCOOL_AVAILABLE_ENVS:\n",
    "    pickle_filename = os.path.join(EXPERT_DATA_DIR, envname + '.pkl')\n",
    "    if os.path.isfile(pickle_filename):\n",
    "        print (\"env %s dataset already exists\" % envname)\n",
    "        continue;\n",
    "                                   \n",
    "    print(\"Running env %s\" % envname)\n",
    "    %time expert_data = run_expert_policy(num_rollouts=50, envname=envname, verbose=False)\n",
    "    with open(pickle_filename, 'wb') as f:\n",
    "        pickle.dump(expert_data, f, pickle.HIGHEST_PROTOCOL) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper modules/functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## manage_datasets.py: Helper functions for loading the specific domain tasks Datasets\n",
    "For example, load the train/test datasets with their labels, and show their input and output vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset RoboschoolAnt-v1\n",
      "Domain name: RoboschoolAnt-v1\n",
      "(45000, 28) (5000, 28) (45000, 8) (5000, 8)\n"
     ]
    }
   ],
   "source": [
    "from manage_datasets import get_datasets\n",
    "from consts import EXPERT_DATA_DIR\n",
    "\n",
    "\n",
    "dataset_name = 'RoboschoolAnt-v1'\n",
    "X_train, X_test, y_train, y_test = get_datasets(dataset_name=dataset_name, dataset_dir=EXPERT_DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.12892047, -0.02399238,  0.99971217,  0.35790426,  0.30509418,\n",
       "        0.058642  ,  0.1139003 , -0.11415579,  0.43144047, -0.75470674,\n",
       "        1.0087281 , -0.03240158,  0.02628127,  0.6814454 , -0.2858721 ,\n",
       "        0.01521853, -0.68756235,  0.23606583,  0.5452577 ,  0.26748422,\n",
       "        0.42372003, -0.5583233 ,  0.58613735,  0.07699564,  0.        ,\n",
       "        1.        ,  1.        ,  1.        ], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.77629344, -0.11997891, -0.30649464, -1.7593687 ,  0.69761764,\n",
       "        0.70613533, -0.05471253,  0.07859467])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'RoboschoolAnt-v1' domain has 28 input features (floats), and the actions vector (what we are trying to predict) have 8 features (floats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## keras_helpers/model_helper.py: Helper functions for defining a Fully Connected (FC) new Keras model.\n",
    "For example lets create a Fully connected network that has the input/output dimensions suitable for 'RoboschoolAnt-v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "from keras_helpers.model_helper import create_model\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "# Define model\n",
    "config_dict = dict(\n",
    "    input_dim=len(X_train[1, :]),\n",
    "    output_dim=len(y_train[1, :]),\n",
    "    units=100,\n",
    "    layers = 3,\n",
    "    l2_reg = 1e-04,\n",
    "    lr = 1e-03,\n",
    "    dropout=None,\n",
    "    use_batchnorm=False)\n",
    "\n",
    "model = create_model(**config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"387pt\" viewBox=\"0.00 0.00 311.00 387.00\" width=\"311pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 383)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-383 307,-383 307,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 140187128308624 -->\n",
       "<g class=\"node\" id=\"node1\"><title>140187128308624</title>\n",
       "<polygon fill=\"none\" points=\"0,-332.5 0,-378.5 303,-378.5 303,-332.5 0,-332.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"71.5\" y=\"-351.8\">input: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"143,-332.5 143,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"177\" y=\"-363.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"143,-355.5 211,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"177\" y=\"-340.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"211,-332.5 211,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"257\" y=\"-363.3\">(None, 28)</text>\n",
       "<polyline fill=\"none\" points=\"211,-355.5 303,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"257\" y=\"-340.3\">(None, 28)</text>\n",
       "</g>\n",
       "<!-- 140187128310472 -->\n",
       "<g class=\"node\" id=\"node2\"><title>140187128310472</title>\n",
       "<polygon fill=\"none\" points=\"3,-249.5 3,-295.5 300,-295.5 300,-249.5 3,-249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"67\" y=\"-268.8\">dense_1: Dense</text>\n",
       "<polyline fill=\"none\" points=\"131,-249.5 131,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"165\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"131,-272.5 199,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"165\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"199,-249.5 199,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"249.5\" y=\"-280.3\">(None, 28)</text>\n",
       "<polyline fill=\"none\" points=\"199,-272.5 300,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"249.5\" y=\"-257.3\">(None, 100)</text>\n",
       "</g>\n",
       "<!-- 140187128308624&#45;&gt;140187128310472 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>140187128308624-&gt;140187128310472</title>\n",
       "<path d=\"M151.5,-332.366C151.5,-324.152 151.5,-314.658 151.5,-305.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"155,-305.607 151.5,-295.607 148,-305.607 155,-305.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140187128310304 -->\n",
       "<g class=\"node\" id=\"node3\"><title>140187128310304</title>\n",
       "<polygon fill=\"none\" points=\"3,-166.5 3,-212.5 300,-212.5 300,-166.5 3,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"67\" y=\"-185.8\">dense_2: Dense</text>\n",
       "<polyline fill=\"none\" points=\"131,-166.5 131,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"165\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"131,-189.5 199,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"165\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"199,-166.5 199,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"249.5\" y=\"-197.3\">(None, 100)</text>\n",
       "<polyline fill=\"none\" points=\"199,-189.5 300,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"249.5\" y=\"-174.3\">(None, 100)</text>\n",
       "</g>\n",
       "<!-- 140187128310472&#45;&gt;140187128310304 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>140187128310472-&gt;140187128310304</title>\n",
       "<path d=\"M151.5,-249.366C151.5,-241.152 151.5,-231.658 151.5,-222.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"155,-222.607 151.5,-212.607 148,-222.607 155,-222.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140187125859160 -->\n",
       "<g class=\"node\" id=\"node4\"><title>140187125859160</title>\n",
       "<polygon fill=\"none\" points=\"3,-83.5 3,-129.5 300,-129.5 300,-83.5 3,-83.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"67\" y=\"-102.8\">dense_3: Dense</text>\n",
       "<polyline fill=\"none\" points=\"131,-83.5 131,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"165\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"131,-106.5 199,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"165\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"199,-83.5 199,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"249.5\" y=\"-114.3\">(None, 100)</text>\n",
       "<polyline fill=\"none\" points=\"199,-106.5 300,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"249.5\" y=\"-91.3\">(None, 100)</text>\n",
       "</g>\n",
       "<!-- 140187128310304&#45;&gt;140187125859160 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>140187128310304-&gt;140187125859160</title>\n",
       "<path d=\"M151.5,-166.366C151.5,-158.152 151.5,-148.658 151.5,-139.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"155,-139.607 151.5,-129.607 148,-139.607 155,-139.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140187125532096 -->\n",
       "<g class=\"node\" id=\"node5\"><title>140187125532096</title>\n",
       "<polygon fill=\"none\" points=\"3,-0.5 3,-46.5 300,-46.5 300,-0.5 3,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"67\" y=\"-19.8\">dense_4: Dense</text>\n",
       "<polyline fill=\"none\" points=\"131,-0.5 131,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"165\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"131,-23.5 199,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"165\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"199,-0.5 199,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"249.5\" y=\"-31.3\">(None, 100)</text>\n",
       "<polyline fill=\"none\" points=\"199,-23.5 300,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"249.5\" y=\"-8.3\">(None, 8)</text>\n",
       "</g>\n",
       "<!-- 140187125859160&#45;&gt;140187125532096 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>140187125859160-&gt;140187125532096</title>\n",
       "<path d=\"M151.5,-83.3664C151.5,-75.1516 151.5,-65.6579 151.5,-56.7252\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"155,-56.6068 151.5,-46.6068 148,-56.6069 155,-56.6068\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plot the model architecture\n",
    "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily add dropout and batchnorm layers to this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"1051pt\" viewBox=\"0.00 0.00 509.00 1051.00\" width=\"509pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 1047)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-1047 505,-1047 505,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 140187124898952 -->\n",
       "<g class=\"node\" id=\"node1\"><title>140187124898952</title>\n",
       "<polygon fill=\"none\" points=\"99,-996.5 99,-1042.5 402,-1042.5 402,-996.5 99,-996.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"170.5\" y=\"-1015.8\">input: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"242,-996.5 242,-1042.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"276\" y=\"-1027.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"242,-1019.5 310,-1019.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"276\" y=\"-1004.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"310,-996.5 310,-1042.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"356\" y=\"-1027.3\">(None, 28)</text>\n",
       "<polyline fill=\"none\" points=\"310,-1019.5 402,-1019.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"356\" y=\"-1004.3\">(None, 28)</text>\n",
       "</g>\n",
       "<!-- 140187125100616 -->\n",
       "<g class=\"node\" id=\"node2\"><title>140187125100616</title>\n",
       "<polygon fill=\"none\" points=\"4.5,-913.5 4.5,-959.5 496.5,-959.5 496.5,-913.5 4.5,-913.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"170.5\" y=\"-932.8\">batch_normalization_1: BatchNormalization</text>\n",
       "<polyline fill=\"none\" points=\"336.5,-913.5 336.5,-959.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"370.5\" y=\"-944.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"336.5,-936.5 404.5,-936.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"370.5\" y=\"-921.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"404.5,-913.5 404.5,-959.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"450.5\" y=\"-944.3\">(None, 28)</text>\n",
       "<polyline fill=\"none\" points=\"404.5,-936.5 496.5,-936.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"450.5\" y=\"-921.3\">(None, 28)</text>\n",
       "</g>\n",
       "<!-- 140187124898952&#45;&gt;140187125100616 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>140187124898952-&gt;140187125100616</title>\n",
       "<path d=\"M250.5,-996.366C250.5,-988.152 250.5,-978.658 250.5,-969.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"254,-969.607 250.5,-959.607 247,-969.607 254,-969.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140187135330344 -->\n",
       "<g class=\"node\" id=\"node3\"><title>140187135330344</title>\n",
       "<polygon fill=\"none\" points=\"92,-830.5 92,-876.5 409,-876.5 409,-830.5 92,-830.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"170.5\" y=\"-849.8\">dropout_1: Dropout</text>\n",
       "<polyline fill=\"none\" points=\"249,-830.5 249,-876.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"283\" y=\"-861.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"249,-853.5 317,-853.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"283\" y=\"-838.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"317,-830.5 317,-876.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"363\" y=\"-861.3\">(None, 28)</text>\n",
       "<polyline fill=\"none\" points=\"317,-853.5 409,-853.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"363\" y=\"-838.3\">(None, 28)</text>\n",
       "</g>\n",
       "<!-- 140187125100616&#45;&gt;140187135330344 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>140187125100616-&gt;140187135330344</title>\n",
       "<path d=\"M250.5,-913.366C250.5,-905.152 250.5,-895.658 250.5,-886.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"254,-886.607 250.5,-876.607 247,-886.607 254,-886.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140187124896992 -->\n",
       "<g class=\"node\" id=\"node4\"><title>140187124896992</title>\n",
       "<polygon fill=\"none\" points=\"102,-747.5 102,-793.5 399,-793.5 399,-747.5 102,-747.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"166\" y=\"-766.8\">dense_5: Dense</text>\n",
       "<polyline fill=\"none\" points=\"230,-747.5 230,-793.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"264\" y=\"-778.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"230,-770.5 298,-770.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"264\" y=\"-755.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"298,-747.5 298,-793.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"348.5\" y=\"-778.3\">(None, 28)</text>\n",
       "<polyline fill=\"none\" points=\"298,-770.5 399,-770.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"348.5\" y=\"-755.3\">(None, 100)</text>\n",
       "</g>\n",
       "<!-- 140187135330344&#45;&gt;140187124896992 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>140187135330344-&gt;140187124896992</title>\n",
       "<path d=\"M250.5,-830.366C250.5,-822.152 250.5,-812.658 250.5,-803.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"254,-803.607 250.5,-793.607 247,-803.607 254,-803.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140187124213352 -->\n",
       "<g class=\"node\" id=\"node5\"><title>140187124213352</title>\n",
       "<polygon fill=\"none\" points=\"0,-664.5 0,-710.5 501,-710.5 501,-664.5 0,-664.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"166\" y=\"-683.8\">batch_normalization_2: BatchNormalization</text>\n",
       "<polyline fill=\"none\" points=\"332,-664.5 332,-710.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"366\" y=\"-695.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"332,-687.5 400,-687.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"366\" y=\"-672.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"400,-664.5 400,-710.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"450.5\" y=\"-695.3\">(None, 100)</text>\n",
       "<polyline fill=\"none\" points=\"400,-687.5 501,-687.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"450.5\" y=\"-672.3\">(None, 100)</text>\n",
       "</g>\n",
       "<!-- 140187124896992&#45;&gt;140187124213352 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>140187124896992-&gt;140187124213352</title>\n",
       "<path d=\"M250.5,-747.366C250.5,-739.152 250.5,-729.658 250.5,-720.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"254,-720.607 250.5,-710.607 247,-720.607 254,-720.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140187124212232 -->\n",
       "<g class=\"node\" id=\"node6\"><title>140187124212232</title>\n",
       "<polygon fill=\"none\" points=\"87.5,-581.5 87.5,-627.5 413.5,-627.5 413.5,-581.5 87.5,-581.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"166\" y=\"-600.8\">dropout_2: Dropout</text>\n",
       "<polyline fill=\"none\" points=\"244.5,-581.5 244.5,-627.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"278.5\" y=\"-612.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"244.5,-604.5 312.5,-604.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"278.5\" y=\"-589.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"312.5,-581.5 312.5,-627.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"363\" y=\"-612.3\">(None, 100)</text>\n",
       "<polyline fill=\"none\" points=\"312.5,-604.5 413.5,-604.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"363\" y=\"-589.3\">(None, 100)</text>\n",
       "</g>\n",
       "<!-- 140187124213352&#45;&gt;140187124212232 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>140187124213352-&gt;140187124212232</title>\n",
       "<path d=\"M250.5,-664.366C250.5,-656.152 250.5,-646.658 250.5,-637.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"254,-637.607 250.5,-627.607 247,-637.607 254,-637.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140187125028456 -->\n",
       "<g class=\"node\" id=\"node7\"><title>140187125028456</title>\n",
       "<polygon fill=\"none\" points=\"102,-498.5 102,-544.5 399,-544.5 399,-498.5 102,-498.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"166\" y=\"-517.8\">dense_6: Dense</text>\n",
       "<polyline fill=\"none\" points=\"230,-498.5 230,-544.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"264\" y=\"-529.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"230,-521.5 298,-521.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"264\" y=\"-506.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"298,-498.5 298,-544.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"348.5\" y=\"-529.3\">(None, 100)</text>\n",
       "<polyline fill=\"none\" points=\"298,-521.5 399,-521.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"348.5\" y=\"-506.3\">(None, 100)</text>\n",
       "</g>\n",
       "<!-- 140187124212232&#45;&gt;140187125028456 -->\n",
       "<g class=\"edge\" id=\"edge6\"><title>140187124212232-&gt;140187125028456</title>\n",
       "<path d=\"M250.5,-581.366C250.5,-573.152 250.5,-563.658 250.5,-554.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"254,-554.607 250.5,-544.607 247,-554.607 254,-554.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140187125101400 -->\n",
       "<g class=\"node\" id=\"node8\"><title>140187125101400</title>\n",
       "<polygon fill=\"none\" points=\"0,-415.5 0,-461.5 501,-461.5 501,-415.5 0,-415.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"166\" y=\"-434.8\">batch_normalization_3: BatchNormalization</text>\n",
       "<polyline fill=\"none\" points=\"332,-415.5 332,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"366\" y=\"-446.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"332,-438.5 400,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"366\" y=\"-423.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"400,-415.5 400,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"450.5\" y=\"-446.3\">(None, 100)</text>\n",
       "<polyline fill=\"none\" points=\"400,-438.5 501,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"450.5\" y=\"-423.3\">(None, 100)</text>\n",
       "</g>\n",
       "<!-- 140187125028456&#45;&gt;140187125101400 -->\n",
       "<g class=\"edge\" id=\"edge7\"><title>140187125028456-&gt;140187125101400</title>\n",
       "<path d=\"M250.5,-498.366C250.5,-490.152 250.5,-480.658 250.5,-471.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"254,-471.607 250.5,-461.607 247,-471.607 254,-471.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140187125103192 -->\n",
       "<g class=\"node\" id=\"node9\"><title>140187125103192</title>\n",
       "<polygon fill=\"none\" points=\"87.5,-332.5 87.5,-378.5 413.5,-378.5 413.5,-332.5 87.5,-332.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"166\" y=\"-351.8\">dropout_3: Dropout</text>\n",
       "<polyline fill=\"none\" points=\"244.5,-332.5 244.5,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"278.5\" y=\"-363.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"244.5,-355.5 312.5,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"278.5\" y=\"-340.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"312.5,-332.5 312.5,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"363\" y=\"-363.3\">(None, 100)</text>\n",
       "<polyline fill=\"none\" points=\"312.5,-355.5 413.5,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"363\" y=\"-340.3\">(None, 100)</text>\n",
       "</g>\n",
       "<!-- 140187125101400&#45;&gt;140187125103192 -->\n",
       "<g class=\"edge\" id=\"edge8\"><title>140187125101400-&gt;140187125103192</title>\n",
       "<path d=\"M250.5,-415.366C250.5,-407.152 250.5,-397.658 250.5,-388.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"254,-388.607 250.5,-378.607 247,-388.607 254,-388.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140187125111552 -->\n",
       "<g class=\"node\" id=\"node10\"><title>140187125111552</title>\n",
       "<polygon fill=\"none\" points=\"102,-249.5 102,-295.5 399,-295.5 399,-249.5 102,-249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"166\" y=\"-268.8\">dense_7: Dense</text>\n",
       "<polyline fill=\"none\" points=\"230,-249.5 230,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"264\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"230,-272.5 298,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"264\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"298,-249.5 298,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"348.5\" y=\"-280.3\">(None, 100)</text>\n",
       "<polyline fill=\"none\" points=\"298,-272.5 399,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"348.5\" y=\"-257.3\">(None, 100)</text>\n",
       "</g>\n",
       "<!-- 140187125103192&#45;&gt;140187125111552 -->\n",
       "<g class=\"edge\" id=\"edge9\"><title>140187125103192-&gt;140187125111552</title>\n",
       "<path d=\"M250.5,-332.366C250.5,-324.152 250.5,-314.658 250.5,-305.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"254,-305.607 250.5,-295.607 247,-305.607 254,-305.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140187125160312 -->\n",
       "<g class=\"node\" id=\"node11\"><title>140187125160312</title>\n",
       "<polygon fill=\"none\" points=\"0,-166.5 0,-212.5 501,-212.5 501,-166.5 0,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"166\" y=\"-185.8\">batch_normalization_4: BatchNormalization</text>\n",
       "<polyline fill=\"none\" points=\"332,-166.5 332,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"366\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"332,-189.5 400,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"366\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"400,-166.5 400,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"450.5\" y=\"-197.3\">(None, 100)</text>\n",
       "<polyline fill=\"none\" points=\"400,-189.5 501,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"450.5\" y=\"-174.3\">(None, 100)</text>\n",
       "</g>\n",
       "<!-- 140187125111552&#45;&gt;140187125160312 -->\n",
       "<g class=\"edge\" id=\"edge10\"><title>140187125111552-&gt;140187125160312</title>\n",
       "<path d=\"M250.5,-249.366C250.5,-241.152 250.5,-231.658 250.5,-222.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"254,-222.607 250.5,-212.607 247,-222.607 254,-222.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140187121068576 -->\n",
       "<g class=\"node\" id=\"node12\"><title>140187121068576</title>\n",
       "<polygon fill=\"none\" points=\"87.5,-83.5 87.5,-129.5 413.5,-129.5 413.5,-83.5 87.5,-83.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"166\" y=\"-102.8\">dropout_4: Dropout</text>\n",
       "<polyline fill=\"none\" points=\"244.5,-83.5 244.5,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"278.5\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"244.5,-106.5 312.5,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"278.5\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"312.5,-83.5 312.5,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"363\" y=\"-114.3\">(None, 100)</text>\n",
       "<polyline fill=\"none\" points=\"312.5,-106.5 413.5,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"363\" y=\"-91.3\">(None, 100)</text>\n",
       "</g>\n",
       "<!-- 140187125160312&#45;&gt;140187121068576 -->\n",
       "<g class=\"edge\" id=\"edge11\"><title>140187125160312-&gt;140187121068576</title>\n",
       "<path d=\"M250.5,-166.366C250.5,-158.152 250.5,-148.658 250.5,-139.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"254,-139.607 250.5,-129.607 247,-139.607 254,-139.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140187120828312 -->\n",
       "<g class=\"node\" id=\"node13\"><title>140187120828312</title>\n",
       "<polygon fill=\"none\" points=\"102,-0.5 102,-46.5 399,-46.5 399,-0.5 102,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"166\" y=\"-19.8\">dense_8: Dense</text>\n",
       "<polyline fill=\"none\" points=\"230,-0.5 230,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"264\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"230,-23.5 298,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"264\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"298,-0.5 298,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"348.5\" y=\"-31.3\">(None, 100)</text>\n",
       "<polyline fill=\"none\" points=\"298,-23.5 399,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"348.5\" y=\"-8.3\">(None, 8)</text>\n",
       "</g>\n",
       "<!-- 140187121068576&#45;&gt;140187120828312 -->\n",
       "<g class=\"edge\" id=\"edge12\"><title>140187121068576-&gt;140187120828312</title>\n",
       "<path d=\"M250.5,-83.3664C250.5,-75.1516 250.5,-65.6579 250.5,-56.7252\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"254,-56.6068 250.5,-46.6068 247,-56.6069 254,-56.6068\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define model\n",
    "config_dict = dict(\n",
    "    input_dim=len(X_train[1, :]),\n",
    "    output_dim=len(y_train[1, :]),\n",
    "    units=100,\n",
    "    layers = 3,\n",
    "    l2_reg = 1e-04,\n",
    "    lr = 1e-03,\n",
    "    dropout=0.1,\n",
    "    use_batchnorm=True)\n",
    "\n",
    "model = create_model(**config_dict)\n",
    "# Plot the model architecture\n",
    "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a FC ANN for each domain\n",
    "For each domain, we will train an ANN with the same network architecture. I did not perform hyperparameters tunning because it is not the main scope of the task. However, you can try resizing the network or add  regularization/dropout/batchnorm layers in order to improve the loss of each of the networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a FC ANN for env RoboschoolAnt-v1\n",
      "loading dataset RoboschoolAnt-v1\n",
      "Domain name: RoboschoolAnt-v1\n",
      "(45000, 28) (5000, 28) (45000, 8) (5000, 8)\n",
      "model_name='model_RoboschoolAnt-v1_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm'\n",
      "model_path='hw1/models/model_RoboschoolAnt-v1_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm'\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/100\n",
      "45000/45000 [==============================] - 1s 29us/step - loss: 0.3793 - mean_squared_error: 0.3518 - val_loss: 0.2265 - val_mean_squared_error: 0.1967\n",
      "Epoch 2/100\n",
      "45000/45000 [==============================] - 1s 26us/step - loss: 0.1996 - mean_squared_error: 0.1687 - val_loss: 0.1824 - val_mean_squared_error: 0.1508\n",
      "Epoch 3/100\n",
      "45000/45000 [==============================] - 1s 30us/step - loss: 0.1665 - mean_squared_error: 0.1345 - val_loss: 0.1583 - val_mean_squared_error: 0.1261\n",
      "Epoch 4/100\n",
      "45000/45000 [==============================] - 1s 27us/step - loss: 0.1495 - mean_squared_error: 0.1172 - val_loss: 0.1451 - val_mean_squared_error: 0.1127\n",
      "Epoch 5/100\n",
      "45000/45000 [==============================] - 1s 29us/step - loss: 0.1389 - mean_squared_error: 0.1066 - val_loss: 0.1355 - val_mean_squared_error: 0.1034\n",
      "Epoch 6/100\n",
      "45000/45000 [==============================] - 1s 28us/step - loss: 0.1315 - mean_squared_error: 0.0995 - val_loss: 0.1303 - val_mean_squared_error: 0.0985\n",
      "Epoch 7/100\n",
      "45000/45000 [==============================] - 1s 27us/step - loss: 0.1245 - mean_squared_error: 0.0929 - val_loss: 0.1354 - val_mean_squared_error: 0.1040\n",
      "Epoch 8/100\n",
      "45000/45000 [==============================] - 1s 24us/step - loss: 0.1199 - mean_squared_error: 0.0887 - val_loss: 0.1190 - val_mean_squared_error: 0.0881\n",
      "Epoch 9/100\n",
      "45000/45000 [==============================] - 1s 28us/step - loss: 0.1165 - mean_squared_error: 0.0858 - val_loss: 0.1165 - val_mean_squared_error: 0.0860\n",
      "Epoch 10/100\n",
      "45000/45000 [==============================] - 1s 27us/step - loss: 0.1125 - mean_squared_error: 0.0822 - val_loss: 0.1137 - val_mean_squared_error: 0.0836\n",
      "Epoch 11/100\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.1093 - mean_squared_error: 0.0794 - val_loss: 0.1127 - val_mean_squared_error: 0.0830\n",
      "Epoch 12/100\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.1065 - mean_squared_error: 0.0770 - val_loss: 0.1109 - val_mean_squared_error: 0.0816\n",
      "Epoch 13/100\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.1040 - mean_squared_error: 0.0749 - val_loss: 0.1072 - val_mean_squared_error: 0.0783\n",
      "Epoch 14/100\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.1022 - mean_squared_error: 0.0734 - val_loss: 0.1019 - val_mean_squared_error: 0.0734\n",
      "Epoch 15/100\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.0999 - mean_squared_error: 0.0715 - val_loss: 0.1037 - val_mean_squared_error: 0.0755\n",
      "Epoch 16/100\n",
      "45000/45000 [==============================] - 1s 26us/step - loss: 0.0985 - mean_squared_error: 0.0704 - val_loss: 0.1065 - val_mean_squared_error: 0.0786\n",
      "Epoch 17/100\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.0966 - mean_squared_error: 0.0688 - val_loss: 0.1012 - val_mean_squared_error: 0.0736\n",
      "Epoch 18/100\n",
      "45000/45000 [==============================] - 1s 26us/step - loss: 0.0948 - mean_squared_error: 0.0673 - val_loss: 0.1028 - val_mean_squared_error: 0.0755 0s - loss: 0.0948 - mean_squared_error\n",
      "Epoch 19/100\n",
      "45000/45000 [==============================] - 1s 26us/step - loss: 0.0934 - mean_squared_error: 0.0662 - val_loss: 0.0978 - val_mean_squared_error: 0.0708\n",
      "Epoch 20/100\n",
      "45000/45000 [==============================] - 1s 31us/step - loss: 0.0924 - mean_squared_error: 0.0655 - val_loss: 0.0956 - val_mean_squared_error: 0.0689\n",
      "Epoch 21/100\n",
      "45000/45000 [==============================] - 1s 33us/step - loss: 0.0909 - mean_squared_error: 0.0642 - val_loss: 0.1032 - val_mean_squared_error: 0.0767\n",
      "Epoch 22/100\n",
      "45000/45000 [==============================] - 1s 28us/step - loss: 0.0902 - mean_squared_error: 0.0637 - val_loss: 0.0955 - val_mean_squared_error: 0.0693s - loss: 0.0902 - mean_squared_err\n",
      "Epoch 23/100\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.0890 - mean_squared_error: 0.0628 - val_loss: 0.0922 - val_mean_squared_error: 0.0661\n",
      "Epoch 24/100\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.0883 - mean_squared_error: 0.0623 - val_loss: 0.0924 - val_mean_squared_error: 0.0666\n",
      "Epoch 25/100\n",
      "45000/45000 [==============================] - 1s 24us/step - loss: 0.0870 - mean_squared_error: 0.0612 - val_loss: 0.0962 - val_mean_squared_error: 0.0706\n",
      "Epoch 26/100\n",
      "45000/45000 [==============================] - 1s 23us/step - loss: 0.0860 - mean_squared_error: 0.0605 - val_loss: 0.0882 - val_mean_squared_error: 0.0628\n",
      "Epoch 27/100\n",
      "45000/45000 [==============================] - 1s 23us/step - loss: 0.0853 - mean_squared_error: 0.0599 - val_loss: 0.0896 - val_mean_squared_error: 0.0644\n",
      "Epoch 28/100\n",
      "45000/45000 [==============================] - 1s 23us/step - loss: 0.0848 - mean_squared_error: 0.0597 - val_loss: 0.0864 - val_mean_squared_error: 0.0613\n",
      "Epoch 29/100\n",
      "45000/45000 [==============================] - 1s 23us/step - loss: 0.0837 - mean_squared_error: 0.0588 - val_loss: 0.0894 - val_mean_squared_error: 0.0645\n",
      "Epoch 30/100\n",
      "45000/45000 [==============================] - 1s 23us/step - loss: 0.0829 - mean_squared_error: 0.0581 - val_loss: 0.0877 - val_mean_squared_error: 0.0630loss: 0.0831 - mean_squared\n",
      "Epoch 31/100\n",
      "45000/45000 [==============================] - 1s 23us/step - loss: 0.0819 - mean_squared_error: 0.0573 - val_loss: 0.0905 - val_mean_squared_error: 0.0660\n",
      "Epoch 32/100\n",
      "45000/45000 [==============================] - 1s 24us/step - loss: 0.0819 - mean_squared_error: 0.0575 - val_loss: 0.0892 - val_mean_squared_error: 0.0648\n",
      "Epoch 33/100\n",
      "45000/45000 [==============================] - 1s 27us/step - loss: 0.0810 - mean_squared_error: 0.0567 - val_loss: 0.0861 - val_mean_squared_error: 0.0619\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 34/100\n",
      "45000/45000 [==============================] - 1s 24us/step - loss: 0.0748 - mean_squared_error: 0.0507 - val_loss: 0.0785 - val_mean_squared_error: 0.0544\n",
      "Epoch 35/100\n",
      "45000/45000 [==============================] - 1s 23us/step - loss: 0.0744 - mean_squared_error: 0.0503 - val_loss: 0.0799 - val_mean_squared_error: 0.0559\n",
      "Epoch 36/100\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.0740 - mean_squared_error: 0.0501 - val_loss: 0.0795 - val_mean_squared_error: 0.0557\n",
      "Epoch 37/100\n",
      "45000/45000 [==============================] - 1s 26us/step - loss: 0.0738 - mean_squared_error: 0.0500 - val_loss: 0.0775 - val_mean_squared_error: 0.0537\n",
      "Epoch 38/100\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.0734 - mean_squared_error: 0.0497 - val_loss: 0.0776 - val_mean_squared_error: 0.0540\n",
      "Epoch 39/100\n",
      "45000/45000 [==============================] - ETA: 0s - loss: 0.0728 - mean_squared_error: 0.04 - 1s 24us/step - loss: 0.0728 - mean_squared_error: 0.0493 - val_loss: 0.0789 - val_mean_squared_error: 0.0554\n",
      "Epoch 40/100\n",
      "45000/45000 [==============================] - 1s 26us/step - loss: 0.0727 - mean_squared_error: 0.0493 - val_loss: 0.0773 - val_mean_squared_error: 0.0539\n",
      "Epoch 41/100\n",
      "45000/45000 [==============================] - 1s 23us/step - loss: 0.0724 - mean_squared_error: 0.0490 - val_loss: 0.0775 - val_mean_squared_error: 0.0542\n",
      "Epoch 42/100\n",
      "45000/45000 [==============================] - 1s 23us/step - loss: 0.0718 - mean_squared_error: 0.0486 - val_loss: 0.0760 - val_mean_squared_error: 0.0529\n",
      "Epoch 43/100\n",
      "45000/45000 [==============================] - 1s 23us/step - loss: 0.0714 - mean_squared_error: 0.0483 - val_loss: 0.0797 - val_mean_squared_error: 0.0567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/100\n",
      "45000/45000 [==============================] - 1s 23us/step - loss: 0.0713 - mean_squared_error: 0.0483 - val_loss: 0.0755 - val_mean_squared_error: 0.0526\n",
      "Epoch 45/100\n",
      "45000/45000 [==============================] - 1s 23us/step - loss: 0.0710 - mean_squared_error: 0.0481 - val_loss: 0.0764 - val_mean_squared_error: 0.0536\n",
      "Epoch 46/100\n",
      "45000/45000 [==============================] - 1s 23us/step - loss: 0.0707 - mean_squared_error: 0.0479 - val_loss: 0.0758 - val_mean_squared_error: 0.0530\n",
      "Epoch 47/100\n",
      "45000/45000 [==============================] - 1s 23us/step - loss: 0.0703 - mean_squared_error: 0.0476 - val_loss: 0.0754 - val_mean_squared_error: 0.0527\n",
      "Epoch 48/100\n",
      "45000/45000 [==============================] - 1s 22us/step - loss: 0.0700 - mean_squared_error: 0.0474 - val_loss: 0.0762 - val_mean_squared_error: 0.0536\n",
      "Epoch 49/100\n",
      "45000/45000 [==============================] - 1s 23us/step - loss: 0.0699 - mean_squared_error: 0.0474 - val_loss: 0.0748 - val_mean_squared_error: 0.0523\n",
      "Epoch 50/100\n",
      "45000/45000 [==============================] - 1s 23us/step - loss: 0.0696 - mean_squared_error: 0.0472 - val_loss: 0.0763 - val_mean_squared_error: 0.0539\n",
      "Epoch 51/100\n",
      "45000/45000 [==============================] - 1s 23us/step - loss: 0.0694 - mean_squared_error: 0.0471 - val_loss: 0.0740 - val_mean_squared_error: 0.0517\n",
      "Epoch 52/100\n",
      "45000/45000 [==============================] - 1s 22us/step - loss: 0.0690 - mean_squared_error: 0.0467 - val_loss: 0.0754 - val_mean_squared_error: 0.0532\n",
      "Epoch 53/100\n",
      "45000/45000 [==============================] - 1s 23us/step - loss: 0.0687 - mean_squared_error: 0.0466 - val_loss: 0.0736 - val_mean_squared_error: 0.0515\n",
      "Epoch 54/100\n",
      "45000/45000 [==============================] - 1s 23us/step - loss: 0.0685 - mean_squared_error: 0.0464 - val_loss: 0.0740 - val_mean_squared_error: 0.0520\n",
      "Epoch 55/100\n",
      "45000/45000 [==============================] - 1s 22us/step - loss: 0.0682 - mean_squared_error: 0.0462 - val_loss: 0.0734 - val_mean_squared_error: 0.0514\n",
      "Epoch 56/100\n",
      "45000/45000 [==============================] - 1s 23us/step - loss: 0.0680 - mean_squared_error: 0.0461 - val_loss: 0.0725 - val_mean_squared_error: 0.0507\n",
      "Epoch 57/100\n",
      "45000/45000 [==============================] - 1s 22us/step - loss: 0.0678 - mean_squared_error: 0.0460 - val_loss: 0.0728 - val_mean_squared_error: 0.0511\n",
      "Epoch 58/100\n",
      "45000/45000 [==============================] - 1s 23us/step - loss: 0.0675 - mean_squared_error: 0.0458 - val_loss: 0.0734 - val_mean_squared_error: 0.0517\n",
      "Epoch 59/100\n",
      "45000/45000 [==============================] - 1s 23us/step - loss: 0.0674 - mean_squared_error: 0.0458 - val_loss: 0.0741 - val_mean_squared_error: 0.0525\n",
      "Epoch 60/100\n",
      "45000/45000 [==============================] - 1s 23us/step - loss: 0.0672 - mean_squared_error: 0.0456 - val_loss: 0.0720 - val_mean_squared_error: 0.0505\n",
      "Epoch 61/100\n",
      "45000/45000 [==============================] - 1s 23us/step - loss: 0.0670 - mean_squared_error: 0.0455 - val_loss: 0.0739 - val_mean_squared_error: 0.0524\n",
      "Epoch 62/100\n",
      "45000/45000 [==============================] - 1s 23us/step - loss: 0.0666 - mean_squared_error: 0.0451 - val_loss: 0.0726 - val_mean_squared_error: 0.0512\n",
      "Epoch 63/100\n",
      "45000/45000 [==============================] - 1s 22us/step - loss: 0.0665 - mean_squared_error: 0.0452 - val_loss: 0.0722 - val_mean_squared_error: 0.0509\n",
      "Epoch 64/100\n",
      "45000/45000 [==============================] - 1s 23us/step - loss: 0.0664 - mean_squared_error: 0.0452 - val_loss: 0.0710 - val_mean_squared_error: 0.0498\n",
      "Epoch 65/100\n",
      "45000/45000 [==============================] - 1s 26us/step - loss: 0.0660 - mean_squared_error: 0.0448 - val_loss: 0.0726 - val_mean_squared_error: 0.0515\n",
      "Epoch 66/100\n",
      "45000/45000 [==============================] - 2s 34us/step - loss: 0.0662 - mean_squared_error: 0.0451 - val_loss: 0.0705 - val_mean_squared_error: 0.0494\n",
      "Epoch 67/100\n",
      "45000/45000 [==============================] - 1s 29us/step - loss: 0.0658 - mean_squared_error: 0.0448 - val_loss: 0.0736 - val_mean_squared_error: 0.0526\n",
      "Epoch 68/100\n",
      "45000/45000 [==============================] - 1s 23us/step - loss: 0.0658 - mean_squared_error: 0.0448 - val_loss: 0.0722 - val_mean_squared_error: 0.0513\n",
      "Epoch 69/100\n",
      "45000/45000 [==============================] - 1s 23us/step - loss: 0.0655 - mean_squared_error: 0.0445 - val_loss: 0.0723 - val_mean_squared_error: 0.0514\n",
      "Epoch 70/100\n",
      "45000/45000 [==============================] - 1s 30us/step - loss: 0.0655 - mean_squared_error: 0.0446 - val_loss: 0.0697 - val_mean_squared_error: 0.0489\n",
      "Epoch 71/100\n",
      "45000/45000 [==============================] - 1s 24us/step - loss: 0.0651 - mean_squared_error: 0.0443 - val_loss: 0.0718 - val_mean_squared_error: 0.0511\n",
      "Epoch 72/100\n",
      "45000/45000 [==============================] - 1s 27us/step - loss: 0.0649 - mean_squared_error: 0.0441 - val_loss: 0.0707 - val_mean_squared_error: 0.0501\n",
      "Epoch 73/100\n",
      "45000/45000 [==============================] - 1s 28us/step - loss: 0.0647 - mean_squared_error: 0.0441 - val_loss: 0.0703 - val_mean_squared_error: 0.0496\n",
      "Epoch 74/100\n",
      "45000/45000 [==============================] - 1s 29us/step - loss: 0.0647 - mean_squared_error: 0.0441 - val_loss: 0.0695 - val_mean_squared_error: 0.0489\n",
      "Epoch 75/100\n",
      "45000/45000 [==============================] - 1s 24us/step - loss: 0.0645 - mean_squared_error: 0.0440 - val_loss: 0.0694 - val_mean_squared_error: 0.0489\n",
      "\n",
      "Epoch 00075: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 76/100\n",
      "45000/45000 [==============================] - 1s 26us/step - loss: 0.0610 - mean_squared_error: 0.0405 - val_loss: 0.0687 - val_mean_squared_error: 0.0483\n",
      "Epoch 77/100\n",
      "45000/45000 [==============================] - 1s 28us/step - loss: 0.0609 - mean_squared_error: 0.0405 - val_loss: 0.0674 - val_mean_squared_error: 0.0470\n",
      "Epoch 78/100\n",
      "45000/45000 [==============================] - 1s 29us/step - loss: 0.0608 - mean_squared_error: 0.0404 - val_loss: 0.0678 - val_mean_squared_error: 0.0474\n",
      "Epoch 79/100\n",
      "45000/45000 [==============================] - 1s 23us/step - loss: 0.0608 - mean_squared_error: 0.0404 - val_loss: 0.0664 - val_mean_squared_error: 0.0461\n",
      "Epoch 80/100\n",
      "45000/45000 [==============================] - 1s 26us/step - loss: 0.0606 - mean_squared_error: 0.0403 - val_loss: 0.0676 - val_mean_squared_error: 0.0473\n",
      "Epoch 81/100\n",
      "45000/45000 [==============================] - 1s 28us/step - loss: 0.0605 - mean_squared_error: 0.0402 - val_loss: 0.0666 - val_mean_squared_error: 0.0463\n",
      "Epoch 82/100\n",
      "45000/45000 [==============================] - 1s 26us/step - loss: 0.0605 - mean_squared_error: 0.0403 - val_loss: 0.0668 - val_mean_squared_error: 0.0466\n",
      "Epoch 83/100\n",
      "45000/45000 [==============================] - 1s 24us/step - loss: 0.0604 - mean_squared_error: 0.0402 - val_loss: 0.0661 - val_mean_squared_error: 0.0459\n",
      "Epoch 84/100\n",
      "45000/45000 [==============================] - 1s 24us/step - loss: 0.0602 - mean_squared_error: 0.0401 - val_loss: 0.0662 - val_mean_squared_error: 0.0460\n",
      "Epoch 85/100\n",
      "45000/45000 [==============================] - 1s 23us/step - loss: 0.0602 - mean_squared_error: 0.0401 - val_loss: 0.0667 - val_mean_squared_error: 0.0466\n",
      "Epoch 86/100\n",
      "45000/45000 [==============================] - 1s 24us/step - loss: 0.0600 - mean_squared_error: 0.0399 - val_loss: 0.0665 - val_mean_squared_error: 0.0464: 0.0600 - mean_squared_error: 0.03 - ETA: 0s - loss: 0.0599 - mean_squar\n",
      "Epoch 87/100\n",
      "45000/45000 [==============================] - 1s 24us/step - loss: 0.0600 - mean_squared_error: 0.0399 - val_loss: 0.0671 - val_mean_squared_error: 0.0471\n",
      "Epoch 88/100\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.0598 - mean_squared_error: 0.0398 - val_loss: 0.0651 - val_mean_squared_error: 0.0452\n",
      "Epoch 89/100\n",
      "45000/45000 [==============================] - 1s 24us/step - loss: 0.0597 - mean_squared_error: 0.0398 - val_loss: 0.0652 - val_mean_squared_error: 0.0452\n",
      "Epoch 90/100\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.0596 - mean_squared_error: 0.0397 - val_loss: 0.0658 - val_mean_squared_error: 0.0459\n",
      "Epoch 91/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.0595 - mean_squared_error: 0.0397 - val_loss: 0.0658 - val_mean_squared_error: 0.0459\n",
      "Epoch 92/100\n",
      "45000/45000 [==============================] - 1s 23us/step - loss: 0.0594 - mean_squared_error: 0.0396 - val_loss: 0.0655 - val_mean_squared_error: 0.0457\n",
      "Epoch 93/100\n",
      "45000/45000 [==============================] - 1s 24us/step - loss: 0.0594 - mean_squared_error: 0.0396 - val_loss: 0.0656 - val_mean_squared_error: 0.0459\n",
      "\n",
      "Epoch 00093: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 94/100\n",
      "45000/45000 [==============================] - 1s 24us/step - loss: 0.0575 - mean_squared_error: 0.0377 - val_loss: 0.0648 - val_mean_squared_error: 0.0450\n",
      "Epoch 95/100\n",
      "45000/45000 [==============================] - 1s 24us/step - loss: 0.0575 - mean_squared_error: 0.0377 - val_loss: 0.0641 - val_mean_squared_error: 0.0444\n",
      "Epoch 96/100\n",
      "45000/45000 [==============================] - 2s 39us/step - loss: 0.0574 - mean_squared_error: 0.0376 - val_loss: 0.0642 - val_mean_squared_error: 0.0445\n",
      "Epoch 97/100\n",
      "45000/45000 [==============================] - 2s 35us/step - loss: 0.0573 - mean_squared_error: 0.0376 - val_loss: 0.0639 - val_mean_squared_error: 0.0442\n",
      "Epoch 98/100\n",
      "45000/45000 [==============================] - 2s 46us/step - loss: 0.0573 - mean_squared_error: 0.0376 - val_loss: 0.0650 - val_mean_squared_error: 0.0453\n",
      "Epoch 99/100\n",
      "45000/45000 [==============================] - 2s 34us/step - loss: 0.0573 - mean_squared_error: 0.0376 - val_loss: 0.0644 - val_mean_squared_error: 0.0447\n",
      "Epoch 100/100\n",
      "45000/45000 [==============================] - 1s 33us/step - loss: 0.0572 - mean_squared_error: 0.0375 - val_loss: 0.0647 - val_mean_squared_error: 0.0450\n",
      "hw1/models/model_RoboschoolAnt-v1_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm/base.hdf5\n",
      "Done training model for RoboschoolAnt-v1\n",
      "{'model_name': 'model_RoboschoolAnt-v1_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm', 'model_path': 'hw1/models/model_RoboschoolAnt-v1_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm', 'test_mse': 0.045005500097009286, 'train_mse': 0.03765979935236048, 'dataset_name': 'RoboschoolAnt-v1'}\n",
      "Training a FC ANN for env RoboschoolHumanoid-v1\n",
      "loading dataset RoboschoolHumanoid-v1\n",
      "Domain name: RoboschoolHumanoid-v1\n",
      "(39654, 44) (4406, 44) (39654, 17) (4406, 17)\n",
      "model_name='model_RoboschoolHumanoid-v1_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm'\n",
      "model_path='hw1/models/model_RoboschoolHumanoid-v1_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm'\n",
      "Train on 39654 samples, validate on 4406 samples\n",
      "Epoch 1/100\n",
      "39654/39654 [==============================] - 1s 29us/step - loss: 0.0866 - mean_squared_error: 0.0681 - val_loss: 0.0569 - val_mean_squared_error: 0.0397\n",
      "Epoch 2/100\n",
      "39654/39654 [==============================] - 1s 24us/step - loss: 0.0528 - mean_squared_error: 0.0362 - val_loss: 0.0475 - val_mean_squared_error: 0.0315\n",
      "Epoch 3/100\n",
      "39654/39654 [==============================] - 1s 25us/step - loss: 0.0467 - mean_squared_error: 0.0312 - val_loss: 0.0437 - val_mean_squared_error: 0.0287\n",
      "Epoch 4/100\n",
      "39654/39654 [==============================] - 1s 25us/step - loss: 0.0432 - mean_squared_error: 0.0287 - val_loss: 0.0408 - val_mean_squared_error: 0.0266\n",
      "Epoch 5/100\n",
      "39654/39654 [==============================] - 1s 27us/step - loss: 0.0408 - mean_squared_error: 0.0269 - val_loss: 0.0383 - val_mean_squared_error: 0.0247\n",
      "Epoch 6/100\n",
      "39654/39654 [==============================] - 1s 30us/step - loss: 0.0391 - mean_squared_error: 0.0257 - val_loss: 0.0378 - val_mean_squared_error: 0.0248\n",
      "Epoch 7/100\n",
      "39654/39654 [==============================] - 1s 25us/step - loss: 0.0375 - mean_squared_error: 0.0247 - val_loss: 0.0374 - val_mean_squared_error: 0.0247\n",
      "Epoch 8/100\n",
      "39654/39654 [==============================] - 1s 25us/step - loss: 0.0362 - mean_squared_error: 0.0237 - val_loss: 0.0356 - val_mean_squared_error: 0.0233\n",
      "Epoch 9/100\n",
      "39654/39654 [==============================] - 1s 25us/step - loss: 0.0352 - mean_squared_error: 0.0230 - val_loss: 0.0340 - val_mean_squared_error: 0.0220\n",
      "Epoch 10/100\n",
      "39654/39654 [==============================] - 1s 25us/step - loss: 0.0343 - mean_squared_error: 0.0224 - val_loss: 0.0329 - val_mean_squared_error: 0.0212\n",
      "Epoch 11/100\n",
      "39654/39654 [==============================] - 1s 25us/step - loss: 0.0336 - mean_squared_error: 0.0220 - val_loss: 0.0333 - val_mean_squared_error: 0.0218\n",
      "Epoch 12/100\n",
      "39654/39654 [==============================] - 1s 24us/step - loss: 0.0329 - mean_squared_error: 0.0215 - val_loss: 0.0323 - val_mean_squared_error: 0.0211\n",
      "Epoch 13/100\n",
      "39654/39654 [==============================] - 1s 25us/step - loss: 0.0323 - mean_squared_error: 0.0211 - val_loss: 0.0317 - val_mean_squared_error: 0.0207\n",
      "Epoch 14/100\n",
      "39654/39654 [==============================] - 1s 24us/step - loss: 0.0317 - mean_squared_error: 0.0208 - val_loss: 0.0307 - val_mean_squared_error: 0.0199\n",
      "Epoch 15/100\n",
      "39654/39654 [==============================] - 1s 25us/step - loss: 0.0311 - mean_squared_error: 0.0203 - val_loss: 0.0319 - val_mean_squared_error: 0.0212\n",
      "Epoch 16/100\n",
      "39654/39654 [==============================] - 1s 25us/step - loss: 0.0308 - mean_squared_error: 0.0202 - val_loss: 0.0304 - val_mean_squared_error: 0.0200\n",
      "Epoch 17/100\n",
      "39654/39654 [==============================] - 1s 25us/step - loss: 0.0303 - mean_squared_error: 0.0199 - val_loss: 0.0293 - val_mean_squared_error: 0.0190\n",
      "Epoch 18/100\n",
      "39654/39654 [==============================] - 1s 25us/step - loss: 0.0300 - mean_squared_error: 0.0198 - val_loss: 0.0295 - val_mean_squared_error: 0.0194\n",
      "Epoch 19/100\n",
      "39654/39654 [==============================] - 1s 26us/step - loss: 0.0296 - mean_squared_error: 0.0195 - val_loss: 0.0294 - val_mean_squared_error: 0.0194\n",
      "Epoch 20/100\n",
      "39654/39654 [==============================] - 1s 25us/step - loss: 0.0293 - mean_squared_error: 0.0194 - val_loss: 0.0285 - val_mean_squared_error: 0.0186\n",
      "Epoch 21/100\n",
      "39654/39654 [==============================] - 1s 26us/step - loss: 0.0290 - mean_squared_error: 0.0191 - val_loss: 0.0280 - val_mean_squared_error: 0.0182\n",
      "Epoch 22/100\n",
      "39654/39654 [==============================] - 1s 27us/step - loss: 0.0286 - mean_squared_error: 0.0189 - val_loss: 0.0283 - val_mean_squared_error: 0.0186\n",
      "Epoch 23/100\n",
      "39654/39654 [==============================] - 1s 28us/step - loss: 0.0284 - mean_squared_error: 0.0188 - val_loss: 0.0283 - val_mean_squared_error: 0.0187\n",
      "Epoch 24/100\n",
      "39654/39654 [==============================] - 1s 28us/step - loss: 0.0282 - mean_squared_error: 0.0186 - val_loss: 0.0283 - val_mean_squared_error: 0.0188\n",
      "Epoch 25/100\n",
      "39654/39654 [==============================] - 1s 24us/step - loss: 0.0279 - mean_squared_error: 0.0185 - val_loss: 0.0278 - val_mean_squared_error: 0.0184\n",
      "Epoch 26/100\n",
      "39654/39654 [==============================] - 1s 24us/step - loss: 0.0278 - mean_squared_error: 0.0185 - val_loss: 0.0274 - val_mean_squared_error: 0.0181\n",
      "Epoch 27/100\n",
      "39654/39654 [==============================] - 1s 25us/step - loss: 0.0275 - mean_squared_error: 0.0183 - val_loss: 0.0267 - val_mean_squared_error: 0.0175\n",
      "Epoch 28/100\n",
      "39654/39654 [==============================] - 1s 23us/step - loss: 0.0273 - mean_squared_error: 0.0181 - val_loss: 0.0269 - val_mean_squared_error: 0.0179\n",
      "Epoch 29/100\n",
      "39654/39654 [==============================] - 1s 24us/step - loss: 0.0271 - mean_squared_error: 0.0180 - val_loss: 0.0264 - val_mean_squared_error: 0.0174\n",
      "Epoch 30/100\n",
      "39654/39654 [==============================] - 1s 23us/step - loss: 0.0269 - mean_squared_error: 0.0179 - val_loss: 0.0275 - val_mean_squared_error: 0.0186\n",
      "Epoch 31/100\n",
      "39654/39654 [==============================] - 1s 23us/step - loss: 0.0267 - mean_squared_error: 0.0177 - val_loss: 0.0270 - val_mean_squared_error: 0.0181\n",
      "Epoch 32/100\n",
      "39654/39654 [==============================] - 1s 24us/step - loss: 0.0266 - mean_squared_error: 0.0177 - val_loss: 0.0276 - val_mean_squared_error: 0.0188\n",
      "Epoch 33/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39654/39654 [==============================] - 1s 25us/step - loss: 0.0264 - mean_squared_error: 0.0176 - val_loss: 0.0259 - val_mean_squared_error: 0.0171\n",
      "Epoch 34/100\n",
      "39654/39654 [==============================] - 1s 24us/step - loss: 0.0263 - mean_squared_error: 0.0175 - val_loss: 0.0261 - val_mean_squared_error: 0.0174\n",
      "Epoch 35/100\n",
      "39654/39654 [==============================] - 1s 23us/step - loss: 0.0260 - mean_squared_error: 0.0173 - val_loss: 0.0264 - val_mean_squared_error: 0.0177\n",
      "Epoch 36/100\n",
      "39654/39654 [==============================] - 1s 23us/step - loss: 0.0260 - mean_squared_error: 0.0173 - val_loss: 0.0266 - val_mean_squared_error: 0.0180\n",
      "Epoch 37/100\n",
      "39654/39654 [==============================] - 1s 23us/step - loss: 0.0258 - mean_squared_error: 0.0172 - val_loss: 0.0258 - val_mean_squared_error: 0.0173\n",
      "Epoch 38/100\n",
      "39654/39654 [==============================] - 1s 23us/step - loss: 0.0257 - mean_squared_error: 0.0172 - val_loss: 0.0258 - val_mean_squared_error: 0.0173\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 39/100\n",
      "39654/39654 [==============================] - 1s 23us/step - loss: 0.0241 - mean_squared_error: 0.0157 - val_loss: 0.0250 - val_mean_squared_error: 0.0166\n",
      "Epoch 40/100\n",
      "39654/39654 [==============================] - 1s 23us/step - loss: 0.0241 - mean_squared_error: 0.0156 - val_loss: 0.0240 - val_mean_squared_error: 0.0156\n",
      "Epoch 41/100\n",
      "39654/39654 [==============================] - 1s 24us/step - loss: 0.0240 - mean_squared_error: 0.0155 - val_loss: 0.0240 - val_mean_squared_error: 0.0156\n",
      "Epoch 42/100\n",
      "39654/39654 [==============================] - 1s 23us/step - loss: 0.0239 - mean_squared_error: 0.0155 - val_loss: 0.0245 - val_mean_squared_error: 0.0161\n",
      "Epoch 43/100\n",
      "39654/39654 [==============================] - 1s 25us/step - loss: 0.0238 - mean_squared_error: 0.0154 - val_loss: 0.0241 - val_mean_squared_error: 0.0158\n",
      "Epoch 44/100\n",
      "39654/39654 [==============================] - 1s 25us/step - loss: 0.0237 - mean_squared_error: 0.0153 - val_loss: 0.0239 - val_mean_squared_error: 0.0156\n",
      "Epoch 45/100\n",
      "39654/39654 [==============================] - 1s 25us/step - loss: 0.0236 - mean_squared_error: 0.0153 - val_loss: 0.0244 - val_mean_squared_error: 0.0161\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 46/100\n",
      "39654/39654 [==============================] - 1s 26us/step - loss: 0.0228 - mean_squared_error: 0.0145 - val_loss: 0.0231 - val_mean_squared_error: 0.0148\n",
      "Epoch 47/100\n",
      "39654/39654 [==============================] - 1s 27us/step - loss: 0.0227 - mean_squared_error: 0.0145 - val_loss: 0.0234 - val_mean_squared_error: 0.0151\n",
      "Epoch 48/100\n",
      "39654/39654 [==============================] - 1s 29us/step - loss: 0.0227 - mean_squared_error: 0.0145 - val_loss: 0.0231 - val_mean_squared_error: 0.0148\n",
      "Epoch 49/100\n",
      "39654/39654 [==============================] - 1s 28us/step - loss: 0.0226 - mean_squared_error: 0.0144 - val_loss: 0.0229 - val_mean_squared_error: 0.0147\n",
      "Epoch 50/100\n",
      "39654/39654 [==============================] - 1s 25us/step - loss: 0.0226 - mean_squared_error: 0.0143 - val_loss: 0.0233 - val_mean_squared_error: 0.0151\n",
      "Epoch 51/100\n",
      "39654/39654 [==============================] - 1s 25us/step - loss: 0.0225 - mean_squared_error: 0.0143 - val_loss: 0.0228 - val_mean_squared_error: 0.0146\n",
      "Epoch 52/100\n",
      "39654/39654 [==============================] - 1s 26us/step - loss: 0.0225 - mean_squared_error: 0.0143 - val_loss: 0.0228 - val_mean_squared_error: 0.0146\n",
      "Epoch 53/100\n",
      "39654/39654 [==============================] - 1s 30us/step - loss: 0.0224 - mean_squared_error: 0.0142 - val_loss: 0.0228 - val_mean_squared_error: 0.0147\n",
      "Epoch 54/100\n",
      "39654/39654 [==============================] - 1s 25us/step - loss: 0.0223 - mean_squared_error: 0.0142 - val_loss: 0.0227 - val_mean_squared_error: 0.0146\n",
      "\n",
      "Epoch 00054: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 55/100\n",
      "39654/39654 [==============================] - 1s 26us/step - loss: 0.0219 - mean_squared_error: 0.0138 - val_loss: 0.0223 - val_mean_squared_error: 0.0142\n",
      "Epoch 56/100\n",
      "39654/39654 [==============================] - 1s 25us/step - loss: 0.0219 - mean_squared_error: 0.0138 - val_loss: 0.0224 - val_mean_squared_error: 0.0143\n",
      "Epoch 57/100\n",
      "39654/39654 [==============================] - 1s 26us/step - loss: 0.0218 - mean_squared_error: 0.0137 - val_loss: 0.0223 - val_mean_squared_error: 0.0142\n",
      "Epoch 58/100\n",
      "39654/39654 [==============================] - 1s 25us/step - loss: 0.0218 - mean_squared_error: 0.0137 - val_loss: 0.0222 - val_mean_squared_error: 0.0141\n",
      "Epoch 59/100\n",
      "39654/39654 [==============================] - 1s 25us/step - loss: 0.0218 - mean_squared_error: 0.0137 - val_loss: 0.0223 - val_mean_squared_error: 0.0142\n",
      "Epoch 60/100\n",
      "39654/39654 [==============================] - 1s 25us/step - loss: 0.0217 - mean_squared_error: 0.0137 - val_loss: 0.0223 - val_mean_squared_error: 0.0142\n",
      "\n",
      "Epoch 00060: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 61/100\n",
      "39654/39654 [==============================] - 1s 24us/step - loss: 0.0215 - mean_squared_error: 0.0135 - val_loss: 0.0221 - val_mean_squared_error: 0.0140\n",
      "Epoch 62/100\n",
      "39654/39654 [==============================] - 1s 26us/step - loss: 0.0215 - mean_squared_error: 0.0134 - val_loss: 0.0220 - val_mean_squared_error: 0.0139\n",
      "Epoch 63/100\n",
      "39654/39654 [==============================] - 1s 25us/step - loss: 0.0215 - mean_squared_error: 0.0134 - val_loss: 0.0220 - val_mean_squared_error: 0.0140\n",
      "Epoch 64/100\n",
      "39654/39654 [==============================] - 1s 27us/step - loss: 0.0215 - mean_squared_error: 0.0134 - val_loss: 0.0220 - val_mean_squared_error: 0.0139\n",
      "Epoch 65/100\n",
      "39654/39654 [==============================] - 1s 25us/step - loss: 0.0214 - mean_squared_error: 0.0134 - val_loss: 0.0219 - val_mean_squared_error: 0.0139\n",
      "Epoch 66/100\n",
      "39654/39654 [==============================] - 1s 26us/step - loss: 0.0214 - mean_squared_error: 0.0134 - val_loss: 0.0220 - val_mean_squared_error: 0.0140\n",
      "Epoch 67/100\n",
      "39654/39654 [==============================] - 1s 25us/step - loss: 0.0214 - mean_squared_error: 0.0134 - val_loss: 0.0220 - val_mean_squared_error: 0.0140\n",
      "\n",
      "Epoch 00067: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 68/100\n",
      "39654/39654 [==============================] - 1s 26us/step - loss: 0.0213 - mean_squared_error: 0.0132 - val_loss: 0.0219 - val_mean_squared_error: 0.0139\n",
      "Epoch 69/100\n",
      "39654/39654 [==============================] - 1s 27us/step - loss: 0.0213 - mean_squared_error: 0.0132 - val_loss: 0.0218 - val_mean_squared_error: 0.0138\n",
      "Epoch 70/100\n",
      "39654/39654 [==============================] - 1s 25us/step - loss: 0.0213 - mean_squared_error: 0.0132 - val_loss: 0.0218 - val_mean_squared_error: 0.0138\n",
      "Epoch 71/100\n",
      "39654/39654 [==============================] - 1s 25us/step - loss: 0.0213 - mean_squared_error: 0.0132 - val_loss: 0.0218 - val_mean_squared_error: 0.0138\n",
      "Epoch 72/100\n",
      "39654/39654 [==============================] - 1s 28us/step - loss: 0.0212 - mean_squared_error: 0.0132 - val_loss: 0.0218 - val_mean_squared_error: 0.0138\n",
      "Epoch 73/100\n",
      "39654/39654 [==============================] - 2s 39us/step - loss: 0.0212 - mean_squared_error: 0.0132 - val_loss: 0.0218 - val_mean_squared_error: 0.0138\n",
      "Epoch 74/100\n",
      "39654/39654 [==============================] - 1s 33us/step - loss: 0.0212 - mean_squared_error: 0.0132 - val_loss: 0.0218 - val_mean_squared_error: 0.0138\n",
      "Epoch 75/100\n",
      "39654/39654 [==============================] - 1s 29us/step - loss: 0.0212 - mean_squared_error: 0.0132 - val_loss: 0.0218 - val_mean_squared_error: 0.0138\n",
      "\n",
      "Epoch 00075: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 76/100\n",
      "39654/39654 [==============================] - 1s 27us/step - loss: 0.0211 - mean_squared_error: 0.0131 - val_loss: 0.0217 - val_mean_squared_error: 0.0137\n",
      "Epoch 77/100\n",
      "39654/39654 [==============================] - 1s 28us/step - loss: 0.0211 - mean_squared_error: 0.0131 - val_loss: 0.0218 - val_mean_squared_error: 0.0138\n",
      "Epoch 78/100\n",
      "39654/39654 [==============================] - 1s 25us/step - loss: 0.0211 - mean_squared_error: 0.0131 - val_loss: 0.0218 - val_mean_squared_error: 0.0138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/100\n",
      "39654/39654 [==============================] - 1s 29us/step - loss: 0.0211 - mean_squared_error: 0.0131 - val_loss: 0.0217 - val_mean_squared_error: 0.0137\n",
      "Epoch 80/100\n",
      "39654/39654 [==============================] - 1s 26us/step - loss: 0.0211 - mean_squared_error: 0.0131 - val_loss: 0.0217 - val_mean_squared_error: 0.0137\n",
      "\n",
      "Epoch 00080: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 81/100\n",
      "39654/39654 [==============================] - 1s 29us/step - loss: 0.0211 - mean_squared_error: 0.0131 - val_loss: 0.0217 - val_mean_squared_error: 0.0137\n",
      "Epoch 82/100\n",
      "39654/39654 [==============================] - 1s 32us/step - loss: 0.0211 - mean_squared_error: 0.0131 - val_loss: 0.0217 - val_mean_squared_error: 0.0137\n",
      "Epoch 83/100\n",
      "39654/39654 [==============================] - 1s 28us/step - loss: 0.0211 - mean_squared_error: 0.0131 - val_loss: 0.0217 - val_mean_squared_error: 0.0137\n",
      "Epoch 84/100\n",
      "39654/39654 [==============================] - 1s 30us/step - loss: 0.0211 - mean_squared_error: 0.0131 - val_loss: 0.0217 - val_mean_squared_error: 0.0137\n",
      "Epoch 85/100\n",
      "39654/39654 [==============================] - 1s 28us/step - loss: 0.0211 - mean_squared_error: 0.0131 - val_loss: 0.0217 - val_mean_squared_error: 0.0137\n",
      "\n",
      "Epoch 00085: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "Epoch 86/100\n",
      "39654/39654 [==============================] - 1s 27us/step - loss: 0.0211 - mean_squared_error: 0.0131 - val_loss: 0.0217 - val_mean_squared_error: 0.0137\n",
      "Epoch 87/100\n",
      "39654/39654 [==============================] - 1s 24us/step - loss: 0.0211 - mean_squared_error: 0.0131 - val_loss: 0.0217 - val_mean_squared_error: 0.0137\n",
      "Epoch 88/100\n",
      "39654/39654 [==============================] - 1s 23us/step - loss: 0.0211 - mean_squared_error: 0.0131 - val_loss: 0.0217 - val_mean_squared_error: 0.0137\n",
      "Epoch 89/100\n",
      "39654/39654 [==============================] - 1s 23us/step - loss: 0.0211 - mean_squared_error: 0.0131 - val_loss: 0.0217 - val_mean_squared_error: 0.0137\n",
      "Epoch 90/100\n",
      "39654/39654 [==============================] - 1s 24us/step - loss: 0.0211 - mean_squared_error: 0.0131 - val_loss: 0.0217 - val_mean_squared_error: 0.0137\n",
      "\n",
      "Epoch 00090: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "Epoch 91/100\n",
      "39654/39654 [==============================] - 1s 25us/step - loss: 0.0210 - mean_squared_error: 0.0131 - val_loss: 0.0217 - val_mean_squared_error: 0.0137\n",
      "Epoch 92/100\n",
      "39654/39654 [==============================] - 1s 25us/step - loss: 0.0210 - mean_squared_error: 0.0131 - val_loss: 0.0217 - val_mean_squared_error: 0.0137\n",
      "Epoch 93/100\n",
      "39654/39654 [==============================] - 1s 25us/step - loss: 0.0210 - mean_squared_error: 0.0131 - val_loss: 0.0217 - val_mean_squared_error: 0.0137\n",
      "Epoch 94/100\n",
      "39654/39654 [==============================] - 1s 26us/step - loss: 0.0210 - mean_squared_error: 0.0131 - val_loss: 0.0217 - val_mean_squared_error: 0.0137\n",
      "Epoch 95/100\n",
      "39654/39654 [==============================] - 1s 25us/step - loss: 0.0210 - mean_squared_error: 0.0131 - val_loss: 0.0217 - val_mean_squared_error: 0.0137\n",
      "Epoch 96/100\n",
      "39654/39654 [==============================] - 1s 25us/step - loss: 0.0210 - mean_squared_error: 0.0131 - val_loss: 0.0217 - val_mean_squared_error: 0.0137\n",
      "\n",
      "Epoch 00096: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "Epoch 97/100\n",
      "39654/39654 [==============================] - 1s 25us/step - loss: 0.0210 - mean_squared_error: 0.0130 - val_loss: 0.0217 - val_mean_squared_error: 0.0137\n",
      "Epoch 98/100\n",
      "39654/39654 [==============================] - 1s 25us/step - loss: 0.0210 - mean_squared_error: 0.0130 - val_loss: 0.0217 - val_mean_squared_error: 0.0137\n",
      "Epoch 99/100\n",
      "39654/39654 [==============================] - 1s 24us/step - loss: 0.0210 - mean_squared_error: 0.0130 - val_loss: 0.0217 - val_mean_squared_error: 0.0137\n",
      "Epoch 100/100\n",
      "39654/39654 [==============================] - 1s 26us/step - loss: 0.0210 - mean_squared_error: 0.0130 - val_loss: 0.0217 - val_mean_squared_error: 0.0137\n",
      "hw1/models/model_RoboschoolHumanoid-v1_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm/base.hdf5\n",
      "Done training model for RoboschoolHumanoid-v1\n",
      "{'model_name': 'model_RoboschoolHumanoid-v1_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm', 'model_path': 'hw1/models/model_RoboschoolHumanoid-v1_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm', 'test_mse': 0.013682615999663115, 'train_mse': 0.013040134573556305, 'dataset_name': 'RoboschoolHumanoid-v1'}\n",
      "Training a FC ANN for env RoboschoolHalfCheetah-v1\n",
      "loading dataset RoboschoolHalfCheetah-v1\n",
      "Domain name: RoboschoolHalfCheetah-v1\n",
      "(38341, 26) (4261, 26) (38341, 6) (4261, 6)\n",
      "model_name='model_RoboschoolHalfCheetah-v1_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm'\n",
      "model_path='hw1/models/model_RoboschoolHalfCheetah-v1_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm'\n",
      "Train on 38341 samples, validate on 4261 samples\n",
      "Epoch 1/100\n",
      "38341/38341 [==============================] - 1s 29us/step - loss: 0.2106 - mean_squared_error: 0.1870 - val_loss: 0.1090 - val_mean_squared_error: 0.0846\n",
      "Epoch 2/100\n",
      "38341/38341 [==============================] - 1s 26us/step - loss: 0.0889 - mean_squared_error: 0.0644 - val_loss: 0.0923 - val_mean_squared_error: 0.0678\n",
      "Epoch 3/100\n",
      "38341/38341 [==============================] - 1s 33us/step - loss: 0.0736 - mean_squared_error: 0.0493 - val_loss: 0.0676 - val_mean_squared_error: 0.0435\n",
      "Epoch 4/100\n",
      "38341/38341 [==============================] - 1s 24us/step - loss: 0.0650 - mean_squared_error: 0.0412 - val_loss: 0.0621 - val_mean_squared_error: 0.0386\n",
      "Epoch 5/100\n",
      "38341/38341 [==============================] - 1s 25us/step - loss: 0.0597 - mean_squared_error: 0.0364 - val_loss: 0.0571 - val_mean_squared_error: 0.0342\n",
      "Epoch 6/100\n",
      "38341/38341 [==============================] - 1s 24us/step - loss: 0.0560 - mean_squared_error: 0.0334 - val_loss: 0.0550 - val_mean_squared_error: 0.0328\n",
      "Epoch 7/100\n",
      "38341/38341 [==============================] - 1s 23us/step - loss: 0.0525 - mean_squared_error: 0.0306 - val_loss: 0.0541 - val_mean_squared_error: 0.0325\n",
      "Epoch 8/100\n",
      "38341/38341 [==============================] - 1s 23us/step - loss: 0.0500 - mean_squared_error: 0.0287 - val_loss: 0.0486 - val_mean_squared_error: 0.0277\n",
      "Epoch 9/100\n",
      "38341/38341 [==============================] - 1s 23us/step - loss: 0.0483 - mean_squared_error: 0.0278 - val_loss: 0.0478 - val_mean_squared_error: 0.0276\n",
      "Epoch 10/100\n",
      "38341/38341 [==============================] - 1s 23us/step - loss: 0.0464 - mean_squared_error: 0.0265 - val_loss: 0.0463 - val_mean_squared_error: 0.0267\n",
      "Epoch 11/100\n",
      "38341/38341 [==============================] - 1s 23us/step - loss: 0.0449 - mean_squared_error: 0.0256 - val_loss: 0.0470 - val_mean_squared_error: 0.0279\n",
      "Epoch 12/100\n",
      "38341/38341 [==============================] - 1s 23us/step - loss: 0.0436 - mean_squared_error: 0.0247 - val_loss: 0.0426 - val_mean_squared_error: 0.0240\n",
      "Epoch 13/100\n",
      "38341/38341 [==============================] - 1s 23us/step - loss: 0.0424 - mean_squared_error: 0.0241 - val_loss: 0.0425 - val_mean_squared_error: 0.0244\n",
      "Epoch 14/100\n",
      "38341/38341 [==============================] - 1s 24us/step - loss: 0.0412 - mean_squared_error: 0.0233 - val_loss: 0.0452 - val_mean_squared_error: 0.0275\n",
      "Epoch 15/100\n",
      "38341/38341 [==============================] - 1s 23us/step - loss: 0.0403 - mean_squared_error: 0.0228 - val_loss: 0.0421 - val_mean_squared_error: 0.0248\n",
      "Epoch 16/100\n",
      "38341/38341 [==============================] - 1s 22us/step - loss: 0.0393 - mean_squared_error: 0.0222 - val_loss: 0.0413 - val_mean_squared_error: 0.0244\n",
      "Epoch 17/100\n",
      "38341/38341 [==============================] - 1s 23us/step - loss: 0.0386 - mean_squared_error: 0.0219 - val_loss: 0.0407 - val_mean_squared_error: 0.0242\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 18/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38341/38341 [==============================] - 1s 23us/step - loss: 0.0351 - mean_squared_error: 0.0186 - val_loss: 0.0361 - val_mean_squared_error: 0.0198\n",
      "Epoch 19/100\n",
      "38341/38341 [==============================] - 1s 23us/step - loss: 0.0349 - mean_squared_error: 0.0186 - val_loss: 0.0366 - val_mean_squared_error: 0.0204\n",
      "Epoch 20/100\n",
      "38341/38341 [==============================] - 1s 23us/step - loss: 0.0343 - mean_squared_error: 0.0183 - val_loss: 0.0350 - val_mean_squared_error: 0.0191\n",
      "Epoch 21/100\n",
      "38341/38341 [==============================] - 1s 25us/step - loss: 0.0340 - mean_squared_error: 0.0182 - val_loss: 0.0437 - val_mean_squared_error: 0.0280\n",
      "Epoch 22/100\n",
      "38341/38341 [==============================] - 1s 23us/step - loss: 0.0340 - mean_squared_error: 0.0183 - val_loss: 0.0367 - val_mean_squared_error: 0.0211\n",
      "Epoch 23/100\n",
      "38341/38341 [==============================] - 1s 23us/step - loss: 0.0331 - mean_squared_error: 0.0176 - val_loss: 0.0341 - val_mean_squared_error: 0.0188\n",
      "Epoch 24/100\n",
      "38341/38341 [==============================] - 1s 23us/step - loss: 0.0330 - mean_squared_error: 0.0177 - val_loss: 0.0322 - val_mean_squared_error: 0.0171\n",
      "Epoch 25/100\n",
      "38341/38341 [==============================] - 1s 23us/step - loss: 0.0324 - mean_squared_error: 0.0173 - val_loss: 0.0333 - val_mean_squared_error: 0.0183\n",
      "Epoch 26/100\n",
      "38341/38341 [==============================] - 1s 23us/step - loss: 0.0323 - mean_squared_error: 0.0173 - val_loss: 0.0343 - val_mean_squared_error: 0.0194\n",
      "Epoch 27/100\n",
      "38341/38341 [==============================] - 1s 23us/step - loss: 0.0320 - mean_squared_error: 0.0172 - val_loss: 0.0344 - val_mean_squared_error: 0.0197\n",
      "Epoch 28/100\n",
      "38341/38341 [==============================] - 1s 23us/step - loss: 0.0316 - mean_squared_error: 0.0170 - val_loss: 0.0318 - val_mean_squared_error: 0.0172\n",
      "Epoch 29/100\n",
      "38341/38341 [==============================] - 1s 22us/step - loss: 0.0312 - mean_squared_error: 0.0167 - val_loss: 0.0362 - val_mean_squared_error: 0.0218\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 30/100\n",
      "38341/38341 [==============================] - 1s 25us/step - loss: 0.0295 - mean_squared_error: 0.0152 - val_loss: 0.0303 - val_mean_squared_error: 0.0160\n",
      "Epoch 31/100\n",
      "38341/38341 [==============================] - 1s 24us/step - loss: 0.0294 - mean_squared_error: 0.0151 - val_loss: 0.0302 - val_mean_squared_error: 0.0159\n",
      "Epoch 32/100\n",
      "38341/38341 [==============================] - 1s 23us/step - loss: 0.0293 - mean_squared_error: 0.0151 - val_loss: 0.0296 - val_mean_squared_error: 0.0154\n",
      "Epoch 33/100\n",
      "38341/38341 [==============================] - 1s 23us/step - loss: 0.0290 - mean_squared_error: 0.0149 - val_loss: 0.0317 - val_mean_squared_error: 0.0175\n",
      "Epoch 34/100\n",
      "38341/38341 [==============================] - 1s 23us/step - loss: 0.0289 - mean_squared_error: 0.0149 - val_loss: 0.0295 - val_mean_squared_error: 0.0155\n",
      "Epoch 35/100\n",
      "38341/38341 [==============================] - 1s 23us/step - loss: 0.0288 - mean_squared_error: 0.0149 - val_loss: 0.0297 - val_mean_squared_error: 0.0158\n",
      "Epoch 36/100\n",
      "38341/38341 [==============================] - 1s 22us/step - loss: 0.0286 - mean_squared_error: 0.0147 - val_loss: 0.0299 - val_mean_squared_error: 0.0161\n",
      "Epoch 37/100\n",
      "38341/38341 [==============================] - 1s 23us/step - loss: 0.0285 - mean_squared_error: 0.0146 - val_loss: 0.0306 - val_mean_squared_error: 0.0168\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 38/100\n",
      "38341/38341 [==============================] - 1s 24us/step - loss: 0.0277 - mean_squared_error: 0.0139 - val_loss: 0.0285 - val_mean_squared_error: 0.0148\n",
      "Epoch 39/100\n",
      "38341/38341 [==============================] - 1s 23us/step - loss: 0.0275 - mean_squared_error: 0.0138 - val_loss: 0.0282 - val_mean_squared_error: 0.0145\n",
      "Epoch 40/100\n",
      "38341/38341 [==============================] - 1s 25us/step - loss: 0.0275 - mean_squared_error: 0.0138 - val_loss: 0.0289 - val_mean_squared_error: 0.0152\n",
      "Epoch 41/100\n",
      "38341/38341 [==============================] - 1s 25us/step - loss: 0.0274 - mean_squared_error: 0.0138 - val_loss: 0.0285 - val_mean_squared_error: 0.0149\n",
      "Epoch 42/100\n",
      "38341/38341 [==============================] - 1s 28us/step - loss: 0.0273 - mean_squared_error: 0.0137 - val_loss: 0.0281 - val_mean_squared_error: 0.0145\n",
      "Epoch 43/100\n",
      "38341/38341 [==============================] - 1s 27us/step - loss: 0.0272 - mean_squared_error: 0.0136 - val_loss: 0.0283 - val_mean_squared_error: 0.0148\n",
      "Epoch 44/100\n",
      "38341/38341 [==============================] - 1s 24us/step - loss: 0.0271 - mean_squared_error: 0.0136 - val_loss: 0.0280 - val_mean_squared_error: 0.0145\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 45/100\n",
      "38341/38341 [==============================] - 1s 25us/step - loss: 0.0267 - mean_squared_error: 0.0132 - val_loss: 0.0275 - val_mean_squared_error: 0.0140\n",
      "Epoch 46/100\n",
      "38341/38341 [==============================] - 1s 25us/step - loss: 0.0266 - mean_squared_error: 0.0131 - val_loss: 0.0277 - val_mean_squared_error: 0.0142\n",
      "Epoch 47/100\n",
      "38341/38341 [==============================] - 1s 26us/step - loss: 0.0266 - mean_squared_error: 0.0131 - val_loss: 0.0276 - val_mean_squared_error: 0.0142\n",
      "Epoch 48/100\n",
      "38341/38341 [==============================] - 1s 27us/step - loss: 0.0265 - mean_squared_error: 0.0131 - val_loss: 0.0274 - val_mean_squared_error: 0.0140\n",
      "Epoch 49/100\n",
      "38341/38341 [==============================] - 1s 31us/step - loss: 0.0265 - mean_squared_error: 0.0131 - val_loss: 0.0275 - val_mean_squared_error: 0.0141\n",
      "Epoch 50/100\n",
      "38341/38341 [==============================] - 1s 26us/step - loss: 0.0265 - mean_squared_error: 0.0131 - val_loss: 0.0274 - val_mean_squared_error: 0.0140\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 51/100\n",
      "38341/38341 [==============================] - 1s 27us/step - loss: 0.0262 - mean_squared_error: 0.0128 - val_loss: 0.0273 - val_mean_squared_error: 0.0139\n",
      "Epoch 52/100\n",
      "38341/38341 [==============================] - 1s 26us/step - loss: 0.0262 - mean_squared_error: 0.0128 - val_loss: 0.0272 - val_mean_squared_error: 0.0138\n",
      "Epoch 53/100\n",
      "38341/38341 [==============================] - 1s 25us/step - loss: 0.0261 - mean_squared_error: 0.0128 - val_loss: 0.0272 - val_mean_squared_error: 0.0138\n",
      "Epoch 54/100\n",
      "38341/38341 [==============================] - 1s 26us/step - loss: 0.0261 - mean_squared_error: 0.0128 - val_loss: 0.0273 - val_mean_squared_error: 0.0139\n",
      "Epoch 55/100\n",
      "38341/38341 [==============================] - 1s 26us/step - loss: 0.0261 - mean_squared_error: 0.0128 - val_loss: 0.0272 - val_mean_squared_error: 0.0139\n",
      "Epoch 56/100\n",
      "38341/38341 [==============================] - 1s 24us/step - loss: 0.0261 - mean_squared_error: 0.0127 - val_loss: 0.0272 - val_mean_squared_error: 0.0139\n",
      "\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 57/100\n",
      "38341/38341 [==============================] - 1s 25us/step - loss: 0.0259 - mean_squared_error: 0.0126 - val_loss: 0.0270 - val_mean_squared_error: 0.0137\n",
      "Epoch 58/100\n",
      "38341/38341 [==============================] - 1s 25us/step - loss: 0.0259 - mean_squared_error: 0.0126 - val_loss: 0.0270 - val_mean_squared_error: 0.0137\n",
      "Epoch 59/100\n",
      "38341/38341 [==============================] - 1s 25us/step - loss: 0.0259 - mean_squared_error: 0.0126 - val_loss: 0.0270 - val_mean_squared_error: 0.0137\n",
      "Epoch 60/100\n",
      "38341/38341 [==============================] - 1s 24us/step - loss: 0.0259 - mean_squared_error: 0.0126 - val_loss: 0.0270 - val_mean_squared_error: 0.0137\n",
      "Epoch 61/100\n",
      "38341/38341 [==============================] - 1s 25us/step - loss: 0.0259 - mean_squared_error: 0.0126 - val_loss: 0.0270 - val_mean_squared_error: 0.0137\n",
      "Epoch 62/100\n",
      "38341/38341 [==============================] - 1s 25us/step - loss: 0.0259 - mean_squared_error: 0.0126 - val_loss: 0.0269 - val_mean_squared_error: 0.0136\n",
      "\n",
      "Epoch 00062: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 63/100\n",
      "38341/38341 [==============================] - 1s 25us/step - loss: 0.0258 - mean_squared_error: 0.0125 - val_loss: 0.0269 - val_mean_squared_error: 0.0136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/100\n",
      "38341/38341 [==============================] - 1s 25us/step - loss: 0.0258 - mean_squared_error: 0.0125 - val_loss: 0.0269 - val_mean_squared_error: 0.0136\n",
      "Epoch 65/100\n",
      "38341/38341 [==============================] - 1s 23us/step - loss: 0.0258 - mean_squared_error: 0.0125 - val_loss: 0.0269 - val_mean_squared_error: 0.0136\n",
      "Epoch 66/100\n",
      "38341/38341 [==============================] - 1s 23us/step - loss: 0.0258 - mean_squared_error: 0.0125 - val_loss: 0.0269 - val_mean_squared_error: 0.0136\n",
      "Epoch 67/100\n",
      "38341/38341 [==============================] - 1s 24us/step - loss: 0.0258 - mean_squared_error: 0.0125 - val_loss: 0.0269 - val_mean_squared_error: 0.0136\n",
      "\n",
      "Epoch 00067: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "Epoch 68/100\n",
      "38341/38341 [==============================] - 1s 24us/step - loss: 0.0257 - mean_squared_error: 0.0125 - val_loss: 0.0269 - val_mean_squared_error: 0.0136\n",
      "Epoch 69/100\n",
      "38341/38341 [==============================] - 1s 24us/step - loss: 0.0257 - mean_squared_error: 0.0125 - val_loss: 0.0269 - val_mean_squared_error: 0.0136\n",
      "Epoch 70/100\n",
      "38341/38341 [==============================] - 1s 24us/step - loss: 0.0257 - mean_squared_error: 0.0125 - val_loss: 0.0269 - val_mean_squared_error: 0.0136\n",
      "Epoch 71/100\n",
      "38341/38341 [==============================] - 1s 24us/step - loss: 0.0257 - mean_squared_error: 0.0125 - val_loss: 0.0269 - val_mean_squared_error: 0.0136\n",
      "Epoch 72/100\n",
      "38341/38341 [==============================] - 1s 25us/step - loss: 0.0257 - mean_squared_error: 0.0125 - val_loss: 0.0269 - val_mean_squared_error: 0.0136\n",
      "\n",
      "Epoch 00072: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "Epoch 73/100\n",
      "38341/38341 [==============================] - 1s 23us/step - loss: 0.0257 - mean_squared_error: 0.0124 - val_loss: 0.0269 - val_mean_squared_error: 0.0136\n",
      "Epoch 74/100\n",
      "38341/38341 [==============================] - 1s 23us/step - loss: 0.0257 - mean_squared_error: 0.0124 - val_loss: 0.0268 - val_mean_squared_error: 0.0136\n",
      "Epoch 75/100\n",
      "38341/38341 [==============================] - 1s 23us/step - loss: 0.0257 - mean_squared_error: 0.0124 - val_loss: 0.0268 - val_mean_squared_error: 0.0136\n",
      "Epoch 76/100\n",
      "38341/38341 [==============================] - 1s 23us/step - loss: 0.0257 - mean_squared_error: 0.0124 - val_loss: 0.0269 - val_mean_squared_error: 0.0136\n",
      "Epoch 77/100\n",
      "38341/38341 [==============================] - 1s 24us/step - loss: 0.0257 - mean_squared_error: 0.0124 - val_loss: 0.0268 - val_mean_squared_error: 0.0136\n",
      "Epoch 78/100\n",
      "38341/38341 [==============================] - 1s 24us/step - loss: 0.0257 - mean_squared_error: 0.0124 - val_loss: 0.0268 - val_mean_squared_error: 0.0136\n",
      "Epoch 79/100\n",
      "38341/38341 [==============================] - 1s 24us/step - loss: 0.0257 - mean_squared_error: 0.0124 - val_loss: 0.0268 - val_mean_squared_error: 0.0136\n",
      "Epoch 80/100\n",
      "38341/38341 [==============================] - 1s 25us/step - loss: 0.0257 - mean_squared_error: 0.0124 - val_loss: 0.0268 - val_mean_squared_error: 0.0136\n",
      "Epoch 81/100\n",
      "38341/38341 [==============================] - 1s 24us/step - loss: 0.0257 - mean_squared_error: 0.0124 - val_loss: 0.0268 - val_mean_squared_error: 0.0136\n",
      "Epoch 82/100\n",
      "38341/38341 [==============================] - 1s 24us/step - loss: 0.0257 - mean_squared_error: 0.0124 - val_loss: 0.0269 - val_mean_squared_error: 0.0136\n",
      "\n",
      "Epoch 00082: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "Epoch 83/100\n",
      "38341/38341 [==============================] - 1s 25us/step - loss: 0.0257 - mean_squared_error: 0.0124 - val_loss: 0.0268 - val_mean_squared_error: 0.0136\n",
      "Epoch 84/100\n",
      "38341/38341 [==============================] - 1s 24us/step - loss: 0.0257 - mean_squared_error: 0.0124 - val_loss: 0.0268 - val_mean_squared_error: 0.0136\n",
      "Epoch 85/100\n",
      "38341/38341 [==============================] - 1s 24us/step - loss: 0.0257 - mean_squared_error: 0.0124 - val_loss: 0.0268 - val_mean_squared_error: 0.0136\n",
      "Epoch 86/100\n",
      "38341/38341 [==============================] - 1s 24us/step - loss: 0.0257 - mean_squared_error: 0.0124 - val_loss: 0.0268 - val_mean_squared_error: 0.0136\n",
      "Epoch 87/100\n",
      "38341/38341 [==============================] - 1s 23us/step - loss: 0.0257 - mean_squared_error: 0.0124 - val_loss: 0.0268 - val_mean_squared_error: 0.0136\n",
      "\n",
      "Epoch 00087: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "Epoch 88/100\n",
      "38341/38341 [==============================] - 1s 23us/step - loss: 0.0257 - mean_squared_error: 0.0124 - val_loss: 0.0268 - val_mean_squared_error: 0.0136\n",
      "Epoch 89/100\n",
      "38341/38341 [==============================] - 1s 23us/step - loss: 0.0257 - mean_squared_error: 0.0124 - val_loss: 0.0268 - val_mean_squared_error: 0.0136\n",
      "Epoch 90/100\n",
      "38341/38341 [==============================] - 1s 23us/step - loss: 0.0257 - mean_squared_error: 0.0124 - val_loss: 0.0268 - val_mean_squared_error: 0.0136\n",
      "Epoch 91/100\n",
      "38341/38341 [==============================] - 1s 23us/step - loss: 0.0257 - mean_squared_error: 0.0124 - val_loss: 0.0268 - val_mean_squared_error: 0.0136\n",
      "Epoch 92/100\n",
      "38341/38341 [==============================] - 1s 24us/step - loss: 0.0257 - mean_squared_error: 0.0124 - val_loss: 0.0268 - val_mean_squared_error: 0.0136\n",
      "\n",
      "Epoch 00092: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
      "Epoch 93/100\n",
      "38341/38341 [==============================] - 1s 23us/step - loss: 0.0257 - mean_squared_error: 0.0124 - val_loss: 0.0268 - val_mean_squared_error: 0.0136\n",
      "Epoch 94/100\n",
      "38341/38341 [==============================] - 1s 24us/step - loss: 0.0257 - mean_squared_error: 0.0124 - val_loss: 0.0268 - val_mean_squared_error: 0.0136\n",
      "Epoch 95/100\n",
      "38341/38341 [==============================] - 1s 24us/step - loss: 0.0257 - mean_squared_error: 0.0124 - val_loss: 0.0268 - val_mean_squared_error: 0.0136\n",
      "Epoch 96/100\n",
      "38341/38341 [==============================] - 1s 25us/step - loss: 0.0257 - mean_squared_error: 0.0124 - val_loss: 0.0268 - val_mean_squared_error: 0.0136\n",
      "Epoch 97/100\n",
      "38341/38341 [==============================] - 1s 24us/step - loss: 0.0257 - mean_squared_error: 0.0124 - val_loss: 0.0268 - val_mean_squared_error: 0.0136\n",
      "\n",
      "Epoch 00097: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
      "Epoch 98/100\n",
      "38341/38341 [==============================] - 1s 24us/step - loss: 0.0257 - mean_squared_error: 0.0124 - val_loss: 0.0268 - val_mean_squared_error: 0.0136\n",
      "Epoch 99/100\n",
      "38341/38341 [==============================] - 1s 24us/step - loss: 0.0257 - mean_squared_error: 0.0124 - val_loss: 0.0268 - val_mean_squared_error: 0.0136\n",
      "Epoch 100/100\n",
      "38341/38341 [==============================] - 1s 24us/step - loss: 0.0257 - mean_squared_error: 0.0124 - val_loss: 0.0268 - val_mean_squared_error: 0.0136\n",
      "hw1/models/model_RoboschoolHalfCheetah-v1_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm/base.hdf5\n",
      "Done training model for RoboschoolHalfCheetah-v1\n",
      "{'model_name': 'model_RoboschoolHalfCheetah-v1_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm', 'model_path': 'hw1/models/model_RoboschoolHalfCheetah-v1_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm', 'test_mse': 0.013571882395915625, 'train_mse': 0.01240620822838095, 'dataset_name': 'RoboschoolHalfCheetah-v1'}\n",
      "Training a FC ANN for env RoboschoolReacher-v1\n",
      "loading dataset RoboschoolReacher-v1\n",
      "Domain name: RoboschoolReacher-v1\n",
      "(6750, 9) (750, 9) (6750, 2) (750, 2)\n",
      "model_name='model_RoboschoolReacher-v1_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm'\n",
      "model_path='hw1/models/model_RoboschoolReacher-v1_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm'\n",
      "Train on 6750 samples, validate on 750 samples\n",
      "Epoch 1/100\n",
      "6750/6750 [==============================] - 0s 54us/step - loss: 0.1125 - mean_squared_error: 0.0928 - val_loss: 0.1082 - val_mean_squared_error: 0.0900\n",
      "Epoch 2/100\n",
      "6750/6750 [==============================] - 0s 25us/step - loss: 0.0921 - mean_squared_error: 0.0745 - val_loss: 0.0886 - val_mean_squared_error: 0.0714\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6750/6750 [==============================] - 0s 27us/step - loss: 0.0782 - mean_squared_error: 0.0611 - val_loss: 0.0753 - val_mean_squared_error: 0.0584\n",
      "Epoch 4/100\n",
      "6750/6750 [==============================] - 0s 26us/step - loss: 0.0679 - mean_squared_error: 0.0511 - val_loss: 0.0694 - val_mean_squared_error: 0.0526\n",
      "Epoch 5/100\n",
      "6750/6750 [==============================] - 0s 25us/step - loss: 0.0584 - mean_squared_error: 0.0416 - val_loss: 0.0620 - val_mean_squared_error: 0.0453\n",
      "Epoch 6/100\n",
      "6750/6750 [==============================] - 0s 25us/step - loss: 0.0529 - mean_squared_error: 0.0362 - val_loss: 0.0513 - val_mean_squared_error: 0.0345\n",
      "Epoch 7/100\n",
      "6750/6750 [==============================] - 0s 24us/step - loss: 0.0476 - mean_squared_error: 0.0309 - val_loss: 0.0501 - val_mean_squared_error: 0.0334\n",
      "Epoch 8/100\n",
      "6750/6750 [==============================] - 0s 24us/step - loss: 0.0436 - mean_squared_error: 0.0269 - val_loss: 0.0432 - val_mean_squared_error: 0.0265\n",
      "Epoch 9/100\n",
      "6750/6750 [==============================] - 0s 25us/step - loss: 0.0405 - mean_squared_error: 0.0239 - val_loss: 0.0432 - val_mean_squared_error: 0.0267\n",
      "Epoch 10/100\n",
      "6750/6750 [==============================] - 0s 25us/step - loss: 0.0378 - mean_squared_error: 0.0213 - val_loss: 0.0398 - val_mean_squared_error: 0.0234\n",
      "Epoch 11/100\n",
      "6750/6750 [==============================] - 0s 25us/step - loss: 0.0358 - mean_squared_error: 0.0195 - val_loss: 0.0368 - val_mean_squared_error: 0.0206\n",
      "Epoch 12/100\n",
      "6750/6750 [==============================] - 0s 26us/step - loss: 0.0342 - mean_squared_error: 0.0180 - val_loss: 0.0351 - val_mean_squared_error: 0.0190\n",
      "Epoch 13/100\n",
      "6750/6750 [==============================] - 0s 24us/step - loss: 0.0318 - mean_squared_error: 0.0158 - val_loss: 0.0347 - val_mean_squared_error: 0.0187\n",
      "Epoch 14/100\n",
      "6750/6750 [==============================] - 0s 25us/step - loss: 0.0318 - mean_squared_error: 0.0160 - val_loss: 0.0351 - val_mean_squared_error: 0.0193\n",
      "Epoch 15/100\n",
      "6750/6750 [==============================] - 0s 25us/step - loss: 0.0300 - mean_squared_error: 0.0142 - val_loss: 0.0331 - val_mean_squared_error: 0.0175\n",
      "Epoch 16/100\n",
      "6750/6750 [==============================] - 0s 26us/step - loss: 0.0285 - mean_squared_error: 0.0130 - val_loss: 0.0329 - val_mean_squared_error: 0.0175\n",
      "Epoch 17/100\n",
      "6750/6750 [==============================] - 0s 25us/step - loss: 0.0277 - mean_squared_error: 0.0124 - val_loss: 0.0304 - val_mean_squared_error: 0.0152\n",
      "Epoch 18/100\n",
      "6750/6750 [==============================] - 0s 25us/step - loss: 0.0268 - mean_squared_error: 0.0116 - val_loss: 0.0334 - val_mean_squared_error: 0.0184\n",
      "Epoch 19/100\n",
      "6750/6750 [==============================] - 0s 23us/step - loss: 0.0263 - mean_squared_error: 0.0114 - val_loss: 0.0311 - val_mean_squared_error: 0.0163\n",
      "Epoch 20/100\n",
      "6750/6750 [==============================] - 0s 23us/step - loss: 0.0260 - mean_squared_error: 0.0113 - val_loss: 0.0294 - val_mean_squared_error: 0.0147\n",
      "Epoch 21/100\n",
      "6750/6750 [==============================] - 0s 23us/step - loss: 0.0253 - mean_squared_error: 0.0108 - val_loss: 0.0295 - val_mean_squared_error: 0.0151\n",
      "Epoch 22/100\n",
      "6750/6750 [==============================] - 0s 25us/step - loss: 0.0247 - mean_squared_error: 0.0105 - val_loss: 0.0298 - val_mean_squared_error: 0.0156\n",
      "Epoch 23/100\n",
      "6750/6750 [==============================] - 0s 27us/step - loss: 0.0239 - mean_squared_error: 0.0098 - val_loss: 0.0291 - val_mean_squared_error: 0.0151\n",
      "Epoch 24/100\n",
      "6750/6750 [==============================] - 0s 26us/step - loss: 0.0231 - mean_squared_error: 0.0091 - val_loss: 0.0300 - val_mean_squared_error: 0.0162\n",
      "Epoch 25/100\n",
      "6750/6750 [==============================] - 0s 26us/step - loss: 0.0232 - mean_squared_error: 0.0095 - val_loss: 0.0293 - val_mean_squared_error: 0.0157\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 26/100\n",
      "6750/6750 [==============================] - 0s 28us/step - loss: 0.0211 - mean_squared_error: 0.0076 - val_loss: 0.0269 - val_mean_squared_error: 0.0133\n",
      "Epoch 27/100\n",
      "6750/6750 [==============================] - 0s 28us/step - loss: 0.0207 - mean_squared_error: 0.0072 - val_loss: 0.0263 - val_mean_squared_error: 0.0129\n",
      "Epoch 28/100\n",
      "6750/6750 [==============================] - 0s 27us/step - loss: 0.0205 - mean_squared_error: 0.0071 - val_loss: 0.0273 - val_mean_squared_error: 0.0139\n",
      "Epoch 29/100\n",
      "6750/6750 [==============================] - 0s 27us/step - loss: 0.0200 - mean_squared_error: 0.0067 - val_loss: 0.0258 - val_mean_squared_error: 0.0125\n",
      "Epoch 30/100\n",
      "6750/6750 [==============================] - 0s 26us/step - loss: 0.0199 - mean_squared_error: 0.0067 - val_loss: 0.0258 - val_mean_squared_error: 0.0126\n",
      "Epoch 31/100\n",
      "6750/6750 [==============================] - 0s 27us/step - loss: 0.0197 - mean_squared_error: 0.0066 - val_loss: 0.0277 - val_mean_squared_error: 0.0147\n",
      "Epoch 32/100\n",
      "6750/6750 [==============================] - 0s 25us/step - loss: 0.0197 - mean_squared_error: 0.0066 - val_loss: 0.0244 - val_mean_squared_error: 0.0114\n",
      "Epoch 33/100\n",
      "6750/6750 [==============================] - 0s 25us/step - loss: 0.0194 - mean_squared_error: 0.0065 - val_loss: 0.0255 - val_mean_squared_error: 0.0126\n",
      "Epoch 34/100\n",
      "6750/6750 [==============================] - 0s 24us/step - loss: 0.0190 - mean_squared_error: 0.0062 - val_loss: 0.0245 - val_mean_squared_error: 0.0117\n",
      "Epoch 35/100\n",
      "6750/6750 [==============================] - 0s 23us/step - loss: 0.0190 - mean_squared_error: 0.0063 - val_loss: 0.0243 - val_mean_squared_error: 0.0116\n",
      "Epoch 36/100\n",
      "6750/6750 [==============================] - 0s 23us/step - loss: 0.0188 - mean_squared_error: 0.0061 - val_loss: 0.0243 - val_mean_squared_error: 0.0117\n",
      "Epoch 37/100\n",
      "6750/6750 [==============================] - 0s 23us/step - loss: 0.0185 - mean_squared_error: 0.0060 - val_loss: 0.0256 - val_mean_squared_error: 0.0131\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 38/100\n",
      "6750/6750 [==============================] - 0s 24us/step - loss: 0.0177 - mean_squared_error: 0.0053 - val_loss: 0.0237 - val_mean_squared_error: 0.0112\n",
      "Epoch 39/100\n",
      "6750/6750 [==============================] - 0s 27us/step - loss: 0.0174 - mean_squared_error: 0.0050 - val_loss: 0.0239 - val_mean_squared_error: 0.0115\n",
      "Epoch 40/100\n",
      "6750/6750 [==============================] - 0s 24us/step - loss: 0.0174 - mean_squared_error: 0.0050 - val_loss: 0.0241 - val_mean_squared_error: 0.0118\n",
      "Epoch 41/100\n",
      "6750/6750 [==============================] - 0s 26us/step - loss: 0.0175 - mean_squared_error: 0.0051 - val_loss: 0.0234 - val_mean_squared_error: 0.0111\n",
      "Epoch 42/100\n",
      "6750/6750 [==============================] - 0s 24us/step - loss: 0.0174 - mean_squared_error: 0.0051 - val_loss: 0.0239 - val_mean_squared_error: 0.0116\n",
      "Epoch 43/100\n",
      "6750/6750 [==============================] - 0s 23us/step - loss: 0.0172 - mean_squared_error: 0.0049 - val_loss: 0.0243 - val_mean_squared_error: 0.0121\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 44/100\n",
      "6750/6750 [==============================] - 0s 23us/step - loss: 0.0169 - mean_squared_error: 0.0047 - val_loss: 0.0236 - val_mean_squared_error: 0.0114\n",
      "Epoch 45/100\n",
      "6750/6750 [==============================] - 0s 23us/step - loss: 0.0167 - mean_squared_error: 0.0045 - val_loss: 0.0231 - val_mean_squared_error: 0.0109\n",
      "Epoch 46/100\n",
      "6750/6750 [==============================] - 0s 23us/step - loss: 0.0166 - mean_squared_error: 0.0045 - val_loss: 0.0232 - val_mean_squared_error: 0.0110\n",
      "Epoch 47/100\n",
      "6750/6750 [==============================] - 0s 25us/step - loss: 0.0165 - mean_squared_error: 0.0044 - val_loss: 0.0229 - val_mean_squared_error: 0.0108\n",
      "Epoch 48/100\n",
      "6750/6750 [==============================] - 0s 22us/step - loss: 0.0165 - mean_squared_error: 0.0044 - val_loss: 0.0229 - val_mean_squared_error: 0.0108\n",
      "Epoch 49/100\n",
      "6750/6750 [==============================] - 0s 24us/step - loss: 0.0165 - mean_squared_error: 0.0044 - val_loss: 0.0228 - val_mean_squared_error: 0.0108\n",
      "Epoch 50/100\n",
      "6750/6750 [==============================] - 0s 24us/step - loss: 0.0164 - mean_squared_error: 0.0044 - val_loss: 0.0229 - val_mean_squared_error: 0.0109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/100\n",
      "6750/6750 [==============================] - 0s 23us/step - loss: 0.0164 - mean_squared_error: 0.0044 - val_loss: 0.0231 - val_mean_squared_error: 0.0111\n",
      "Epoch 52/100\n",
      "6750/6750 [==============================] - 0s 23us/step - loss: 0.0164 - mean_squared_error: 0.0044 - val_loss: 0.0226 - val_mean_squared_error: 0.0107\n",
      "Epoch 53/100\n",
      "6750/6750 [==============================] - 0s 23us/step - loss: 0.0163 - mean_squared_error: 0.0043 - val_loss: 0.0226 - val_mean_squared_error: 0.0106\n",
      "Epoch 54/100\n",
      "6750/6750 [==============================] - 0s 23us/step - loss: 0.0163 - mean_squared_error: 0.0043 - val_loss: 0.0226 - val_mean_squared_error: 0.0107\n",
      "Epoch 55/100\n",
      "6750/6750 [==============================] - 0s 24us/step - loss: 0.0162 - mean_squared_error: 0.0043 - val_loss: 0.0229 - val_mean_squared_error: 0.0110\n",
      "Epoch 56/100\n",
      "6750/6750 [==============================] - 0s 24us/step - loss: 0.0162 - mean_squared_error: 0.0043 - val_loss: 0.0228 - val_mean_squared_error: 0.0109\n",
      "Epoch 57/100\n",
      "6750/6750 [==============================] - 0s 23us/step - loss: 0.0162 - mean_squared_error: 0.0043 - val_loss: 0.0227 - val_mean_squared_error: 0.0109\n",
      "\n",
      "Epoch 00057: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 58/100\n",
      "6750/6750 [==============================] - 0s 23us/step - loss: 0.0159 - mean_squared_error: 0.0041 - val_loss: 0.0222 - val_mean_squared_error: 0.0104\n",
      "Epoch 59/100\n",
      "6750/6750 [==============================] - 0s 23us/step - loss: 0.0159 - mean_squared_error: 0.0041 - val_loss: 0.0223 - val_mean_squared_error: 0.0105\n",
      "Epoch 60/100\n",
      "6750/6750 [==============================] - 0s 31us/step - loss: 0.0158 - mean_squared_error: 0.0040 - val_loss: 0.0224 - val_mean_squared_error: 0.0106\n",
      "Epoch 61/100\n",
      "6750/6750 [==============================] - 0s 27us/step - loss: 0.0158 - mean_squared_error: 0.0040 - val_loss: 0.0224 - val_mean_squared_error: 0.0106\n",
      "Epoch 62/100\n",
      "6750/6750 [==============================] - 0s 24us/step - loss: 0.0158 - mean_squared_error: 0.0040 - val_loss: 0.0222 - val_mean_squared_error: 0.0105\n",
      "Epoch 63/100\n",
      "6750/6750 [==============================] - 0s 24us/step - loss: 0.0158 - mean_squared_error: 0.0040 - val_loss: 0.0222 - val_mean_squared_error: 0.0105\n",
      "\n",
      "Epoch 00063: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 64/100\n",
      "6750/6750 [==============================] - 0s 25us/step - loss: 0.0156 - mean_squared_error: 0.0039 - val_loss: 0.0221 - val_mean_squared_error: 0.0104\n",
      "Epoch 65/100\n",
      "6750/6750 [==============================] - 0s 26us/step - loss: 0.0156 - mean_squared_error: 0.0039 - val_loss: 0.0221 - val_mean_squared_error: 0.0104\n",
      "Epoch 66/100\n",
      "6750/6750 [==============================] - 0s 26us/step - loss: 0.0156 - mean_squared_error: 0.0039 - val_loss: 0.0221 - val_mean_squared_error: 0.0104\n",
      "Epoch 67/100\n",
      "6750/6750 [==============================] - 0s 25us/step - loss: 0.0156 - mean_squared_error: 0.0039 - val_loss: 0.0221 - val_mean_squared_error: 0.0104\n",
      "Epoch 68/100\n",
      "6750/6750 [==============================] - 0s 24us/step - loss: 0.0156 - mean_squared_error: 0.0039 - val_loss: 0.0221 - val_mean_squared_error: 0.0104\n",
      "\n",
      "Epoch 00068: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 69/100\n",
      "6750/6750 [==============================] - 0s 24us/step - loss: 0.0155 - mean_squared_error: 0.0038 - val_loss: 0.0220 - val_mean_squared_error: 0.0104\n",
      "Epoch 70/100\n",
      "6750/6750 [==============================] - 0s 24us/step - loss: 0.0155 - mean_squared_error: 0.0038 - val_loss: 0.0220 - val_mean_squared_error: 0.0103\n",
      "Epoch 71/100\n",
      "6750/6750 [==============================] - 0s 24us/step - loss: 0.0155 - mean_squared_error: 0.0038 - val_loss: 0.0220 - val_mean_squared_error: 0.0104\n",
      "Epoch 72/100\n",
      "6750/6750 [==============================] - 0s 25us/step - loss: 0.0155 - mean_squared_error: 0.0038 - val_loss: 0.0220 - val_mean_squared_error: 0.0103\n",
      "Epoch 73/100\n",
      "6750/6750 [==============================] - 0s 26us/step - loss: 0.0155 - mean_squared_error: 0.0038 - val_loss: 0.0220 - val_mean_squared_error: 0.0104\n",
      "\n",
      "Epoch 00073: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 74/100\n",
      "6750/6750 [==============================] - 0s 24us/step - loss: 0.0154 - mean_squared_error: 0.0038 - val_loss: 0.0221 - val_mean_squared_error: 0.0104\n",
      "Epoch 75/100\n",
      "6750/6750 [==============================] - 0s 26us/step - loss: 0.0154 - mean_squared_error: 0.0038 - val_loss: 0.0220 - val_mean_squared_error: 0.0104\n",
      "Epoch 76/100\n",
      "6750/6750 [==============================] - 0s 26us/step - loss: 0.0154 - mean_squared_error: 0.0038 - val_loss: 0.0220 - val_mean_squared_error: 0.0103\n",
      "Epoch 77/100\n",
      "6750/6750 [==============================] - 0s 32us/step - loss: 0.0154 - mean_squared_error: 0.0038 - val_loss: 0.0220 - val_mean_squared_error: 0.0104\n",
      "Epoch 78/100\n",
      "6750/6750 [==============================] - 0s 23us/step - loss: 0.0154 - mean_squared_error: 0.0038 - val_loss: 0.0220 - val_mean_squared_error: 0.0103\n",
      "\n",
      "Epoch 00078: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "Epoch 79/100\n",
      "6750/6750 [==============================] - 0s 25us/step - loss: 0.0154 - mean_squared_error: 0.0037 - val_loss: 0.0219 - val_mean_squared_error: 0.0103\n",
      "Epoch 80/100\n",
      "6750/6750 [==============================] - 0s 28us/step - loss: 0.0154 - mean_squared_error: 0.0037 - val_loss: 0.0220 - val_mean_squared_error: 0.0103\n",
      "Epoch 81/100\n",
      "6750/6750 [==============================] - 0s 24us/step - loss: 0.0154 - mean_squared_error: 0.0037 - val_loss: 0.0219 - val_mean_squared_error: 0.0103\n",
      "Epoch 82/100\n",
      "6750/6750 [==============================] - 0s 24us/step - loss: 0.0154 - mean_squared_error: 0.0037 - val_loss: 0.0220 - val_mean_squared_error: 0.0103\n",
      "Epoch 83/100\n",
      "6750/6750 [==============================] - 0s 28us/step - loss: 0.0154 - mean_squared_error: 0.0037 - val_loss: 0.0220 - val_mean_squared_error: 0.0103\n",
      "Epoch 84/100\n",
      "6750/6750 [==============================] - 0s 26us/step - loss: 0.0154 - mean_squared_error: 0.0037 - val_loss: 0.0220 - val_mean_squared_error: 0.0103\n",
      "\n",
      "Epoch 00084: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "Epoch 85/100\n",
      "6750/6750 [==============================] - 0s 25us/step - loss: 0.0154 - mean_squared_error: 0.0037 - val_loss: 0.0220 - val_mean_squared_error: 0.0103\n",
      "Epoch 86/100\n",
      "6750/6750 [==============================] - 0s 25us/step - loss: 0.0154 - mean_squared_error: 0.0037 - val_loss: 0.0220 - val_mean_squared_error: 0.0103\n",
      "Epoch 87/100\n",
      "6750/6750 [==============================] - 0s 24us/step - loss: 0.0154 - mean_squared_error: 0.0037 - val_loss: 0.0220 - val_mean_squared_error: 0.0103\n",
      "Epoch 88/100\n",
      "6750/6750 [==============================] - 0s 31us/step - loss: 0.0154 - mean_squared_error: 0.0037 - val_loss: 0.0220 - val_mean_squared_error: 0.0103\n",
      "Epoch 89/100\n",
      "6750/6750 [==============================] - 0s 35us/step - loss: 0.0154 - mean_squared_error: 0.0037 - val_loss: 0.0220 - val_mean_squared_error: 0.0103\n",
      "\n",
      "Epoch 00089: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "Epoch 90/100\n",
      "6750/6750 [==============================] - 0s 64us/step - loss: 0.0154 - mean_squared_error: 0.0037 - val_loss: 0.0220 - val_mean_squared_error: 0.0103\n",
      "Epoch 91/100\n",
      "6750/6750 [==============================] - 0s 49us/step - loss: 0.0154 - mean_squared_error: 0.0037 - val_loss: 0.0220 - val_mean_squared_error: 0.0103\n",
      "Epoch 92/100\n",
      "6750/6750 [==============================] - 0s 33us/step - loss: 0.0154 - mean_squared_error: 0.0037 - val_loss: 0.0220 - val_mean_squared_error: 0.0103\n",
      "Epoch 93/100\n",
      "6750/6750 [==============================] - 0s 25us/step - loss: 0.0154 - mean_squared_error: 0.0037 - val_loss: 0.0219 - val_mean_squared_error: 0.0103\n",
      "Epoch 94/100\n",
      "6750/6750 [==============================] - 0s 25us/step - loss: 0.0154 - mean_squared_error: 0.0037 - val_loss: 0.0220 - val_mean_squared_error: 0.0103\n",
      "\n",
      "Epoch 00094: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "Epoch 95/100\n",
      "6750/6750 [==============================] - 0s 27us/step - loss: 0.0154 - mean_squared_error: 0.0037 - val_loss: 0.0220 - val_mean_squared_error: 0.0103\n",
      "Epoch 96/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6750/6750 [==============================] - 0s 26us/step - loss: 0.0154 - mean_squared_error: 0.0037 - val_loss: 0.0220 - val_mean_squared_error: 0.0103\n",
      "Epoch 97/100\n",
      "6750/6750 [==============================] - 0s 24us/step - loss: 0.0154 - mean_squared_error: 0.0037 - val_loss: 0.0220 - val_mean_squared_error: 0.0103\n",
      "Epoch 98/100\n",
      "6750/6750 [==============================] - 0s 23us/step - loss: 0.0154 - mean_squared_error: 0.0037 - val_loss: 0.0220 - val_mean_squared_error: 0.0103\n",
      "Epoch 99/100\n",
      "6750/6750 [==============================] - 0s 23us/step - loss: 0.0154 - mean_squared_error: 0.0037 - val_loss: 0.0220 - val_mean_squared_error: 0.0103\n",
      "\n",
      "Epoch 00099: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
      "Epoch 100/100\n",
      "6750/6750 [==============================] - 0s 23us/step - loss: 0.0154 - mean_squared_error: 0.0037 - val_loss: 0.0220 - val_mean_squared_error: 0.0103\n",
      "hw1/models/model_RoboschoolReacher-v1_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm/base.hdf5\n",
      "Done training model for RoboschoolReacher-v1\n",
      "{'model_name': 'model_RoboschoolReacher-v1_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm', 'model_path': 'hw1/models/model_RoboschoolReacher-v1_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm', 'test_mse': 0.010304973950460067, 'train_mse': 0.003708159562594425, 'dataset_name': 'RoboschoolReacher-v1'}\n",
      "Training a FC ANN for env RoboschoolHopper-v1\n",
      "loading dataset RoboschoolHopper-v1\n",
      "Domain name: RoboschoolHopper-v1\n",
      "(32200, 15) (3578, 15) (32200, 3) (3578, 3)\n",
      "model_name='model_RoboschoolHopper-v1_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm'\n",
      "model_path='hw1/models/model_RoboschoolHopper-v1_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm'\n",
      "Train on 32200 samples, validate on 3578 samples\n",
      "Epoch 1/100\n",
      "32200/32200 [==============================] - 1s 31us/step - loss: 0.3089 - mean_squared_error: 0.2888 - val_loss: 0.1208 - val_mean_squared_error: 0.0997\n",
      "Epoch 2/100\n",
      "32200/32200 [==============================] - 1s 23us/step - loss: 0.0952 - mean_squared_error: 0.0730 - val_loss: 0.0775 - val_mean_squared_error: 0.0547\n",
      "Epoch 3/100\n",
      "32200/32200 [==============================] - 1s 24us/step - loss: 0.0673 - mean_squared_error: 0.0443 - val_loss: 0.0662 - val_mean_squared_error: 0.0432\n",
      "Epoch 4/100\n",
      "32200/32200 [==============================] - 1s 23us/step - loss: 0.0573 - mean_squared_error: 0.0344 - val_loss: 0.0581 - val_mean_squared_error: 0.0355\n",
      "Epoch 5/100\n",
      "32200/32200 [==============================] - 1s 27us/step - loss: 0.0510 - mean_squared_error: 0.0287 - val_loss: 0.0486 - val_mean_squared_error: 0.0266\n",
      "Epoch 6/100\n",
      "32200/32200 [==============================] - 1s 25us/step - loss: 0.0480 - mean_squared_error: 0.0262 - val_loss: 0.0512 - val_mean_squared_error: 0.0298\n",
      "Epoch 7/100\n",
      "32200/32200 [==============================] - 1s 24us/step - loss: 0.0451 - mean_squared_error: 0.0240 - val_loss: 0.0430 - val_mean_squared_error: 0.0221\n",
      "Epoch 8/100\n",
      "32200/32200 [==============================] - 1s 25us/step - loss: 0.0418 - mean_squared_error: 0.0213 - val_loss: 0.0390 - val_mean_squared_error: 0.0188\n",
      "Epoch 9/100\n",
      "32200/32200 [==============================] - 1s 24us/step - loss: 0.0399 - mean_squared_error: 0.0200 - val_loss: 0.0390 - val_mean_squared_error: 0.0194\n",
      "Epoch 10/100\n",
      "32200/32200 [==============================] - 1s 24us/step - loss: 0.0389 - mean_squared_error: 0.0196 - val_loss: 0.0395 - val_mean_squared_error: 0.0205\n",
      "Epoch 11/100\n",
      "32200/32200 [==============================] - 1s 23us/step - loss: 0.0362 - mean_squared_error: 0.0174 - val_loss: 0.0400 - val_mean_squared_error: 0.0214\n",
      "Epoch 12/100\n",
      "32200/32200 [==============================] - 1s 24us/step - loss: 0.0357 - mean_squared_error: 0.0174 - val_loss: 0.0361 - val_mean_squared_error: 0.0181\n",
      "Epoch 13/100\n",
      "32200/32200 [==============================] - 1s 24us/step - loss: 0.0348 - mean_squared_error: 0.0170 - val_loss: 0.0343 - val_mean_squared_error: 0.0168\n",
      "Epoch 14/100\n",
      "32200/32200 [==============================] - 1s 23us/step - loss: 0.0341 - mean_squared_error: 0.0167 - val_loss: 0.0384 - val_mean_squared_error: 0.0212\n",
      "Epoch 15/100\n",
      "32200/32200 [==============================] - 1s 23us/step - loss: 0.0333 - mean_squared_error: 0.0163 - val_loss: 0.0334 - val_mean_squared_error: 0.0166\n",
      "Epoch 16/100\n",
      "32200/32200 [==============================] - 1s 23us/step - loss: 0.0316 - mean_squared_error: 0.0150 - val_loss: 0.0323 - val_mean_squared_error: 0.0159\n",
      "Epoch 17/100\n",
      "32200/32200 [==============================] - 1s 23us/step - loss: 0.0314 - mean_squared_error: 0.0151 - val_loss: 0.0323 - val_mean_squared_error: 0.0163\n",
      "Epoch 18/100\n",
      "32200/32200 [==============================] - 1s 24us/step - loss: 0.0308 - mean_squared_error: 0.0149 - val_loss: 0.0351 - val_mean_squared_error: 0.0194\n",
      "Epoch 19/100\n",
      "32200/32200 [==============================] - 1s 25us/step - loss: 0.0302 - mean_squared_error: 0.0145 - val_loss: 0.0345 - val_mean_squared_error: 0.0190\n",
      "Epoch 20/100\n",
      "32200/32200 [==============================] - 1s 24us/step - loss: 0.0292 - mean_squared_error: 0.0138 - val_loss: 0.0283 - val_mean_squared_error: 0.0131\n",
      "Epoch 21/100\n",
      "32200/32200 [==============================] - 1s 24us/step - loss: 0.0291 - mean_squared_error: 0.0140 - val_loss: 0.0339 - val_mean_squared_error: 0.0189\n",
      "Epoch 22/100\n",
      "32200/32200 [==============================] - 1s 24us/step - loss: 0.0280 - mean_squared_error: 0.0132 - val_loss: 0.0284 - val_mean_squared_error: 0.0137\n",
      "Epoch 23/100\n",
      "32200/32200 [==============================] - 1s 24us/step - loss: 0.0276 - mean_squared_error: 0.0131 - val_loss: 0.0283 - val_mean_squared_error: 0.0138\n",
      "Epoch 24/100\n",
      "32200/32200 [==============================] - 1s 24us/step - loss: 0.0278 - mean_squared_error: 0.0134 - val_loss: 0.0251 - val_mean_squared_error: 0.0109\n",
      "Epoch 25/100\n",
      "32200/32200 [==============================] - 1s 23us/step - loss: 0.0275 - mean_squared_error: 0.0134 - val_loss: 0.0278 - val_mean_squared_error: 0.0138\n",
      "Epoch 26/100\n",
      "32200/32200 [==============================] - 1s 23us/step - loss: 0.0263 - mean_squared_error: 0.0124 - val_loss: 0.0305 - val_mean_squared_error: 0.0167\n",
      "Epoch 27/100\n",
      "32200/32200 [==============================] - 1s 23us/step - loss: 0.0261 - mean_squared_error: 0.0124 - val_loss: 0.0286 - val_mean_squared_error: 0.0150\n",
      "Epoch 28/100\n",
      "32200/32200 [==============================] - 1s 23us/step - loss: 0.0257 - mean_squared_error: 0.0122 - val_loss: 0.0265 - val_mean_squared_error: 0.0131\n",
      "Epoch 29/100\n",
      "32200/32200 [==============================] - 1s 24us/step - loss: 0.0253 - mean_squared_error: 0.0120 - val_loss: 0.0286 - val_mean_squared_error: 0.0154\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 30/100\n",
      "32200/32200 [==============================] - 1s 25us/step - loss: 0.0227 - mean_squared_error: 0.0096 - val_loss: 0.0225 - val_mean_squared_error: 0.0094\n",
      "Epoch 31/100\n",
      "32200/32200 [==============================] - 1s 26us/step - loss: 0.0221 - mean_squared_error: 0.0091 - val_loss: 0.0247 - val_mean_squared_error: 0.0117\n",
      "Epoch 32/100\n",
      "32200/32200 [==============================] - 1s 24us/step - loss: 0.0220 - mean_squared_error: 0.0091 - val_loss: 0.0221 - val_mean_squared_error: 0.0092\n",
      "Epoch 33/100\n",
      "32200/32200 [==============================] - 1s 25us/step - loss: 0.0217 - mean_squared_error: 0.0089 - val_loss: 0.0222 - val_mean_squared_error: 0.0095\n",
      "Epoch 34/100\n",
      "32200/32200 [==============================] - 1s 28us/step - loss: 0.0218 - mean_squared_error: 0.0091 - val_loss: 0.0219 - val_mean_squared_error: 0.0093\n",
      "Epoch 35/100\n",
      "32200/32200 [==============================] - 1s 24us/step - loss: 0.0214 - mean_squared_error: 0.0089 - val_loss: 0.0218 - val_mean_squared_error: 0.0093\n",
      "Epoch 36/100\n",
      "32200/32200 [==============================] - 1s 24us/step - loss: 0.0214 - mean_squared_error: 0.0090 - val_loss: 0.0214 - val_mean_squared_error: 0.0090\n",
      "Epoch 37/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32200/32200 [==============================] - 1s 24us/step - loss: 0.0212 - mean_squared_error: 0.0089 - val_loss: 0.0215 - val_mean_squared_error: 0.0092\n",
      "Epoch 38/100\n",
      "32200/32200 [==============================] - 1s 23us/step - loss: 0.0208 - mean_squared_error: 0.0086 - val_loss: 0.0213 - val_mean_squared_error: 0.0091\n",
      "Epoch 39/100\n",
      "32200/32200 [==============================] - 1s 24us/step - loss: 0.0212 - mean_squared_error: 0.0091 - val_loss: 0.0217 - val_mean_squared_error: 0.0096\n",
      "Epoch 40/100\n",
      "32200/32200 [==============================] - 1s 25us/step - loss: 0.0205 - mean_squared_error: 0.0085 - val_loss: 0.0209 - val_mean_squared_error: 0.0090\n",
      "Epoch 41/100\n",
      "32200/32200 [==============================] - 1s 24us/step - loss: 0.0209 - mean_squared_error: 0.0090 - val_loss: 0.0209 - val_mean_squared_error: 0.0090\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 42/100\n",
      "32200/32200 [==============================] - 1s 24us/step - loss: 0.0191 - mean_squared_error: 0.0073 - val_loss: 0.0193 - val_mean_squared_error: 0.0075\n",
      "Epoch 43/100\n",
      "32200/32200 [==============================] - 1s 24us/step - loss: 0.0190 - mean_squared_error: 0.0073 - val_loss: 0.0197 - val_mean_squared_error: 0.0080\n",
      "Epoch 44/100\n",
      "32200/32200 [==============================] - 1s 25us/step - loss: 0.0190 - mean_squared_error: 0.0073 - val_loss: 0.0204 - val_mean_squared_error: 0.0087\n",
      "Epoch 45/100\n",
      "32200/32200 [==============================] - 1s 26us/step - loss: 0.0190 - mean_squared_error: 0.0073 - val_loss: 0.0192 - val_mean_squared_error: 0.0076\n",
      "Epoch 46/100\n",
      "32200/32200 [==============================] - 1s 25us/step - loss: 0.0188 - mean_squared_error: 0.0072 - val_loss: 0.0196 - val_mean_squared_error: 0.0080\n",
      "Epoch 47/100\n",
      "32200/32200 [==============================] - 1s 24us/step - loss: 0.0188 - mean_squared_error: 0.0072 - val_loss: 0.0197 - val_mean_squared_error: 0.0081\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 48/100\n",
      "32200/32200 [==============================] - 1s 24us/step - loss: 0.0181 - mean_squared_error: 0.0066 - val_loss: 0.0186 - val_mean_squared_error: 0.0071\n",
      "Epoch 49/100\n",
      "32200/32200 [==============================] - 1s 24us/step - loss: 0.0180 - mean_squared_error: 0.0066 - val_loss: 0.0188 - val_mean_squared_error: 0.0073\n",
      "Epoch 50/100\n",
      "32200/32200 [==============================] - 1s 24us/step - loss: 0.0180 - mean_squared_error: 0.0066 - val_loss: 0.0188 - val_mean_squared_error: 0.0073\n",
      "Epoch 51/100\n",
      "32200/32200 [==============================] - 1s 24us/step - loss: 0.0180 - mean_squared_error: 0.0066 - val_loss: 0.0184 - val_mean_squared_error: 0.0070\n",
      "Epoch 52/100\n",
      "32200/32200 [==============================] - 1s 23us/step - loss: 0.0179 - mean_squared_error: 0.0065 - val_loss: 0.0184 - val_mean_squared_error: 0.0071\n",
      "Epoch 53/100\n",
      "32200/32200 [==============================] - 1s 25us/step - loss: 0.0179 - mean_squared_error: 0.0065 - val_loss: 0.0185 - val_mean_squared_error: 0.0071\n",
      "\n",
      "Epoch 00053: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 54/100\n",
      "32200/32200 [==============================] - 1s 23us/step - loss: 0.0176 - mean_squared_error: 0.0062 - val_loss: 0.0184 - val_mean_squared_error: 0.0071\n",
      "Epoch 55/100\n",
      "32200/32200 [==============================] - 1s 23us/step - loss: 0.0175 - mean_squared_error: 0.0062 - val_loss: 0.0182 - val_mean_squared_error: 0.0069\n",
      "Epoch 56/100\n",
      "32200/32200 [==============================] - 1s 24us/step - loss: 0.0175 - mean_squared_error: 0.0062 - val_loss: 0.0182 - val_mean_squared_error: 0.0069\n",
      "Epoch 57/100\n",
      "32200/32200 [==============================] - 1s 24us/step - loss: 0.0174 - mean_squared_error: 0.0062 - val_loss: 0.0182 - val_mean_squared_error: 0.0069\n",
      "Epoch 58/100\n",
      "32200/32200 [==============================] - 1s 24us/step - loss: 0.0174 - mean_squared_error: 0.0061 - val_loss: 0.0181 - val_mean_squared_error: 0.0068\n",
      "Epoch 59/100\n",
      "32200/32200 [==============================] - 1s 23us/step - loss: 0.0174 - mean_squared_error: 0.0061 - val_loss: 0.0181 - val_mean_squared_error: 0.0068\n",
      "Epoch 60/100\n",
      "32200/32200 [==============================] - 1s 23us/step - loss: 0.0173 - mean_squared_error: 0.0061 - val_loss: 0.0182 - val_mean_squared_error: 0.0070\n",
      "Epoch 61/100\n",
      "32200/32200 [==============================] - 1s 23us/step - loss: 0.0173 - mean_squared_error: 0.0061 - val_loss: 0.0181 - val_mean_squared_error: 0.0069\n",
      "Epoch 62/100\n",
      "32200/32200 [==============================] - 1s 23us/step - loss: 0.0173 - mean_squared_error: 0.0061 - val_loss: 0.0181 - val_mean_squared_error: 0.0069\n",
      "Epoch 63/100\n",
      "32200/32200 [==============================] - 1s 24us/step - loss: 0.0173 - mean_squared_error: 0.0061 - val_loss: 0.0182 - val_mean_squared_error: 0.0070\n",
      "\n",
      "Epoch 00063: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 64/100\n",
      "32200/32200 [==============================] - 1s 24us/step - loss: 0.0171 - mean_squared_error: 0.0059 - val_loss: 0.0178 - val_mean_squared_error: 0.0067\n",
      "Epoch 65/100\n",
      "32200/32200 [==============================] - 1s 24us/step - loss: 0.0171 - mean_squared_error: 0.0059 - val_loss: 0.0178 - val_mean_squared_error: 0.0066\n",
      "Epoch 66/100\n",
      "32200/32200 [==============================] - 1s 25us/step - loss: 0.0171 - mean_squared_error: 0.0059 - val_loss: 0.0178 - val_mean_squared_error: 0.0067\n",
      "Epoch 67/100\n",
      "32200/32200 [==============================] - 1s 23us/step - loss: 0.0170 - mean_squared_error: 0.0059 - val_loss: 0.0179 - val_mean_squared_error: 0.0068\n",
      "Epoch 68/100\n",
      "32200/32200 [==============================] - 1s 25us/step - loss: 0.0170 - mean_squared_error: 0.0059 - val_loss: 0.0179 - val_mean_squared_error: 0.0067\n",
      "Epoch 69/100\n",
      "32200/32200 [==============================] - 1s 23us/step - loss: 0.0170 - mean_squared_error: 0.0059 - val_loss: 0.0179 - val_mean_squared_error: 0.0067\n",
      "\n",
      "Epoch 00069: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 70/100\n",
      "32200/32200 [==============================] - 1s 24us/step - loss: 0.0169 - mean_squared_error: 0.0058 - val_loss: 0.0178 - val_mean_squared_error: 0.0066\n",
      "Epoch 71/100\n",
      "32200/32200 [==============================] - 1s 24us/step - loss: 0.0169 - mean_squared_error: 0.0058 - val_loss: 0.0178 - val_mean_squared_error: 0.0067\n",
      "Epoch 72/100\n",
      "32200/32200 [==============================] - 1s 24us/step - loss: 0.0169 - mean_squared_error: 0.0058 - val_loss: 0.0178 - val_mean_squared_error: 0.0066\n",
      "Epoch 73/100\n",
      "32200/32200 [==============================] - 1s 26us/step - loss: 0.0169 - mean_squared_error: 0.0058 - val_loss: 0.0178 - val_mean_squared_error: 0.0066\n",
      "Epoch 74/100\n",
      "32200/32200 [==============================] - 1s 24us/step - loss: 0.0169 - mean_squared_error: 0.0058 - val_loss: 0.0177 - val_mean_squared_error: 0.0066\n",
      "\n",
      "Epoch 00074: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 75/100\n",
      "32200/32200 [==============================] - 1s 25us/step - loss: 0.0168 - mean_squared_error: 0.0057 - val_loss: 0.0177 - val_mean_squared_error: 0.0066\n",
      "Epoch 76/100\n",
      "32200/32200 [==============================] - 1s 23us/step - loss: 0.0168 - mean_squared_error: 0.0057 - val_loss: 0.0177 - val_mean_squared_error: 0.0066\n",
      "Epoch 77/100\n",
      "32200/32200 [==============================] - 1s 24us/step - loss: 0.0168 - mean_squared_error: 0.0057 - val_loss: 0.0177 - val_mean_squared_error: 0.0066\n",
      "Epoch 78/100\n",
      "32200/32200 [==============================] - 1s 23us/step - loss: 0.0168 - mean_squared_error: 0.0057 - val_loss: 0.0177 - val_mean_squared_error: 0.0066\n",
      "Epoch 79/100\n",
      "32200/32200 [==============================] - 1s 24us/step - loss: 0.0168 - mean_squared_error: 0.0057 - val_loss: 0.0177 - val_mean_squared_error: 0.0066\n",
      "\n",
      "Epoch 00079: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "Epoch 80/100\n",
      "32200/32200 [==============================] - 1s 24us/step - loss: 0.0168 - mean_squared_error: 0.0057 - val_loss: 0.0177 - val_mean_squared_error: 0.0066\n",
      "Epoch 81/100\n",
      "32200/32200 [==============================] - 1s 25us/step - loss: 0.0168 - mean_squared_error: 0.0057 - val_loss: 0.0177 - val_mean_squared_error: 0.0066\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32200/32200 [==============================] - 1s 25us/step - loss: 0.0168 - mean_squared_error: 0.0057 - val_loss: 0.0176 - val_mean_squared_error: 0.0066\n",
      "Epoch 83/100\n",
      "32200/32200 [==============================] - 1s 24us/step - loss: 0.0168 - mean_squared_error: 0.0057 - val_loss: 0.0177 - val_mean_squared_error: 0.0066\n",
      "Epoch 84/100\n",
      "32200/32200 [==============================] - 1s 24us/step - loss: 0.0168 - mean_squared_error: 0.0057 - val_loss: 0.0177 - val_mean_squared_error: 0.0066\n",
      "Epoch 85/100\n",
      "32200/32200 [==============================] - 1s 24us/step - loss: 0.0168 - mean_squared_error: 0.0057 - val_loss: 0.0177 - val_mean_squared_error: 0.0066\n",
      "Epoch 86/100\n",
      "32200/32200 [==============================] - 1s 24us/step - loss: 0.0168 - mean_squared_error: 0.0057 - val_loss: 0.0177 - val_mean_squared_error: 0.0066\n",
      "\n",
      "Epoch 00086: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "Epoch 87/100\n",
      "32200/32200 [==============================] - 1s 27us/step - loss: 0.0168 - mean_squared_error: 0.0057 - val_loss: 0.0176 - val_mean_squared_error: 0.0066\n",
      "Epoch 88/100\n",
      "32200/32200 [==============================] - 1s 26us/step - loss: 0.0168 - mean_squared_error: 0.0057 - val_loss: 0.0177 - val_mean_squared_error: 0.0066\n",
      "Epoch 89/100\n",
      "32200/32200 [==============================] - 1s 25us/step - loss: 0.0168 - mean_squared_error: 0.0057 - val_loss: 0.0176 - val_mean_squared_error: 0.0066\n",
      "Epoch 90/100\n",
      "32200/32200 [==============================] - 1s 23us/step - loss: 0.0168 - mean_squared_error: 0.0057 - val_loss: 0.0176 - val_mean_squared_error: 0.0066\n",
      "Epoch 91/100\n",
      "32200/32200 [==============================] - 1s 28us/step - loss: 0.0168 - mean_squared_error: 0.0057 - val_loss: 0.0176 - val_mean_squared_error: 0.0066\n",
      "\n",
      "Epoch 00091: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "Epoch 92/100\n",
      "32200/32200 [==============================] - 1s 24us/step - loss: 0.0168 - mean_squared_error: 0.0057 - val_loss: 0.0176 - val_mean_squared_error: 0.0066\n",
      "Epoch 93/100\n",
      "32200/32200 [==============================] - 1s 24us/step - loss: 0.0168 - mean_squared_error: 0.0057 - val_loss: 0.0176 - val_mean_squared_error: 0.0066\n",
      "Epoch 94/100\n",
      "32200/32200 [==============================] - 1s 23us/step - loss: 0.0168 - mean_squared_error: 0.0057 - val_loss: 0.0176 - val_mean_squared_error: 0.0066\n",
      "Epoch 95/100\n",
      "32200/32200 [==============================] - 1s 23us/step - loss: 0.0168 - mean_squared_error: 0.0057 - val_loss: 0.0176 - val_mean_squared_error: 0.0066\n",
      "Epoch 96/100\n",
      "32200/32200 [==============================] - 1s 23us/step - loss: 0.0168 - mean_squared_error: 0.0057 - val_loss: 0.0176 - val_mean_squared_error: 0.0066\n",
      "\n",
      "Epoch 00096: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "Epoch 97/100\n",
      "32200/32200 [==============================] - 1s 24us/step - loss: 0.0167 - mean_squared_error: 0.0057 - val_loss: 0.0176 - val_mean_squared_error: 0.0066\n",
      "Epoch 98/100\n",
      "32200/32200 [==============================] - 1s 23us/step - loss: 0.0167 - mean_squared_error: 0.0057 - val_loss: 0.0176 - val_mean_squared_error: 0.0066\n",
      "Epoch 99/100\n",
      "32200/32200 [==============================] - 1s 24us/step - loss: 0.0167 - mean_squared_error: 0.0057 - val_loss: 0.0176 - val_mean_squared_error: 0.0066\n",
      "Epoch 100/100\n",
      "32200/32200 [==============================] - 1s 25us/step - loss: 0.0167 - mean_squared_error: 0.0057 - val_loss: 0.0176 - val_mean_squared_error: 0.0066\n",
      "hw1/models/model_RoboschoolHopper-v1_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm/base.hdf5\n",
      "Done training model for RoboschoolHopper-v1\n",
      "{'model_name': 'model_RoboschoolHopper-v1_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm', 'model_path': 'hw1/models/model_RoboschoolHopper-v1_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm', 'test_mse': 0.00656041671302159, 'train_mse': 0.005662971807407676, 'dataset_name': 'RoboschoolHopper-v1'}\n",
      "Training a FC ANN for env RoboschoolWalker2d-v1\n",
      "loading dataset RoboschoolWalker2d-v1\n",
      "Domain name: RoboschoolWalker2d-v1\n",
      "(45000, 22) (5000, 22) (45000, 6) (5000, 6)\n",
      "model_name='model_RoboschoolWalker2d-v1_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm'\n",
      "model_path='hw1/models/model_RoboschoolWalker2d-v1_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm'\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/100\n",
      "45000/45000 [==============================] - 1s 31us/step - loss: 0.1106 - mean_squared_error: 0.0903 - val_loss: 0.0607 - val_mean_squared_error: 0.0399\n",
      "Epoch 2/100\n",
      "45000/45000 [==============================] - 1s 25us/step - loss: 0.0514 - mean_squared_error: 0.0308 - val_loss: 0.0473 - val_mean_squared_error: 0.0271\n",
      "Epoch 3/100\n",
      "45000/45000 [==============================] - 1s 28us/step - loss: 0.0429 - mean_squared_error: 0.0232 - val_loss: 0.0408 - val_mean_squared_error: 0.0218\n",
      "Epoch 4/100\n",
      "45000/45000 [==============================] - 2s 38us/step - loss: 0.0386 - mean_squared_error: 0.0201 - val_loss: 0.0390 - val_mean_squared_error: 0.0210\n",
      "Epoch 5/100\n",
      "45000/45000 [==============================] - 2s 34us/step - loss: 0.0353 - mean_squared_error: 0.0178 - val_loss: 0.0350 - val_mean_squared_error: 0.0180\n",
      "Epoch 6/100\n",
      "45000/45000 [==============================] - 1s 28us/step - loss: 0.0329 - mean_squared_error: 0.0163 - val_loss: 0.0347 - val_mean_squared_error: 0.0186\n",
      "Epoch 7/100\n",
      "45000/45000 [==============================] - 1s 30us/step - loss: 0.0311 - mean_squared_error: 0.0154 - val_loss: 0.0323 - val_mean_squared_error: 0.0169\n",
      "Epoch 8/100\n",
      "45000/45000 [==============================] - 1s 28us/step - loss: 0.0296 - mean_squared_error: 0.0147 - val_loss: 0.0323 - val_mean_squared_error: 0.0177\n",
      "Epoch 9/100\n",
      "45000/45000 [==============================] - 1s 29us/step - loss: 0.0284 - mean_squared_error: 0.0141 - val_loss: 0.0280 - val_mean_squared_error: 0.0140\n",
      "Epoch 10/100\n",
      "45000/45000 [==============================] - 1s 29us/step - loss: 0.0273 - mean_squared_error: 0.0135 - val_loss: 0.0284 - val_mean_squared_error: 0.0149\n",
      "Epoch 11/100\n",
      "45000/45000 [==============================] - 1s 28us/step - loss: 0.0264 - mean_squared_error: 0.0131 - val_loss: 0.0273 - val_mean_squared_error: 0.0143\n",
      "Epoch 12/100\n",
      "45000/45000 [==============================] - 1s 32us/step - loss: 0.0255 - mean_squared_error: 0.0127 - val_loss: 0.0254 - val_mean_squared_error: 0.0128\n",
      "Epoch 13/100\n",
      "45000/45000 [==============================] - 1s 30us/step - loss: 0.0248 - mean_squared_error: 0.0124 - val_loss: 0.0260 - val_mean_squared_error: 0.0138\n",
      "Epoch 14/100\n",
      "45000/45000 [==============================] - 1s 28us/step - loss: 0.0242 - mean_squared_error: 0.0121 - val_loss: 0.0287 - val_mean_squared_error: 0.0168\n",
      "Epoch 15/100\n",
      "45000/45000 [==============================] - 1s 28us/step - loss: 0.0235 - mean_squared_error: 0.0118 - val_loss: 0.0235 - val_mean_squared_error: 0.0119\n",
      "Epoch 16/100\n",
      "45000/45000 [==============================] - 1s 28us/step - loss: 0.0231 - mean_squared_error: 0.0117 - val_loss: 0.0223 - val_mean_squared_error: 0.0110\n",
      "Epoch 17/100\n",
      "45000/45000 [==============================] - 1s 29us/step - loss: 0.0227 - mean_squared_error: 0.0115 - val_loss: 0.0255 - val_mean_squared_error: 0.0144\n",
      "Epoch 18/100\n",
      "45000/45000 [==============================] - 1s 31us/step - loss: 0.0222 - mean_squared_error: 0.0113 - val_loss: 0.0218 - val_mean_squared_error: 0.0110\n",
      "Epoch 19/100\n",
      "45000/45000 [==============================] - 1s 28us/step - loss: 0.0217 - mean_squared_error: 0.0110 - val_loss: 0.0219 - val_mean_squared_error: 0.0113\n",
      "Epoch 20/100\n",
      "45000/45000 [==============================] - 1s 28us/step - loss: 0.0213 - mean_squared_error: 0.0109 - val_loss: 0.0210 - val_mean_squared_error: 0.0107\n",
      "Epoch 21/100\n",
      "45000/45000 [==============================] - 1s 28us/step - loss: 0.0211 - mean_squared_error: 0.0108 - val_loss: 0.0210 - val_mean_squared_error: 0.0108\n",
      "Epoch 22/100\n",
      "45000/45000 [==============================] - 1s 28us/step - loss: 0.0206 - mean_squared_error: 0.0105 - val_loss: 0.0241 - val_mean_squared_error: 0.0141\n",
      "Epoch 23/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000/45000 [==============================] - 1s 28us/step - loss: 0.0203 - mean_squared_error: 0.0103 - val_loss: 0.0218 - val_mean_squared_error: 0.0119\n",
      "Epoch 24/100\n",
      "45000/45000 [==============================] - 1s 28us/step - loss: 0.0201 - mean_squared_error: 0.0103 - val_loss: 0.0214 - val_mean_squared_error: 0.0116\n",
      "Epoch 25/100\n",
      "45000/45000 [==============================] - 1s 28us/step - loss: 0.0199 - mean_squared_error: 0.0102 - val_loss: 0.0220 - val_mean_squared_error: 0.0124\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 26/100\n",
      "45000/45000 [==============================] - 1s 29us/step - loss: 0.0182 - mean_squared_error: 0.0087 - val_loss: 0.0184 - val_mean_squared_error: 0.0089\n",
      "Epoch 27/100\n",
      "45000/45000 [==============================] - 1s 28us/step - loss: 0.0181 - mean_squared_error: 0.0087 - val_loss: 0.0195 - val_mean_squared_error: 0.0101\n",
      "Epoch 28/100\n",
      "45000/45000 [==============================] - 1s 28us/step - loss: 0.0180 - mean_squared_error: 0.0086 - val_loss: 0.0184 - val_mean_squared_error: 0.0091\n",
      "Epoch 29/100\n",
      "45000/45000 [==============================] - 1s 27us/step - loss: 0.0178 - mean_squared_error: 0.0085 - val_loss: 0.0192 - val_mean_squared_error: 0.0099\n",
      "Epoch 30/100\n",
      "45000/45000 [==============================] - 1s 28us/step - loss: 0.0177 - mean_squared_error: 0.0085 - val_loss: 0.0184 - val_mean_squared_error: 0.0092\n",
      "Epoch 31/100\n",
      "45000/45000 [==============================] - 1s 28us/step - loss: 0.0176 - mean_squared_error: 0.0084 - val_loss: 0.0179 - val_mean_squared_error: 0.0088\n",
      "Epoch 32/100\n",
      "45000/45000 [==============================] - 1s 29us/step - loss: 0.0174 - mean_squared_error: 0.0083 - val_loss: 0.0175 - val_mean_squared_error: 0.0085\n",
      "Epoch 33/100\n",
      "45000/45000 [==============================] - 1s 28us/step - loss: 0.0173 - mean_squared_error: 0.0083 - val_loss: 0.0199 - val_mean_squared_error: 0.0110\n",
      "Epoch 34/100\n",
      "45000/45000 [==============================] - 1s 28us/step - loss: 0.0172 - mean_squared_error: 0.0083 - val_loss: 0.0181 - val_mean_squared_error: 0.0092\n",
      "Epoch 35/100\n",
      "45000/45000 [==============================] - 1s 28us/step - loss: 0.0170 - mean_squared_error: 0.0081 - val_loss: 0.0169 - val_mean_squared_error: 0.0080\n",
      "Epoch 36/100\n",
      "45000/45000 [==============================] - 1s 29us/step - loss: 0.0169 - mean_squared_error: 0.0081 - val_loss: 0.0174 - val_mean_squared_error: 0.0087\n",
      "Epoch 37/100\n",
      "45000/45000 [==============================] - 1s 28us/step - loss: 0.0168 - mean_squared_error: 0.0081 - val_loss: 0.0172 - val_mean_squared_error: 0.0085\n",
      "Epoch 38/100\n",
      "45000/45000 [==============================] - 1s 29us/step - loss: 0.0167 - mean_squared_error: 0.0080 - val_loss: 0.0170 - val_mean_squared_error: 0.0084\n",
      "Epoch 39/100\n",
      "45000/45000 [==============================] - 1s 30us/step - loss: 0.0165 - mean_squared_error: 0.0079 - val_loss: 0.0170 - val_mean_squared_error: 0.0084\n",
      "Epoch 40/100\n",
      "45000/45000 [==============================] - 1s 29us/step - loss: 0.0164 - mean_squared_error: 0.0079 - val_loss: 0.0179 - val_mean_squared_error: 0.0094\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 41/100\n",
      "45000/45000 [==============================] - 1s 28us/step - loss: 0.0156 - mean_squared_error: 0.0071 - val_loss: 0.0159 - val_mean_squared_error: 0.0074\n",
      "Epoch 42/100\n",
      "45000/45000 [==============================] - 1s 29us/step - loss: 0.0156 - mean_squared_error: 0.0071 - val_loss: 0.0161 - val_mean_squared_error: 0.0076\n",
      "Epoch 43/100\n",
      "45000/45000 [==============================] - 1s 33us/step - loss: 0.0156 - mean_squared_error: 0.0071 - val_loss: 0.0161 - val_mean_squared_error: 0.0076\n",
      "Epoch 44/100\n",
      "45000/45000 [==============================] - 1s 33us/step - loss: 0.0155 - mean_squared_error: 0.0071 - val_loss: 0.0164 - val_mean_squared_error: 0.0080\n",
      "Epoch 45/100\n",
      "45000/45000 [==============================] - 1s 33us/step - loss: 0.0154 - mean_squared_error: 0.0070 - val_loss: 0.0160 - val_mean_squared_error: 0.0076\n",
      "Epoch 46/100\n",
      "45000/45000 [==============================] - 1s 31us/step - loss: 0.0153 - mean_squared_error: 0.0070 - val_loss: 0.0158 - val_mean_squared_error: 0.0074\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 47/100\n",
      "45000/45000 [==============================] - 1s 28us/step - loss: 0.0149 - mean_squared_error: 0.0066 - val_loss: 0.0154 - val_mean_squared_error: 0.0071\n",
      "Epoch 48/100\n",
      "45000/45000 [==============================] - 1s 28us/step - loss: 0.0149 - mean_squared_error: 0.0065 - val_loss: 0.0154 - val_mean_squared_error: 0.0070\n",
      "Epoch 49/100\n",
      "45000/45000 [==============================] - 1s 30us/step - loss: 0.0148 - mean_squared_error: 0.0065 - val_loss: 0.0158 - val_mean_squared_error: 0.0075\n",
      "Epoch 50/100\n",
      "45000/45000 [==============================] - 1s 32us/step - loss: 0.0148 - mean_squared_error: 0.0065 - val_loss: 0.0153 - val_mean_squared_error: 0.0070\n",
      "Epoch 51/100\n",
      "45000/45000 [==============================] - 1s 31us/step - loss: 0.0148 - mean_squared_error: 0.0065 - val_loss: 0.0152 - val_mean_squared_error: 0.0069\n",
      "Epoch 52/100\n",
      "45000/45000 [==============================] - 1s 32us/step - loss: 0.0148 - mean_squared_error: 0.0065 - val_loss: 0.0152 - val_mean_squared_error: 0.0070\n",
      "Epoch 53/100\n",
      "45000/45000 [==============================] - 1s 32us/step - loss: 0.0147 - mean_squared_error: 0.0065 - val_loss: 0.0153 - val_mean_squared_error: 0.0070\n",
      "Epoch 54/100\n",
      "45000/45000 [==============================] - 1s 32us/step - loss: 0.0147 - mean_squared_error: 0.0064 - val_loss: 0.0151 - val_mean_squared_error: 0.0068\n",
      "Epoch 55/100\n",
      "45000/45000 [==============================] - 1s 30us/step - loss: 0.0146 - mean_squared_error: 0.0064 - val_loss: 0.0151 - val_mean_squared_error: 0.0069\n",
      "Epoch 56/100\n",
      "45000/45000 [==============================] - 1s 31us/step - loss: 0.0146 - mean_squared_error: 0.0064 - val_loss: 0.0150 - val_mean_squared_error: 0.0068\n",
      "Epoch 57/100\n",
      "45000/45000 [==============================] - 1s 31us/step - loss: 0.0146 - mean_squared_error: 0.0064 - val_loss: 0.0149 - val_mean_squared_error: 0.0068\n",
      "Epoch 58/100\n",
      "45000/45000 [==============================] - 1s 32us/step - loss: 0.0145 - mean_squared_error: 0.0064 - val_loss: 0.0153 - val_mean_squared_error: 0.0072\n",
      "Epoch 59/100\n",
      "45000/45000 [==============================] - 1s 31us/step - loss: 0.0145 - mean_squared_error: 0.0064 - val_loss: 0.0151 - val_mean_squared_error: 0.0070\n",
      "\n",
      "Epoch 00059: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 60/100\n",
      "45000/45000 [==============================] - 1s 32us/step - loss: 0.0143 - mean_squared_error: 0.0061 - val_loss: 0.0146 - val_mean_squared_error: 0.0065\n",
      "Epoch 61/100\n",
      "45000/45000 [==============================] - 1s 31us/step - loss: 0.0142 - mean_squared_error: 0.0061 - val_loss: 0.0146 - val_mean_squared_error: 0.0065\n",
      "Epoch 62/100\n",
      "45000/45000 [==============================] - 1s 31us/step - loss: 0.0142 - mean_squared_error: 0.0061 - val_loss: 0.0146 - val_mean_squared_error: 0.0065\n",
      "Epoch 63/100\n",
      "45000/45000 [==============================] - 1s 30us/step - loss: 0.0142 - mean_squared_error: 0.0061 - val_loss: 0.0146 - val_mean_squared_error: 0.0065\n",
      "Epoch 64/100\n",
      "45000/45000 [==============================] - 1s 28us/step - loss: 0.0142 - mean_squared_error: 0.0061 - val_loss: 0.0147 - val_mean_squared_error: 0.0066\n",
      "Epoch 65/100\n",
      "45000/45000 [==============================] - 1s 29us/step - loss: 0.0142 - mean_squared_error: 0.0061 - val_loss: 0.0146 - val_mean_squared_error: 0.0065\n",
      "\n",
      "Epoch 00065: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 66/100\n",
      "45000/45000 [==============================] - 1s 30us/step - loss: 0.0140 - mean_squared_error: 0.0060 - val_loss: 0.0144 - val_mean_squared_error: 0.0064\n",
      "Epoch 67/100\n",
      "45000/45000 [==============================] - 1s 29us/step - loss: 0.0140 - mean_squared_error: 0.0060 - val_loss: 0.0144 - val_mean_squared_error: 0.0064\n",
      "Epoch 68/100\n",
      "45000/45000 [==============================] - 1s 28us/step - loss: 0.0140 - mean_squared_error: 0.0060 - val_loss: 0.0144 - val_mean_squared_error: 0.0064\n",
      "Epoch 69/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000/45000 [==============================] - 1s 28us/step - loss: 0.0140 - mean_squared_error: 0.0059 - val_loss: 0.0145 - val_mean_squared_error: 0.0064\n",
      "Epoch 70/100\n",
      "45000/45000 [==============================] - 1s 29us/step - loss: 0.0140 - mean_squared_error: 0.0059 - val_loss: 0.0144 - val_mean_squared_error: 0.0063loss: 0.0140 - mean_squ\n",
      "Epoch 71/100\n",
      "45000/45000 [==============================] - 1s 30us/step - loss: 0.0140 - mean_squared_error: 0.0059 - val_loss: 0.0144 - val_mean_squared_error: 0.0063\n",
      "\n",
      "Epoch 00071: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 72/100\n",
      "45000/45000 [==============================] - 1s 29us/step - loss: 0.0139 - mean_squared_error: 0.0059 - val_loss: 0.0144 - val_mean_squared_error: 0.0063\n",
      "Epoch 73/100\n",
      "45000/45000 [==============================] - 1s 29us/step - loss: 0.0139 - mean_squared_error: 0.0059 - val_loss: 0.0143 - val_mean_squared_error: 0.0063\n",
      "Epoch 74/100\n",
      "45000/45000 [==============================] - 1s 29us/step - loss: 0.0139 - mean_squared_error: 0.0059 - val_loss: 0.0144 - val_mean_squared_error: 0.0064\n",
      "Epoch 75/100\n",
      "45000/45000 [==============================] - 1s 29us/step - loss: 0.0139 - mean_squared_error: 0.0059 - val_loss: 0.0143 - val_mean_squared_error: 0.0063\n",
      "Epoch 76/100\n",
      "45000/45000 [==============================] - 1s 29us/step - loss: 0.0139 - mean_squared_error: 0.0059 - val_loss: 0.0143 - val_mean_squared_error: 0.0063\n",
      "\n",
      "Epoch 00076: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 77/100\n",
      "45000/45000 [==============================] - 1s 30us/step - loss: 0.0139 - mean_squared_error: 0.0058 - val_loss: 0.0143 - val_mean_squared_error: 0.0063\n",
      "Epoch 78/100\n",
      "45000/45000 [==============================] - 1s 30us/step - loss: 0.0139 - mean_squared_error: 0.0058 - val_loss: 0.0143 - val_mean_squared_error: 0.0063\n",
      "Epoch 79/100\n",
      "45000/45000 [==============================] - 1s 30us/step - loss: 0.0139 - mean_squared_error: 0.0058 - val_loss: 0.0143 - val_mean_squared_error: 0.0063\n",
      "Epoch 80/100\n",
      "45000/45000 [==============================] - 1s 30us/step - loss: 0.0139 - mean_squared_error: 0.0058 - val_loss: 0.0143 - val_mean_squared_error: 0.0063\n",
      "Epoch 81/100\n",
      "45000/45000 [==============================] - 1s 31us/step - loss: 0.0138 - mean_squared_error: 0.0058 - val_loss: 0.0143 - val_mean_squared_error: 0.0063\n",
      "Epoch 82/100\n",
      "45000/45000 [==============================] - 1s 29us/step - loss: 0.0138 - mean_squared_error: 0.0058 - val_loss: 0.0143 - val_mean_squared_error: 0.0063\n",
      "\n",
      "Epoch 00082: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "Epoch 83/100\n",
      "45000/45000 [==============================] - 1s 29us/step - loss: 0.0138 - mean_squared_error: 0.0058 - val_loss: 0.0143 - val_mean_squared_error: 0.0062\n",
      "Epoch 84/100\n",
      "45000/45000 [==============================] - 1s 30us/step - loss: 0.0138 - mean_squared_error: 0.0058 - val_loss: 0.0143 - val_mean_squared_error: 0.0063\n",
      "Epoch 85/100\n",
      "45000/45000 [==============================] - 2s 34us/step - loss: 0.0138 - mean_squared_error: 0.0058 - val_loss: 0.0143 - val_mean_squared_error: 0.0062\n",
      "Epoch 86/100\n",
      "45000/45000 [==============================] - 1s 32us/step - loss: 0.0138 - mean_squared_error: 0.0058 - val_loss: 0.0143 - val_mean_squared_error: 0.0062\n",
      "Epoch 87/100\n",
      "45000/45000 [==============================] - 1s 28us/step - loss: 0.0138 - mean_squared_error: 0.0058 - val_loss: 0.0143 - val_mean_squared_error: 0.0062\n",
      "\n",
      "Epoch 00087: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "Epoch 88/100\n",
      "45000/45000 [==============================] - 1s 30us/step - loss: 0.0138 - mean_squared_error: 0.0058 - val_loss: 0.0143 - val_mean_squared_error: 0.0062\n",
      "Epoch 89/100\n",
      "45000/45000 [==============================] - 1s 28us/step - loss: 0.0138 - mean_squared_error: 0.0058 - val_loss: 0.0143 - val_mean_squared_error: 0.0062\n",
      "Epoch 90/100\n",
      "45000/45000 [==============================] - 1s 30us/step - loss: 0.0138 - mean_squared_error: 0.0058 - val_loss: 0.0143 - val_mean_squared_error: 0.0062\n",
      "Epoch 91/100\n",
      "45000/45000 [==============================] - 1s 29us/step - loss: 0.0138 - mean_squared_error: 0.0058 - val_loss: 0.0143 - val_mean_squared_error: 0.0062\n",
      "Epoch 92/100\n",
      "45000/45000 [==============================] - 1s 29us/step - loss: 0.0138 - mean_squared_error: 0.0058 - val_loss: 0.0143 - val_mean_squared_error: 0.0062\n",
      "\n",
      "Epoch 00092: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "Epoch 93/100\n",
      "45000/45000 [==============================] - 1s 31us/step - loss: 0.0138 - mean_squared_error: 0.0058 - val_loss: 0.0142 - val_mean_squared_error: 0.0062\n",
      "Epoch 94/100\n",
      "45000/45000 [==============================] - 1s 28us/step - loss: 0.0138 - mean_squared_error: 0.0058 - val_loss: 0.0143 - val_mean_squared_error: 0.0062\n",
      "Epoch 95/100\n",
      "45000/45000 [==============================] - 1s 28us/step - loss: 0.0138 - mean_squared_error: 0.0058 - val_loss: 0.0143 - val_mean_squared_error: 0.0062\n",
      "Epoch 96/100\n",
      "45000/45000 [==============================] - 1s 32us/step - loss: 0.0138 - mean_squared_error: 0.0058 - val_loss: 0.0143 - val_mean_squared_error: 0.0062\n",
      "Epoch 97/100\n",
      "45000/45000 [==============================] - 1s 30us/step - loss: 0.0138 - mean_squared_error: 0.0058 - val_loss: 0.0143 - val_mean_squared_error: 0.0062\n",
      "\n",
      "Epoch 00097: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "Epoch 98/100\n",
      "45000/45000 [==============================] - 1s 29us/step - loss: 0.0138 - mean_squared_error: 0.0058 - val_loss: 0.0143 - val_mean_squared_error: 0.0062\n",
      "Epoch 99/100\n",
      "45000/45000 [==============================] - 1s 28us/step - loss: 0.0138 - mean_squared_error: 0.0058 - val_loss: 0.0143 - val_mean_squared_error: 0.0062\n",
      "Epoch 100/100\n",
      "45000/45000 [==============================] - 1s 30us/step - loss: 0.0138 - mean_squared_error: 0.0058 - val_loss: 0.0143 - val_mean_squared_error: 0.0062\n",
      "hw1/models/model_RoboschoolWalker2d-v1_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm/base.hdf5\n",
      "Done training model for RoboschoolWalker2d-v1\n",
      "{'model_name': 'model_RoboschoolWalker2d-v1_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm', 'model_path': 'hw1/models/model_RoboschoolWalker2d-v1_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm', 'test_mse': 0.006228809558052957, 'train_mse': 0.005773473504999525, 'dataset_name': 'RoboschoolWalker2d-v1'}\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.callbacks import Callback, LambdaCallback, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "from manage_datasets import get_datasets\n",
    "from keras_helpers.model_helper import create_model, get_model_name, calc_mse\n",
    "from keras_helpers.keras_train_stats import KerasTrainStats\n",
    "from utils import create_dir_if_not_exists\n",
    "\n",
    "\n",
    "SAVED_MODELS_DIR = 'models'\n",
    "MODEL_FILE_NAME = \"base.hdf5\"\n",
    "create_dir_if_not_exists(SAVED_MODELS_DIR)\n",
    "\n",
    "model_mse = []\n",
    "\n",
    "for envname in ROBOSCOOL_AVAILABLE_ENVS:\n",
    "    print(\"Training a FC ANN for env %s\" % envname)\n",
    "    \n",
    "    # Load the datasets\n",
    "    X_train, X_test, y_train, y_test = get_datasets(dataset_name=envname, dataset_dir=EXPERT_DATA_DIR)\n",
    "    \n",
    "    # Define the model params    \n",
    "    config_dict = dict(\n",
    "        input_dim=len(X_train[1, :]),\n",
    "        output_dim=len(y_train[1, :]),\n",
    "        units=100,\n",
    "        layers = 3,\n",
    "        l2_reg = 1e-04,\n",
    "        optimizer_cls=keras.optimizers.Adam,\n",
    "        lr = 1e-03,\n",
    "        dropout=None,\n",
    "        use_batchnorm=False)\n",
    "    \n",
    "    # Create the model\n",
    "    model = create_model(**config_dict)\n",
    "    \n",
    "    # Compile the model\n",
    "    optimizer = config_dict['optimizer_cls'](lr=config_dict['lr'])\n",
    "    model.compile(loss='mse', optimizer=optimizer, metrics=['mse'])\n",
    "    \n",
    "    # Set a unique name/directory for the model\n",
    "    model_name = get_model_name(base_name=envname, **config_dict)\n",
    "    model_path = os.path.join(SAVED_MODELS_DIR, model_name)\n",
    "    print(\"model_name='%s'\" % model_name)\n",
    "    print(\"model_path='%s'\" % model_path)\n",
    "    create_dir_if_not_exists(model_path)\n",
    "    model_filename = os.path.join(model_path, MODEL_FILE_NAME)\n",
    "    \n",
    "    \n",
    "    # Train the model\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_mean_squared_error', factor=0.5, patience=5, verbose=1)\n",
    "    tf_board = TensorBoard()\n",
    "\n",
    "    # Define a train_stats object\n",
    "    train_stats = KerasTrainStats(model_name=model_name, history_dir=model_path)\n",
    "\n",
    "\n",
    "    # Train the model\n",
    "    batch_size = 64\n",
    "    epochs = 100\n",
    "\n",
    "    _ = model.fit([X_train], y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        callbacks=[train_stats.print_callback, reduce_lr, tf_board],\n",
    "        validation_data=([X_test], y_test)\n",
    "        )\n",
    "    \n",
    "    \n",
    "    # Save the model\n",
    "    model.save(model_filename)\n",
    "    print(model_filename)\n",
    "    \n",
    "    # Calculate the MSE for each \n",
    "    res = calc_mse(model, envname, X_train, X_test, y_train, y_test)\n",
    "    res['model_name'] = model_name\n",
    "    res['model_path'] = model_path\n",
    "    model_mse.append(res)\n",
    "    print(\"Done training model for %s\" % envname)\n",
    "    #print(\"KerasTrainStats(model_name=model_name, history_dir=model_path).plt_history(start_epoch=5, metric_str='mean_squared_error', title='%s MSE')\" % dataset_name)\n",
    "    print(res)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the MSE error on both train/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>test_mse</th>\n",
       "      <th>train_mse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RoboschoolAnt-v1</td>\n",
       "      <td>0.045006</td>\n",
       "      <td>0.037660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RoboschoolHumanoid-v1</td>\n",
       "      <td>0.013683</td>\n",
       "      <td>0.013040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RoboschoolHalfCheetah-v1</td>\n",
       "      <td>0.013572</td>\n",
       "      <td>0.012406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RoboschoolReacher-v1</td>\n",
       "      <td>0.010305</td>\n",
       "      <td>0.003708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RoboschoolHopper-v1</td>\n",
       "      <td>0.006560</td>\n",
       "      <td>0.005663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RoboschoolWalker2d-v1</td>\n",
       "      <td>0.006229</td>\n",
       "      <td>0.005773</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               dataset_name  test_mse  train_mse\n",
       "0          RoboschoolAnt-v1  0.045006   0.037660\n",
       "1     RoboschoolHumanoid-v1  0.013683   0.013040\n",
       "2  RoboschoolHalfCheetah-v1  0.013572   0.012406\n",
       "3      RoboschoolReacher-v1  0.010305   0.003708\n",
       "4       RoboschoolHopper-v1  0.006560   0.005663\n",
       "5     RoboschoolWalker2d-v1  0.006229   0.005773"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_stats = pd.DataFrame(model_mse)\n",
    "train_df_stats[['dataset_name', 'test_mse', 'train_mse']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'dataset_name': 'RoboschoolAnt-v1',\n",
       "  'model_name': 'model_RoboschoolAnt-v1_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm',\n",
       "  'model_path': 'hw1/models/model_RoboschoolAnt-v1_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm',\n",
       "  'test_mse': 0.045005500097009286,\n",
       "  'train_mse': 0.03765979935236048},\n",
       " {'dataset_name': 'RoboschoolHumanoid-v1',\n",
       "  'model_name': 'model_RoboschoolHumanoid-v1_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm',\n",
       "  'model_path': 'hw1/models/model_RoboschoolHumanoid-v1_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm',\n",
       "  'test_mse': 0.013682615999663115,\n",
       "  'train_mse': 0.013040134573556305},\n",
       " {'dataset_name': 'RoboschoolHalfCheetah-v1',\n",
       "  'model_name': 'model_RoboschoolHalfCheetah-v1_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm',\n",
       "  'model_path': 'hw1/models/model_RoboschoolHalfCheetah-v1_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm',\n",
       "  'test_mse': 0.013571882395915625,\n",
       "  'train_mse': 0.01240620822838095},\n",
       " {'dataset_name': 'RoboschoolReacher-v1',\n",
       "  'model_name': 'model_RoboschoolReacher-v1_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm',\n",
       "  'model_path': 'hw1/models/model_RoboschoolReacher-v1_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm',\n",
       "  'test_mse': 0.010304973950460067,\n",
       "  'train_mse': 0.003708159562594425},\n",
       " {'dataset_name': 'RoboschoolHopper-v1',\n",
       "  'model_name': 'model_RoboschoolHopper-v1_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm',\n",
       "  'model_path': 'hw1/models/model_RoboschoolHopper-v1_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm',\n",
       "  'test_mse': 0.00656041671302159,\n",
       "  'train_mse': 0.005662971807407676},\n",
       " {'dataset_name': 'RoboschoolWalker2d-v1',\n",
       "  'model_name': 'model_RoboschoolWalker2d-v1_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm',\n",
       "  'model_path': 'hw1/models/model_RoboschoolWalker2d-v1_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm',\n",
       "  'test_mse': 0.006228809558052957,\n",
       "  'train_mse': 0.005773473504999525}]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_stats.to_dict(\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now lets define a policy based on the models\n",
    "The files `supervised_model_policy.py` and `run_supervised_policy.py` provide a `SupervisedModelPolicy` class and a method to run each of the new trained policy. It is worth taking a look at them and understand the implementasion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_supervised_policy import run_supervised_model_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'RoboschoolAnt-v1': 'hw1/models/model_RoboschoolAnt-v1_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm/base.hdf5',\n",
       " 'RoboschoolHalfCheetah-v1': 'hw1/models/model_RoboschoolHalfCheetah-v1_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm/base.hdf5',\n",
       " 'RoboschoolHopper-v1': 'hw1/models/model_RoboschoolHopper-v1_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm/base.hdf5',\n",
       " 'RoboschoolHumanoid-v1': 'hw1/models/model_RoboschoolHumanoid-v1_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm/base.hdf5',\n",
       " 'RoboschoolReacher-v1': 'hw1/models/model_RoboschoolReacher-v1_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm/base.hdf5',\n",
       " 'RoboschoolWalker2d-v1': 'hw1/models/model_RoboschoolWalker2d-v1_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm/base.hdf5'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a mapping between environment name to the model's full path\n",
    "env_to_model = {i['dataset_name']: os.path.join(i['model_path'], MODEL_FILE_NAME) for i in df.to_dict(\"records\")}\n",
    "env_to_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can now run the trajectories for each environment based on our model based policies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUPERVISED_MODELD_DATA_DIR = 'supervised_modeled_data'\n",
    "!mkdir -p {SUPERVISED_MODELD_DATA_DIR}\n",
    "#!rm -rf {SUPERVISED_MODELD_DATA_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running env RoboschoolAnt-v1 on supervised modeled data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Env description: Supervised model policy for module RoboschoolAnt-v1\n",
      "mean return 1877.2712311909445\n",
      "std of return 325.90571792640606\n",
      "CPU times: user 2min 36s, sys: 1min 14s, total: 3min 50s\n",
      "Wall time: 3min 42s\n",
      "Running env RoboschoolHumanoid-v1 on supervised modeled data\n",
      "Env description: Supervised model policy for module RoboschoolHumanoid-v1\n",
      "mean return 55.859387505426675\n",
      "std of return 18.571915481313283\n",
      "CPU times: user 12.9 s, sys: 5.27 s, total: 18.2 s\n",
      "Wall time: 17.9 s\n",
      "Running env RoboschoolHalfCheetah-v1 on supervised modeled data\n",
      "Env description: Supervised model policy for module RoboschoolHalfCheetah-v1\n",
      "mean return 2262.958939587815\n",
      "std of return 898.4748019329373\n",
      "CPU times: user 2min 2s, sys: 57.2 s, total: 2min 59s\n",
      "Wall time: 2min 53s\n",
      "Running env RoboschoolReacher-v1 on supervised modeled data\n",
      "Env description: Supervised model policy for module RoboschoolReacher-v1\n",
      "mean return 18.13609429203137\n",
      "std of return 8.195581301095807\n",
      "CPU times: user 20.8 s, sys: 4.94 s, total: 25.8 s\n",
      "Wall time: 24.7 s\n",
      "Running env RoboschoolHopper-v1 on supervised modeled data\n",
      "Env description: Supervised model policy for module RoboschoolHopper-v1\n",
      "mean return 1551.5115038345082\n",
      "std of return 704.7112268625207\n",
      "CPU times: user 1min 42s, sys: 43.9 s, total: 2min 26s\n",
      "Wall time: 2min 21s\n",
      "Running env RoboschoolWalker2d-v1 on supervised modeled data\n",
      "Env description: Supervised model policy for module RoboschoolWalker2d-v1\n",
      "mean return 2034.65627557789\n",
      "std of return 561.6191056861418\n",
      "CPU times: user 2min 29s, sys: 1min 9s, total: 3min 38s\n",
      "Wall time: 3min 31s\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "for envname in ROBOSCOOL_AVAILABLE_ENVS:\n",
    "    \n",
    "    model_filename = env_to_model[envname]\n",
    "    pickeld_filename = os.path.join(SUPERVISED_MODELD_DATA_DIR, envname + '.pkl')\n",
    "    if os.path.isfile(pickeld_filename):\n",
    "        print(\"Skipping env %s - trajectories already axists\" % envname)\n",
    "        continue\n",
    "    \n",
    "    print(\"Running env %s on supervised modeled data\" % envname)\n",
    "    %time supervised_modeled_data = run_supervised_model_policy(num_rollouts=50, envname=envname, model_filename=model_filename, verbose=False)\n",
    "    with open(pickeld_filename, 'wb') as f:\n",
    "        pickle.dump(supervised_modeled_data, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoboschoolAnt-v1.pkl\t      RoboschoolHumanoid-v1.pkl\r\n",
      "RoboschoolHalfCheetah-v1.pkl  RoboschoolReacher-v1.pkl\r\n",
      "RoboschoolHopper-v1.pkl       RoboschoolWalker2d-v1.pkl\r\n"
     ]
    }
   ],
   "source": [
    "!ls {SUPERVISED_MODELD_DATA_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze the Model vs Expert performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the returns for expert/model for each environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset RoboschoolAnt-v1\n",
      "loading dataset RoboschoolAnt-v1\n",
      "loading dataset RoboschoolHumanoid-v1\n",
      "loading dataset RoboschoolHumanoid-v1\n",
      "loading dataset RoboschoolHalfCheetah-v1\n",
      "loading dataset RoboschoolHalfCheetah-v1\n",
      "loading dataset RoboschoolReacher-v1\n",
      "loading dataset RoboschoolReacher-v1\n",
      "loading dataset RoboschoolHopper-v1\n",
      "loading dataset RoboschoolHopper-v1\n",
      "loading dataset RoboschoolWalker2d-v1\n",
      "loading dataset RoboschoolWalker2d-v1\n"
     ]
    }
   ],
   "source": [
    "from manage_datasets import load_dataset\n",
    "\n",
    "returns = {}\n",
    "for envname in ROBOSCOOL_AVAILABLE_ENVS:\n",
    "    returns[envname] = {}\n",
    "    returns[envname]['expert'] = load_dataset(dataset_name=envname, dataset_dir=EXPERT_DATA_DIR)['returns']\n",
    "    returns[envname]['model'] = load_dataset(dataset_name=envname, dataset_dir=SUPERVISED_MODELD_DATA_DIR)['returns']\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Expert vs Model returns histograms for each environments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x1080 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils import grid_iterator\n",
    "\n",
    "fig_title = \"Expert vs Model's returns\"\n",
    "density = True # If true, than the histogram's integral sums up to 1\n",
    "\n",
    "for ax, env_index in grid_iterator(n_rows=3, n_cols=2, axis_size=5, fig_title=fig_title):\n",
    "    envname = ROBOSCOOL_AVAILABLE_ENVS[env_index]\n",
    "    ax.hist(returns[envname]['expert'], label='expert', density=density, bins=25)\n",
    "    ax.hist(returns[envname]['model'], label='model', density=density, bins=25, alpha=0.5)\n",
    "    ax.set_aspect('auto')\n",
    "    ax.set_title(envname)\n",
    "    ax.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset RoboschoolAnt-v1\n",
      "loading dataset RoboschoolAnt-v1\n",
      "loading dataset RoboschoolHumanoid-v1\n",
      "loading dataset RoboschoolHumanoid-v1\n",
      "loading dataset RoboschoolHalfCheetah-v1\n",
      "loading dataset RoboschoolHalfCheetah-v1\n",
      "loading dataset RoboschoolReacher-v1\n",
      "loading dataset RoboschoolReacher-v1\n",
      "loading dataset RoboschoolHopper-v1\n",
      "loading dataset RoboschoolHopper-v1\n",
      "loading dataset RoboschoolWalker2d-v1\n",
      "loading dataset RoboschoolWalker2d-v1\n"
     ]
    }
   ],
   "source": [
    "from manage_datasets import load_dataset\n",
    "\n",
    "envs_stats = []\n",
    "for envname in ROBOSCOOL_AVAILABLE_ENVS:\n",
    "    expert_data = load_dataset(dataset_name=envname, dataset_dir=EXPERT_DATA_DIR)\n",
    "    dataset_size = len(expert_data['observations'])\n",
    "    expert_returns = expert_data['returns']\n",
    "    model_returns = load_dataset(dataset_name=envname, dataset_dir=SUPERVISED_MODELD_DATA_DIR)['returns']\n",
    "    \n",
    "    env_stats = {\n",
    "        'envname': envname,\n",
    "        'dataset_size': dataset_size,\n",
    "        'expert_mean': expert_returns.mean(),\n",
    "        'expert_std': expert_returns.std(),\n",
    "        'model_mean': model_returns.mean(),\n",
    "        'model_std': model_returns.std(),\n",
    "    }\n",
    "    envs_stats.append(env_stats)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['envname', 'dataset_size','expert_mean', 'model_mean', 'expert_std', 'model_std']\n",
    "stats_df = pd.DataFrame(envs_stats)[cols]\n",
    "stats_df['model_to_expert'] = stats_df['model_mean'] / stats_df['expert_mean']\n",
    "stats_df.sort_values(['dataset_size', 'model_to_expert'], ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>envname</th>\n",
       "      <th>dataset_size</th>\n",
       "      <th>expert_mean</th>\n",
       "      <th>model_mean</th>\n",
       "      <th>expert_std</th>\n",
       "      <th>model_std</th>\n",
       "      <th>model_to_expert</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RoboschoolAnt-v1</td>\n",
       "      <td>50000</td>\n",
       "      <td>1818.829300</td>\n",
       "      <td>1877.271231</td>\n",
       "      <td>381.867818</td>\n",
       "      <td>325.905718</td>\n",
       "      <td>1.032132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RoboschoolWalker2d-v1</td>\n",
       "      <td>50000</td>\n",
       "      <td>2205.412558</td>\n",
       "      <td>2034.656276</td>\n",
       "      <td>81.940065</td>\n",
       "      <td>561.619106</td>\n",
       "      <td>0.922574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RoboschoolHumanoid-v1</td>\n",
       "      <td>44060</td>\n",
       "      <td>2803.916301</td>\n",
       "      <td>55.859388</td>\n",
       "      <td>1010.564916</td>\n",
       "      <td>18.571915</td>\n",
       "      <td>0.019922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RoboschoolHalfCheetah-v1</td>\n",
       "      <td>42602</td>\n",
       "      <td>2271.298439</td>\n",
       "      <td>2262.958940</td>\n",
       "      <td>909.003381</td>\n",
       "      <td>898.474802</td>\n",
       "      <td>0.996328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RoboschoolHopper-v1</td>\n",
       "      <td>35778</td>\n",
       "      <td>1541.857022</td>\n",
       "      <td>1551.511504</td>\n",
       "      <td>687.020552</td>\n",
       "      <td>704.711227</td>\n",
       "      <td>1.006262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RoboschoolReacher-v1</td>\n",
       "      <td>7500</td>\n",
       "      <td>18.612237</td>\n",
       "      <td>18.136094</td>\n",
       "      <td>10.257760</td>\n",
       "      <td>8.195581</td>\n",
       "      <td>0.974418</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    envname  dataset_size  expert_mean   model_mean  \\\n",
       "0          RoboschoolAnt-v1         50000  1818.829300  1877.271231   \n",
       "5     RoboschoolWalker2d-v1         50000  2205.412558  2034.656276   \n",
       "1     RoboschoolHumanoid-v1         44060  2803.916301    55.859388   \n",
       "2  RoboschoolHalfCheetah-v1         42602  2271.298439  2262.958940   \n",
       "4       RoboschoolHopper-v1         35778  1541.857022  1551.511504   \n",
       "3      RoboschoolReacher-v1          7500    18.612237    18.136094   \n",
       "\n",
       "    expert_std   model_std  model_to_expert  \n",
       "0   381.867818  325.905718         1.032132  \n",
       "5    81.940065  561.619106         0.922574  \n",
       "1  1010.564916   18.571915         0.019922  \n",
       "2   909.003381  898.474802         0.996328  \n",
       "4   687.020552  704.711227         1.006262  \n",
       "3    10.257760    8.195581         0.974418  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing RoboschoolAnt-v1 and RoboschoolHumanoid-v1\n",
    "1. Dataset size: train dataset was about the same size, 45K for the RoboschoolAnt-v1 (90% of 50K) vs 40K for the RoboschoolHumanoid-v1. \n",
    "2. Model architecture: the same model arcitecture was used for both tasks: 3 Fully Connected layers with 100 neurons each. L2 regularization of 1e-04 was used with no dropout nor Batch Normalization. \n",
    "3. Optimizer, Epochs and Learning rate: both models were trained for 100 epochs with Adam optimizer with initial learning rate of 1e-03. The learning rate was halved after every five cosecutive epochs with no Validation MSE improvement.\n",
    "4. Performance: for the RoboschoolAnt-v1, the model performance outperformed the expert by ~3.2% (I'm not sure why this is happaning, maybe the trained model implicitly added some sort of required regularization over the expert network...). For the RoboschoolHumanoid-v1 task the model achived less than %2 (~1.9%) of the expert mean reward. For the RoboschoolAnt-v1 task, the return std was also decreased (expert std of ~382, vs model's std of ~307)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check RoboschoolAnt-v1 model performance vs train set size\n",
    "The RoboschoolAnt-v1 model significantly outperformed RoboschoolHumanoid-v1 model's performance against the expert. It would be interesting to see the amount of training data needed in order to reach these performance. Another objective is to validate that the perfomance gap between the two tasks was not couse by the fact that RoboschoolHumanoid-v1 was train on a smaller dataset (45K for RoboschoolAnt-v1 vs 40K for RoboschoolHumanoid-v1).\n",
    "\n",
    "Note - we will plot the performance in terms of validation loss instead of model_mean/expert_mean in order to avoid running the full 50 trajectories on all training set sizes. In addition, we will report the model_mean/expert_mean specifically for the specific case of training over 40K examples, in order to assure that the performance difference with RoboschoolHumanoid-v1 was not cause by the training set size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset RoboschoolAnt-v1\n",
      "Domain name: RoboschoolAnt-v1\n",
      "(45000, 28) (5000, 28) (45000, 8) (5000, 8)\n",
      "Training a FC ANN for env RoboschoolAnt-v1 over a training set size 10000 \n",
      "model_name='model_RoboschoolAnt-v1_training_set_size_10000_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm'\n",
      "model_path='models/model_RoboschoolAnt-v1_training_set_size_10000_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm'\n",
      "Train on 10000 samples, validate on 5000 samples\n",
      "Epoch 1/100\n",
      "10000/10000 [==============================] - 1s 80us/step - loss: 0.6969 - mean_squared_error: 0.6731 - val_loss: 0.4206 - val_mean_squared_error: 0.3960\n",
      "Epoch 2/100\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.3624 - mean_squared_error: 0.3369 - val_loss: 0.3170 - val_mean_squared_error: 0.2906\n",
      "Epoch 3/100\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 0.2811 - mean_squared_error: 0.2538 - val_loss: 0.2623 - val_mean_squared_error: 0.2342\n",
      "Epoch 4/100\n",
      "10000/10000 [==============================] - 0s 38us/step - loss: 0.2405 - mean_squared_error: 0.2120 - val_loss: 0.2299 - val_mean_squared_error: 0.2008\n",
      "Epoch 5/100\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.2153 - mean_squared_error: 0.1857 - val_loss: 0.2135 - val_mean_squared_error: 0.1836\n",
      "Epoch 6/100\n",
      "10000/10000 [==============================] - 0s 38us/step - loss: 0.1995 - mean_squared_error: 0.1692 - val_loss: 0.2058 - val_mean_squared_error: 0.1752\n",
      "Epoch 7/100\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.1861 - mean_squared_error: 0.1553 - val_loss: 0.1973 - val_mean_squared_error: 0.1663\n",
      "Epoch 8/100\n",
      "10000/10000 [==============================] - 0s 38us/step - loss: 0.1770 - mean_squared_error: 0.1457 - val_loss: 0.1821 - val_mean_squared_error: 0.1507\n",
      "Epoch 9/100\n",
      "10000/10000 [==============================] - 0s 38us/step - loss: 0.1702 - mean_squared_error: 0.1386 - val_loss: 0.1822 - val_mean_squared_error: 0.1505\n",
      "Epoch 10/100\n",
      "10000/10000 [==============================] - 0s 38us/step - loss: 0.1640 - mean_squared_error: 0.1321 - val_loss: 0.1694 - val_mean_squared_error: 0.1374\n",
      "Epoch 11/100\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 0.1588 - mean_squared_error: 0.1267 - val_loss: 0.1693 - val_mean_squared_error: 0.1372\n",
      "Epoch 12/100\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 0.1525 - mean_squared_error: 0.12 - 0s 36us/step - loss: 0.1525 - mean_squared_error: 0.1203 - val_loss: 0.1597 - val_mean_squared_error: 0.1274\n",
      "Epoch 13/100\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 0.1499 - mean_squared_error: 0.1176 - val_loss: 0.1601 - val_mean_squared_error: 0.1278\n",
      "Epoch 14/100\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 0.1449 - mean_squared_error: 0.1125 - val_loss: 0.1574 - val_mean_squared_error: 0.1250\n",
      "Epoch 15/100\n",
      "10000/10000 [==============================] - 0s 38us/step - loss: 0.1418 - mean_squared_error: 0.1093 - val_loss: 0.1558 - val_mean_squared_error: 0.1233\n",
      "Epoch 16/100\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 0.1381 - mean_squared_error: 0.1056 - val_loss: 0.1540 - val_mean_squared_error: 0.1215\n",
      "Epoch 17/100\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 0.1358 - mean_squared_error: 0.1033 - val_loss: 0.1487 - val_mean_squared_error: 0.1162\n",
      "Epoch 18/100\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 0.1335 - mean_squared_error: 0.1010 - val_loss: 0.1421 - val_mean_squared_error: 0.1095\n",
      "Epoch 19/100\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 0.1318 - mean_squared_error: 0.0993 - val_loss: 0.1461 - val_mean_squared_error: 0.1136\n",
      "Epoch 20/100\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 0.1304 - mean_squared_error: 0.0979 - val_loss: 0.1423 - val_mean_squared_error: 0.1098\n",
      "Epoch 21/100\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 0.1281 - mean_squared_error: 0.0957 - val_loss: 0.1444 - val_mean_squared_error: 0.1119\n",
      "Epoch 22/100\n",
      "10000/10000 [==============================] - 0s 44us/step - loss: 0.1253 - mean_squared_error: 0.0929 - val_loss: 0.1395 - val_mean_squared_error: 0.1070\n",
      "Epoch 23/100\n",
      "10000/10000 [==============================] - 0s 45us/step - loss: 0.1243 - mean_squared_error: 0.0919 - val_loss: 0.1391 - val_mean_squared_error: 0.1067\n",
      "Epoch 24/100\n",
      "10000/10000 [==============================] - 0s 44us/step - loss: 0.1222 - mean_squared_error: 0.0899 - val_loss: 0.1386 - val_mean_squared_error: 0.1062\n",
      "Epoch 25/100\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 0.1205 - mean_squared_error: 0.0882 - val_loss: 0.1351 - val_mean_squared_error: 0.1028\n",
      "Epoch 26/100\n",
      "10000/10000 [==============================] - 0s 38us/step - loss: 0.1196 - mean_squared_error: 0.0874 - val_loss: 0.1350 - val_mean_squared_error: 0.1029\n",
      "Epoch 27/100\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 0.1179 - mean_squared_error: 0.0858 - val_loss: 0.1356 - val_mean_squared_error: 0.1035\n",
      "Epoch 28/100\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 0.1172 - mean_squared_error: 0.0851 - val_loss: 0.1333 - val_mean_squared_error: 0.1012\n",
      "Epoch 29/100\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 0.1154 - mean_squared_error: 0.0833 - val_loss: 0.1361 - val_mean_squared_error: 0.1041\n",
      "Epoch 30/100\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 0.1146 - mean_squared_error: 0.0826 - val_loss: 0.1325 - val_mean_squared_error: 0.1006\n",
      "Epoch 31/100\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 0.1130 - mean_squared_error: 0.0811 - val_loss: 0.1341 - val_mean_squared_error: 0.1022\n",
      "Epoch 32/100\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 0.1128 - mean_squared_error: 0.0810 - val_loss: 0.1295 - val_mean_squared_error: 0.0977\n",
      "Epoch 33/100\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 0.1107 - mean_squared_error: 0.0790 - val_loss: 0.1286 - val_mean_squared_error: 0.0970\n",
      "Epoch 34/100\n",
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.1096 - mean_squared_error: 0.0780 - val_loss: 0.1250 - val_mean_squared_error: 0.0934\n",
      "Epoch 35/100\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 0.1087 - mean_squared_error: 0.0771 - val_loss: 0.1277 - val_mean_squared_error: 0.0962\n",
      "Epoch 36/100\n",
      "10000/10000 [==============================] - 0s 38us/step - loss: 0.1086 - mean_squared_error: 0.0772 - val_loss: 0.1265 - val_mean_squared_error: 0.0950\n",
      "Epoch 37/100\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.1081 - mean_squared_error: 0.0767 - val_loss: 0.1229 - val_mean_squared_error: 0.0915\n",
      "Epoch 38/100\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.1066 - mean_squared_error: 0.0753 - val_loss: 0.1248 - val_mean_squared_error: 0.0936\n",
      "Epoch 39/100\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.1058 - mean_squared_error: 0.0746 - val_loss: 0.1230 - val_mean_squared_error: 0.0918\n",
      "Epoch 40/100\n",
      "10000/10000 [==============================] - 0s 38us/step - loss: 0.1047 - mean_squared_error: 0.0736 - val_loss: 0.1291 - val_mean_squared_error: 0.0980\n",
      "Epoch 41/100\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.1039 - mean_squared_error: 0.0729 - val_loss: 0.1237 - val_mean_squared_error: 0.0927\n",
      "Epoch 42/100\n",
      "10000/10000 [==============================] - 0s 38us/step - loss: 0.1032 - mean_squared_error: 0.0722 - val_loss: 0.1271 - val_mean_squared_error: 0.0961\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 43/100\n",
      "10000/10000 [==============================] - 0s 38us/step - loss: 0.0970 - mean_squared_error: 0.0661 - val_loss: 0.1159 - val_mean_squared_error: 0.0850\n",
      "Epoch 44/100\n",
      "10000/10000 [==============================] - 0s 38us/step - loss: 0.0955 - mean_squared_error: 0.0647 - val_loss: 0.1175 - val_mean_squared_error: 0.0867\n",
      "Epoch 45/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 35us/step - loss: 0.0955 - mean_squared_error: 0.0647 - val_loss: 0.1161 - val_mean_squared_error: 0.0853\n",
      "Epoch 46/100\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0949 - mean_squared_error: 0.0642 - val_loss: 0.1157 - val_mean_squared_error: 0.0849\n",
      "Epoch 47/100\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0945 - mean_squared_error: 0.0638 - val_loss: 0.1164 - val_mean_squared_error: 0.0857\n",
      "Epoch 48/100\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 0.0947 - mean_squared_error: 0.0641 - val_loss: 0.1179 - val_mean_squared_error: 0.0872\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 49/100\n",
      "10000/10000 [==============================] - 0s 38us/step - loss: 0.0920 - mean_squared_error: 0.0614 - val_loss: 0.1124 - val_mean_squared_error: 0.0818\n",
      "Epoch 50/100\n",
      "10000/10000 [==============================] - 0s 38us/step - loss: 0.0910 - mean_squared_error: 0.0604 - val_loss: 0.1129 - val_mean_squared_error: 0.0823\n",
      "Epoch 51/100\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0907 - mean_squared_error: 0.0601 - val_loss: 0.1120 - val_mean_squared_error: 0.0815\n",
      "Epoch 52/100\n",
      "10000/10000 [==============================] - 0s 38us/step - loss: 0.0908 - mean_squared_error: 0.0602 - val_loss: 0.1126 - val_mean_squared_error: 0.0820\n",
      "Epoch 53/100\n",
      "10000/10000 [==============================] - 0s 38us/step - loss: 0.0903 - mean_squared_error: 0.0598 - val_loss: 0.1126 - val_mean_squared_error: 0.0821\n",
      "Epoch 54/100\n",
      "10000/10000 [==============================] - 0s 38us/step - loss: 0.0904 - mean_squared_error: 0.0600 - val_loss: 0.1120 - val_mean_squared_error: 0.0816\n",
      "Epoch 55/100\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 0.0901 - mean_squared_error: 0.0597 - val_loss: 0.1116 - val_mean_squared_error: 0.0811\n",
      "Epoch 56/100\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 0.0901 - mean_squared_error: 0.0597 - val_loss: 0.1135 - val_mean_squared_error: 0.0831\n",
      "Epoch 57/100\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 0.0897 - mean_squared_error: 0.0593 - val_loss: 0.1112 - val_mean_squared_error: 0.0808\n",
      "Epoch 58/100\n",
      "10000/10000 [==============================] - 0s 47us/step - loss: 0.0893 - mean_squared_error: 0.0589 - val_loss: 0.1112 - val_mean_squared_error: 0.0808\n",
      "Epoch 59/100\n",
      "10000/10000 [==============================] - 0s 44us/step - loss: 0.0894 - mean_squared_error: 0.0590 - val_loss: 0.1110 - val_mean_squared_error: 0.0807\n",
      "Epoch 60/100\n",
      "10000/10000 [==============================] - 0s 46us/step - loss: 0.0892 - mean_squared_error: 0.0589 - val_loss: 0.1113 - val_mean_squared_error: 0.0810\n",
      "Epoch 61/100\n",
      "10000/10000 [==============================] - 0s 42us/step - loss: 0.0893 - mean_squared_error: 0.0590 - val_loss: 0.1111 - val_mean_squared_error: 0.0808\n",
      "Epoch 62/100\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 0.0888 - mean_squared_error: 0.0585 - val_loss: 0.1111 - val_mean_squared_error: 0.0809\n",
      "Epoch 63/100\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 0.0885 - mean_squared_error: 0.0583 - val_loss: 0.1104 - val_mean_squared_error: 0.0802\n",
      "Epoch 64/100\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 0.0885 - mean_squared_error: 0.0584 - val_loss: 0.1116 - val_mean_squared_error: 0.0814\n",
      "Epoch 65/100\n",
      "10000/10000 [==============================] - 0s 47us/step - loss: 0.0882 - mean_squared_error: 0.0581 - val_loss: 0.1101 - val_mean_squared_error: 0.0800\n",
      "Epoch 66/100\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 0.0880 - mean_squared_error: 0.0579 - val_loss: 0.1094 - val_mean_squared_error: 0.0793\n",
      "Epoch 67/100\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 0.0877 - mean_squared_error: 0.0576 - val_loss: 0.1099 - val_mean_squared_error: 0.0798\n",
      "Epoch 68/100\n",
      "10000/10000 [==============================] - 0s 38us/step - loss: 0.0874 - mean_squared_error: 0.0574 - val_loss: 0.1098 - val_mean_squared_error: 0.0798\n",
      "Epoch 69/100\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 0.0870 - mean_squared_error: 0.0570 - val_loss: 0.1112 - val_mean_squared_error: 0.0812\n",
      "Epoch 70/100\n",
      "10000/10000 [==============================] - 0s 44us/step - loss: 0.0872 - mean_squared_error: 0.0572 - val_loss: 0.1100 - val_mean_squared_error: 0.0800\n",
      "Epoch 71/100\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 0.0871 - mean_squared_error: 0.0572 - val_loss: 0.1091 - val_mean_squared_error: 0.0792\n",
      "\n",
      "Epoch 00071: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 72/100\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 0.0852 - mean_squared_error: 0.0553 - val_loss: 0.1083 - val_mean_squared_error: 0.0784\n",
      "Epoch 73/100\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 0.0850 - mean_squared_error: 0.0551 - val_loss: 0.1088 - val_mean_squared_error: 0.0789\n",
      "Epoch 74/100\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 0.0849 - mean_squared_error: 0.0551 - val_loss: 0.1089 - val_mean_squared_error: 0.0790\n",
      "Epoch 75/100\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 0.0849 - mean_squared_error: 0.0550 - val_loss: 0.1076 - val_mean_squared_error: 0.0777\n",
      "Epoch 76/100\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 0.0847 - mean_squared_error: 0.0548 - val_loss: 0.1077 - val_mean_squared_error: 0.0779\n",
      "Epoch 77/100\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 0.0846 - mean_squared_error: 0.0548 - val_loss: 0.1084 - val_mean_squared_error: 0.0785\n",
      "Epoch 78/100\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 0.0845 - mean_squared_error: 0.0547 - val_loss: 0.1078 - val_mean_squared_error: 0.0780\n",
      "Epoch 79/100\n",
      "10000/10000 [==============================] - 1s 56us/step - loss: 0.0845 - mean_squared_error: 0.0547 - val_loss: 0.1084 - val_mean_squared_error: 0.0787\n",
      "Epoch 80/100\n",
      "10000/10000 [==============================] - 0s 48us/step - loss: 0.0843 - mean_squared_error: 0.0545 - val_loss: 0.1079 - val_mean_squared_error: 0.0781\n",
      "\n",
      "Epoch 00080: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 81/100\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 0.0835 - mean_squared_error: 0.0537 - val_loss: 0.1068 - val_mean_squared_error: 0.0770\n",
      "Epoch 82/100\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 0.0833 - mean_squared_error: 0.0536 - val_loss: 0.1071 - val_mean_squared_error: 0.0773\n",
      "Epoch 83/100\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0832 - mean_squared_error: 0.0535 - val_loss: 0.1069 - val_mean_squared_error: 0.0771\n",
      "Epoch 84/100\n",
      "10000/10000 [==============================] - 0s 38us/step - loss: 0.0833 - mean_squared_error: 0.0535 - val_loss: 0.1068 - val_mean_squared_error: 0.0771\n",
      "Epoch 85/100\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0832 - mean_squared_error: 0.0534 - val_loss: 0.1067 - val_mean_squared_error: 0.0770\n",
      "Epoch 86/100\n",
      "10000/10000 [==============================] - 0s 43us/step - loss: 0.0831 - mean_squared_error: 0.0534 - val_loss: 0.1067 - val_mean_squared_error: 0.0770\n",
      "\n",
      "Epoch 00086: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 87/100\n",
      "10000/10000 [==============================] - 0s 44us/step - loss: 0.0827 - mean_squared_error: 0.0530 - val_loss: 0.1064 - val_mean_squared_error: 0.0767\n",
      "Epoch 88/100\n",
      "10000/10000 [==============================] - 0s 41us/step - loss: 0.0826 - mean_squared_error: 0.0529 - val_loss: 0.1064 - val_mean_squared_error: 0.0767\n",
      "Epoch 89/100\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 0.0826 - mean_squared_error: 0.0529 - val_loss: 0.1066 - val_mean_squared_error: 0.0769\n",
      "Epoch 90/100\n",
      "10000/10000 [==============================] - 0s 38us/step - loss: 0.0826 - mean_squared_error: 0.0529 - val_loss: 0.1064 - val_mean_squared_error: 0.0767\n",
      "Epoch 91/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 36us/step - loss: 0.0825 - mean_squared_error: 0.0529 - val_loss: 0.1063 - val_mean_squared_error: 0.0766\n",
      "Epoch 92/100\n",
      "10000/10000 [==============================] - 0s 38us/step - loss: 0.0825 - mean_squared_error: 0.0528 - val_loss: 0.1063 - val_mean_squared_error: 0.0766\n",
      "Epoch 93/100\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 0.0825 - mean_squared_error: 0.0528 - val_loss: 0.1062 - val_mean_squared_error: 0.0765\n",
      "Epoch 94/100\n",
      "10000/10000 [==============================] - 0s 38us/step - loss: 0.0825 - mean_squared_error: 0.0528 - val_loss: 0.1063 - val_mean_squared_error: 0.0767\n",
      "Epoch 95/100\n",
      "10000/10000 [==============================] - 0s 37us/step - loss: 0.0825 - mean_squared_error: 0.0528 - val_loss: 0.1064 - val_mean_squared_error: 0.0767\n",
      "Epoch 96/100\n",
      "10000/10000 [==============================] - 0s 39us/step - loss: 0.0824 - mean_squared_error: 0.0527 - val_loss: 0.1062 - val_mean_squared_error: 0.0766\n",
      "Epoch 97/100\n",
      "10000/10000 [==============================] - 0s 40us/step - loss: 0.0824 - mean_squared_error: 0.0527 - val_loss: 0.1062 - val_mean_squared_error: 0.0765\n",
      "\n",
      "Epoch 00097: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 98/100\n",
      "10000/10000 [==============================] - 0s 48us/step - loss: 0.0821 - mean_squared_error: 0.0525 - val_loss: 0.1061 - val_mean_squared_error: 0.0765\n",
      "Epoch 99/100\n",
      "10000/10000 [==============================] - 0s 47us/step - loss: 0.0821 - mean_squared_error: 0.0524 - val_loss: 0.1061 - val_mean_squared_error: 0.0765\n",
      "Epoch 100/100\n",
      "10000/10000 [==============================] - 0s 38us/step - loss: 0.0821 - mean_squared_error: 0.0524 - val_loss: 0.1061 - val_mean_squared_error: 0.0764\n",
      "models/model_RoboschoolAnt-v1_training_set_size_10000_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm/base.hdf5\n",
      "Done training model for RoboschoolAnt-v1\n",
      "{'model_name': 'model_RoboschoolAnt-v1_training_set_size_10000_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm', 'model_path': 'models/model_RoboschoolAnt-v1_training_set_size_10000_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm', 'test_mse': 0.07644564813741347, 'train_mse': 0.0729776201980172, 'dataset_name': 'RoboschoolAnt-v1'}\n",
      "Training a FC ANN for env RoboschoolAnt-v1 over a training set size 20000 \n",
      "model_name='model_RoboschoolAnt-v1_training_set_size_20000_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm'\n",
      "model_path='models/model_RoboschoolAnt-v1_training_set_size_20000_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm'\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/100\n",
      "20000/20000 [==============================] - 1s 53us/step - loss: 0.5226 - mean_squared_error: 0.4974 - val_loss: 0.3066 - val_mean_squared_error: 0.2799\n",
      "Epoch 2/100\n",
      "20000/20000 [==============================] - 1s 30us/step - loss: 0.2646 - mean_squared_error: 0.2367 - val_loss: 0.2297 - val_mean_squared_error: 0.2010\n",
      "Epoch 3/100\n",
      "20000/20000 [==============================] - 1s 34us/step - loss: 0.2123 - mean_squared_error: 0.1830 - val_loss: 0.1993 - val_mean_squared_error: 0.1694\n",
      "Epoch 4/100\n",
      "20000/20000 [==============================] - 1s 33us/step - loss: 0.1883 - mean_squared_error: 0.1580 - val_loss: 0.1797 - val_mean_squared_error: 0.1491\n",
      "Epoch 5/100\n",
      "20000/20000 [==============================] - 1s 40us/step - loss: 0.1714 - mean_squared_error: 0.1405 - val_loss: 0.1693 - val_mean_squared_error: 0.1381\n",
      "Epoch 6/100\n",
      "20000/20000 [==============================] - 1s 33us/step - loss: 0.1607 - mean_squared_error: 0.1294 - val_loss: 0.1652 - val_mean_squared_error: 0.1338\n",
      "Epoch 7/100\n",
      "20000/20000 [==============================] - 1s 33us/step - loss: 0.1529 - mean_squared_error: 0.1214 - val_loss: 0.1540 - val_mean_squared_error: 0.1224\n",
      "Epoch 8/100\n",
      "20000/20000 [==============================] - 1s 33us/step - loss: 0.1468 - mean_squared_error: 0.1152 - val_loss: 0.1495 - val_mean_squared_error: 0.1179\n",
      "Epoch 9/100\n",
      "20000/20000 [==============================] - 1s 32us/step - loss: 0.1404 - mean_squared_error: 0.1087 - val_loss: 0.1434 - val_mean_squared_error: 0.1117\n",
      "Epoch 10/100\n",
      "20000/20000 [==============================] - 1s 32us/step - loss: 0.1362 - mean_squared_error: 0.1046 - val_loss: 0.1400 - val_mean_squared_error: 0.1084\n",
      "Epoch 11/100\n",
      "20000/20000 [==============================] - ETA: 0s - loss: 0.1323 - mean_squared_error: 0.10 - 1s 31us/step - loss: 0.1321 - mean_squared_error: 0.1005 - val_loss: 0.1375 - val_mean_squared_error: 0.1060\n",
      "Epoch 12/100\n",
      "20000/20000 [==============================] - 1s 32us/step - loss: 0.1291 - mean_squared_error: 0.0976 - val_loss: 0.1358 - val_mean_squared_error: 0.1044\n",
      "Epoch 13/100\n",
      "20000/20000 [==============================] - 1s 32us/step - loss: 0.1259 - mean_squared_error: 0.0946 - val_loss: 0.1309 - val_mean_squared_error: 0.0996\n",
      "Epoch 14/100\n",
      "20000/20000 [==============================] - 1s 33us/step - loss: 0.1229 - mean_squared_error: 0.0917 - val_loss: 0.1301 - val_mean_squared_error: 0.0990\n",
      "Epoch 15/100\n",
      "20000/20000 [==============================] - 1s 31us/step - loss: 0.1206 - mean_squared_error: 0.0896 - val_loss: 0.1273 - val_mean_squared_error: 0.0962\n",
      "Epoch 16/100\n",
      "20000/20000 [==============================] - 1s 32us/step - loss: 0.1191 - mean_squared_error: 0.0881 - val_loss: 0.1273 - val_mean_squared_error: 0.0965\n",
      "Epoch 17/100\n",
      "20000/20000 [==============================] - 1s 32us/step - loss: 0.1166 - mean_squared_error: 0.0858 - val_loss: 0.1232 - val_mean_squared_error: 0.0925\n",
      "Epoch 18/100\n",
      "20000/20000 [==============================] - 1s 31us/step - loss: 0.1149 - mean_squared_error: 0.0843 - val_loss: 0.1197 - val_mean_squared_error: 0.0891\n",
      "Epoch 19/100\n",
      "20000/20000 [==============================] - 1s 31us/step - loss: 0.1131 - mean_squared_error: 0.0826 - val_loss: 0.1235 - val_mean_squared_error: 0.0931\n",
      "Epoch 20/100\n",
      "20000/20000 [==============================] - 1s 32us/step - loss: 0.1116 - mean_squared_error: 0.0813 - val_loss: 0.1220 - val_mean_squared_error: 0.0918\n",
      "Epoch 21/100\n",
      "20000/20000 [==============================] - 1s 32us/step - loss: 0.1100 - mean_squared_error: 0.0798 - val_loss: 0.1156 - val_mean_squared_error: 0.0856\n",
      "Epoch 22/100\n",
      "20000/20000 [==============================] - 1s 32us/step - loss: 0.1082 - mean_squared_error: 0.0782 - val_loss: 0.1143 - val_mean_squared_error: 0.0844\n",
      "Epoch 23/100\n",
      "20000/20000 [==============================] - 1s 31us/step - loss: 0.1073 - mean_squared_error: 0.0775 - val_loss: 0.1154 - val_mean_squared_error: 0.0857\n",
      "Epoch 24/100\n",
      "20000/20000 [==============================] - 1s 32us/step - loss: 0.1064 - mean_squared_error: 0.0768 - val_loss: 0.1116 - val_mean_squared_error: 0.0819\n",
      "Epoch 25/100\n",
      "20000/20000 [==============================] - 1s 32us/step - loss: 0.1045 - mean_squared_error: 0.0750 - val_loss: 0.1172 - val_mean_squared_error: 0.0877\n",
      "Epoch 26/100\n",
      "20000/20000 [==============================] - 1s 31us/step - loss: 0.1038 - mean_squared_error: 0.0744 - val_loss: 0.1106 - val_mean_squared_error: 0.0812\n",
      "Epoch 27/100\n",
      "20000/20000 [==============================] - 1s 31us/step - loss: 0.1018 - mean_squared_error: 0.0726 - val_loss: 0.1097 - val_mean_squared_error: 0.0805\n",
      "Epoch 28/100\n",
      "20000/20000 [==============================] - 1s 32us/step - loss: 0.1011 - mean_squared_error: 0.0720 - val_loss: 0.1088 - val_mean_squared_error: 0.0798\n",
      "Epoch 29/100\n",
      "20000/20000 [==============================] - 1s 31us/step - loss: 0.0999 - mean_squared_error: 0.0710 - val_loss: 0.1101 - val_mean_squared_error: 0.0812\n",
      "Epoch 30/100\n",
      "20000/20000 [==============================] - 1s 32us/step - loss: 0.0997 - mean_squared_error: 0.0709 - val_loss: 0.1079 - val_mean_squared_error: 0.0792\n",
      "Epoch 31/100\n",
      "20000/20000 [==============================] - 1s 32us/step - loss: 0.0985 - mean_squared_error: 0.0698 - val_loss: 0.1087 - val_mean_squared_error: 0.0801\n",
      "Epoch 32/100\n",
      "20000/20000 [==============================] - 1s 32us/step - loss: 0.0975 - mean_squared_error: 0.0690 - val_loss: 0.1118 - val_mean_squared_error: 0.0834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/100\n",
      "20000/20000 [==============================] - 1s 37us/step - loss: 0.0964 - mean_squared_error: 0.0680 - val_loss: 0.1057 - val_mean_squared_error: 0.0774\n",
      "Epoch 34/100\n",
      "20000/20000 [==============================] - 1s 39us/step - loss: 0.0960 - mean_squared_error: 0.0677 - val_loss: 0.1047 - val_mean_squared_error: 0.0765\n",
      "Epoch 35/100\n",
      "20000/20000 [==============================] - 1s 36us/step - loss: 0.0952 - mean_squared_error: 0.0670 - val_loss: 0.1027 - val_mean_squared_error: 0.0747\n",
      "Epoch 36/100\n",
      "20000/20000 [==============================] - 1s 37us/step - loss: 0.0945 - mean_squared_error: 0.0665 - val_loss: 0.1054 - val_mean_squared_error: 0.0775\n",
      "Epoch 37/100\n",
      "20000/20000 [==============================] - 1s 38us/step - loss: 0.0940 - mean_squared_error: 0.0662 - val_loss: 0.1040 - val_mean_squared_error: 0.0762\n",
      "Epoch 38/100\n",
      "20000/20000 [==============================] - 1s 37us/step - loss: 0.0936 - mean_squared_error: 0.0659 - val_loss: 0.1022 - val_mean_squared_error: 0.0746\n",
      "Epoch 39/100\n",
      "20000/20000 [==============================] - 1s 39us/step - loss: 0.0919 - mean_squared_error: 0.0643 - val_loss: 0.1007 - val_mean_squared_error: 0.0732\n",
      "Epoch 40/100\n",
      "20000/20000 [==============================] - 1s 37us/step - loss: 0.0916 - mean_squared_error: 0.0641 - val_loss: 0.1023 - val_mean_squared_error: 0.0748\n",
      "Epoch 41/100\n",
      "20000/20000 [==============================] - 1s 34us/step - loss: 0.0906 - mean_squared_error: 0.0632 - val_loss: 0.0999 - val_mean_squared_error: 0.0726\n",
      "Epoch 42/100\n",
      "20000/20000 [==============================] - 1s 33us/step - loss: 0.0911 - mean_squared_error: 0.0639 - val_loss: 0.1004 - val_mean_squared_error: 0.0732\n",
      "Epoch 43/100\n",
      "20000/20000 [==============================] - 1s 35us/step - loss: 0.0895 - mean_squared_error: 0.0624 - val_loss: 0.1029 - val_mean_squared_error: 0.0758\n",
      "Epoch 44/100\n",
      "20000/20000 [==============================] - 1s 38us/step - loss: 0.0896 - mean_squared_error: 0.0626 - val_loss: 0.0994 - val_mean_squared_error: 0.0724\n",
      "Epoch 45/100\n",
      "20000/20000 [==============================] - 1s 33us/step - loss: 0.0890 - mean_squared_error: 0.0620 - val_loss: 0.0995 - val_mean_squared_error: 0.0726\n",
      "Epoch 46/100\n",
      "20000/20000 [==============================] - 1s 40us/step - loss: 0.0881 - mean_squared_error: 0.0613 - val_loss: 0.0964 - val_mean_squared_error: 0.0696\n",
      "Epoch 47/100\n",
      "20000/20000 [==============================] - 1s 45us/step - loss: 0.0874 - mean_squared_error: 0.0606 - val_loss: 0.1012 - val_mean_squared_error: 0.0745\n",
      "Epoch 48/100\n",
      "20000/20000 [==============================] - 1s 35us/step - loss: 0.0872 - mean_squared_error: 0.0605 - val_loss: 0.1010 - val_mean_squared_error: 0.0744\n",
      "Epoch 49/100\n",
      "20000/20000 [==============================] - 1s 34us/step - loss: 0.0871 - mean_squared_error: 0.0605 - val_loss: 0.0990 - val_mean_squared_error: 0.0725loss: 0.0871 - mean_squared_error: 0.\n",
      "Epoch 50/100\n",
      "20000/20000 [==============================] - 1s 37us/step - loss: 0.0860 - mean_squared_error: 0.0595 - val_loss: 0.0954 - val_mean_squared_error: 0.0689\n",
      "Epoch 51/100\n",
      "20000/20000 [==============================] - 1s 38us/step - loss: 0.0860 - mean_squared_error: 0.0596 - val_loss: 0.0961 - val_mean_squared_error: 0.0698\n",
      "Epoch 52/100\n",
      "20000/20000 [==============================] - 1s 33us/step - loss: 0.0854 - mean_squared_error: 0.0591 - val_loss: 0.0952 - val_mean_squared_error: 0.0690\n",
      "Epoch 53/100\n",
      "20000/20000 [==============================] - 1s 40us/step - loss: 0.0847 - mean_squared_error: 0.0586 - val_loss: 0.0992 - val_mean_squared_error: 0.0731\n",
      "Epoch 54/100\n",
      "20000/20000 [==============================] - 1s 34us/step - loss: 0.0844 - mean_squared_error: 0.0583 - val_loss: 0.0954 - val_mean_squared_error: 0.0694\n",
      "Epoch 55/100\n",
      "20000/20000 [==============================] - 1s 35us/step - loss: 0.0841 - mean_squared_error: 0.0581 - val_loss: 0.0935 - val_mean_squared_error: 0.0676\n",
      "Epoch 56/100\n",
      "20000/20000 [==============================] - 1s 41us/step - loss: 0.0835 - mean_squared_error: 0.0575 - val_loss: 0.0992 - val_mean_squared_error: 0.0734\n",
      "Epoch 57/100\n",
      "20000/20000 [==============================] - 1s 40us/step - loss: 0.0831 - mean_squared_error: 0.0572 - val_loss: 0.0921 - val_mean_squared_error: 0.0663\n",
      "Epoch 58/100\n",
      "20000/20000 [==============================] - 1s 39us/step - loss: 0.0832 - mean_squared_error: 0.0575 - val_loss: 0.0928 - val_mean_squared_error: 0.0671\n",
      "Epoch 59/100\n",
      "20000/20000 [==============================] - 1s 38us/step - loss: 0.0824 - mean_squared_error: 0.0567 - val_loss: 0.0933 - val_mean_squared_error: 0.0677\n",
      "Epoch 60/100\n",
      "20000/20000 [==============================] - 1s 41us/step - loss: 0.0820 - mean_squared_error: 0.0564 - val_loss: 0.0918 - val_mean_squared_error: 0.0663\n",
      "Epoch 61/100\n",
      "20000/20000 [==============================] - 1s 48us/step - loss: 0.0814 - mean_squared_error: 0.0559 - val_loss: 0.0938 - val_mean_squared_error: 0.0684\n",
      "Epoch 62/100\n",
      "20000/20000 [==============================] - 1s 42us/step - loss: 0.0814 - mean_squared_error: 0.0560 - val_loss: 0.0925 - val_mean_squared_error: 0.0672\n",
      "\n",
      "Epoch 00062: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 63/100\n",
      "20000/20000 [==============================] - 1s 44us/step - loss: 0.0751 - mean_squared_error: 0.0498 - val_loss: 0.0867 - val_mean_squared_error: 0.0614\n",
      "Epoch 64/100\n",
      "20000/20000 [==============================] - 1s 38us/step - loss: 0.0749 - mean_squared_error: 0.0497 - val_loss: 0.0874 - val_mean_squared_error: 0.0621\n",
      "Epoch 65/100\n",
      "20000/20000 [==============================] - 1s 35us/step - loss: 0.0747 - mean_squared_error: 0.0494 - val_loss: 0.0867 - val_mean_squared_error: 0.0615\n",
      "Epoch 66/100\n",
      "20000/20000 [==============================] - 1s 35us/step - loss: 0.0745 - mean_squared_error: 0.0494 - val_loss: 0.0868 - val_mean_squared_error: 0.0617\n",
      "Epoch 67/100\n",
      "20000/20000 [==============================] - 1s 34us/step - loss: 0.0744 - mean_squared_error: 0.0493 - val_loss: 0.0871 - val_mean_squared_error: 0.0620\n",
      "Epoch 68/100\n",
      "20000/20000 [==============================] - 1s 35us/step - loss: 0.0741 - mean_squared_error: 0.0491 - val_loss: 0.0866 - val_mean_squared_error: 0.0616\n",
      "\n",
      "Epoch 00068: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 69/100\n",
      "20000/20000 [==============================] - 1s 36us/step - loss: 0.0713 - mean_squared_error: 0.0463 - val_loss: 0.0841 - val_mean_squared_error: 0.0591\n",
      "Epoch 70/100\n",
      "20000/20000 [==============================] - 1s 39us/step - loss: 0.0711 - mean_squared_error: 0.0462 - val_loss: 0.0839 - val_mean_squared_error: 0.0589\n",
      "Epoch 71/100\n",
      "20000/20000 [==============================] - 1s 44us/step - loss: 0.0711 - mean_squared_error: 0.0462 - val_loss: 0.0840 - val_mean_squared_error: 0.0591\n",
      "Epoch 72/100\n",
      "20000/20000 [==============================] - 1s 41us/step - loss: 0.0708 - mean_squared_error: 0.0459 - val_loss: 0.0835 - val_mean_squared_error: 0.0586\n",
      "Epoch 73/100\n",
      "20000/20000 [==============================] - 1s 35us/step - loss: 0.0705 - mean_squared_error: 0.0457 - val_loss: 0.0843 - val_mean_squared_error: 0.0595\n",
      "Epoch 74/100\n",
      "20000/20000 [==============================] - 1s 36us/step - loss: 0.0706 - mean_squared_error: 0.0457 - val_loss: 0.0834 - val_mean_squared_error: 0.0586\n",
      "Epoch 75/100\n",
      "20000/20000 [==============================] - 1s 36us/step - loss: 0.0704 - mean_squared_error: 0.0457 - val_loss: 0.0834 - val_mean_squared_error: 0.0587\n",
      "Epoch 76/100\n",
      "20000/20000 [==============================] - 1s 36us/step - loss: 0.0702 - mean_squared_error: 0.0455 - val_loss: 0.0831 - val_mean_squared_error: 0.0584\n",
      "Epoch 77/100\n",
      "20000/20000 [==============================] - 1s 31us/step - loss: 0.0702 - mean_squared_error: 0.0455 - val_loss: 0.0841 - val_mean_squared_error: 0.0594\n",
      "Epoch 78/100\n",
      "20000/20000 [==============================] - 1s 33us/step - loss: 0.0702 - mean_squared_error: 0.0455 - val_loss: 0.0835 - val_mean_squared_error: 0.0589\n",
      "Epoch 79/100\n",
      "20000/20000 [==============================] - 1s 36us/step - loss: 0.0701 - mean_squared_error: 0.0454 - val_loss: 0.0837 - val_mean_squared_error: 0.0591\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 1s 36us/step - loss: 0.0699 - mean_squared_error: 0.0453 - val_loss: 0.0831 - val_mean_squared_error: 0.0586\n",
      "Epoch 81/100\n",
      "20000/20000 [==============================] - 1s 33us/step - loss: 0.0697 - mean_squared_error: 0.0452 - val_loss: 0.0825 - val_mean_squared_error: 0.0580\n",
      "Epoch 82/100\n",
      "20000/20000 [==============================] - 1s 33us/step - loss: 0.0697 - mean_squared_error: 0.0452 - val_loss: 0.0840 - val_mean_squared_error: 0.0595\n",
      "Epoch 83/100\n",
      "20000/20000 [==============================] - 1s 33us/step - loss: 0.0694 - mean_squared_error: 0.0449 - val_loss: 0.0825 - val_mean_squared_error: 0.0580\n",
      "Epoch 84/100\n",
      "20000/20000 [==============================] - 1s 36us/step - loss: 0.0694 - mean_squared_error: 0.0450 - val_loss: 0.0836 - val_mean_squared_error: 0.0592\n",
      "Epoch 85/100\n",
      "20000/20000 [==============================] - 1s 34us/step - loss: 0.0692 - mean_squared_error: 0.0448 - val_loss: 0.0821 - val_mean_squared_error: 0.0577\n",
      "Epoch 86/100\n",
      "20000/20000 [==============================] - 1s 36us/step - loss: 0.0691 - mean_squared_error: 0.0447 - val_loss: 0.0831 - val_mean_squared_error: 0.0588\n",
      "Epoch 87/100\n",
      "20000/20000 [==============================] - 1s 34us/step - loss: 0.0689 - mean_squared_error: 0.0446 - val_loss: 0.0831 - val_mean_squared_error: 0.0589\n",
      "Epoch 88/100\n",
      "20000/20000 [==============================] - 1s 33us/step - loss: 0.0687 - mean_squared_error: 0.0444 - val_loss: 0.0825 - val_mean_squared_error: 0.0582\n",
      "Epoch 89/100\n",
      "20000/20000 [==============================] - 1s 32us/step - loss: 0.0688 - mean_squared_error: 0.0445 - val_loss: 0.0816 - val_mean_squared_error: 0.0573\n",
      "Epoch 90/100\n",
      "20000/20000 [==============================] - 1s 33us/step - loss: 0.0685 - mean_squared_error: 0.0443 - val_loss: 0.0823 - val_mean_squared_error: 0.0581\n",
      "Epoch 91/100\n",
      "20000/20000 [==============================] - 1s 35us/step - loss: 0.0684 - mean_squared_error: 0.0442 - val_loss: 0.0815 - val_mean_squared_error: 0.0573\n",
      "Epoch 92/100\n",
      "20000/20000 [==============================] - 1s 32us/step - loss: 0.0684 - mean_squared_error: 0.0443 - val_loss: 0.0823 - val_mean_squared_error: 0.0582\n",
      "Epoch 93/100\n",
      "20000/20000 [==============================] - 1s 35us/step - loss: 0.0684 - mean_squared_error: 0.0443 - val_loss: 0.0816 - val_mean_squared_error: 0.0575\n",
      "Epoch 94/100\n",
      "20000/20000 [==============================] - 1s 35us/step - loss: 0.0681 - mean_squared_error: 0.0440 - val_loss: 0.0822 - val_mean_squared_error: 0.0581\n",
      "\n",
      "Epoch 00094: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 95/100\n",
      "20000/20000 [==============================] - 1s 32us/step - loss: 0.0665 - mean_squared_error: 0.0425 - val_loss: 0.0805 - val_mean_squared_error: 0.0565\n",
      "Epoch 96/100\n",
      "20000/20000 [==============================] - 1s 36us/step - loss: 0.0664 - mean_squared_error: 0.0424 - val_loss: 0.0808 - val_mean_squared_error: 0.0568\n",
      "Epoch 97/100\n",
      "20000/20000 [==============================] - 1s 38us/step - loss: 0.0663 - mean_squared_error: 0.0423 - val_loss: 0.0801 - val_mean_squared_error: 0.0562\n",
      "Epoch 98/100\n",
      "20000/20000 [==============================] - 1s 35us/step - loss: 0.0663 - mean_squared_error: 0.0423 - val_loss: 0.0802 - val_mean_squared_error: 0.0562\n",
      "Epoch 99/100\n",
      "20000/20000 [==============================] - 1s 36us/step - loss: 0.0663 - mean_squared_error: 0.0424 - val_loss: 0.0801 - val_mean_squared_error: 0.0561\n",
      "Epoch 100/100\n",
      "20000/20000 [==============================] - 1s 37us/step - loss: 0.0661 - mean_squared_error: 0.0422 - val_loss: 0.0802 - val_mean_squared_error: 0.0563\n",
      "models/model_RoboschoolAnt-v1_training_set_size_20000_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm/base.hdf5\n",
      "Done training model for RoboschoolAnt-v1\n",
      "{'model_name': 'model_RoboschoolAnt-v1_training_set_size_20000_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm', 'model_path': 'models/model_RoboschoolAnt-v1_training_set_size_20000_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm', 'test_mse': 0.056303518806646996, 'train_mse': 0.05093946977465122, 'dataset_name': 'RoboschoolAnt-v1'}\n",
      "Training a FC ANN for env RoboschoolAnt-v1 over a training set size 30000 \n",
      "model_name='model_RoboschoolAnt-v1_training_set_size_30000_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm'\n",
      "model_path='models/model_RoboschoolAnt-v1_training_set_size_30000_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm'\n",
      "Train on 30000 samples, validate on 5000 samples\n",
      "Epoch 1/100\n",
      "30000/30000 [==============================] - 2s 61us/step - loss: 0.4388 - mean_squared_error: 0.4122 - val_loss: 0.2662 - val_mean_squared_error: 0.2378\n",
      "Epoch 2/100\n",
      "30000/30000 [==============================] - 1s 34us/step - loss: 0.2279 - mean_squared_error: 0.1986 - val_loss: 0.2056 - val_mean_squared_error: 0.1755\n",
      "Epoch 3/100\n",
      "30000/30000 [==============================] - 1s 35us/step - loss: 0.1885 - mean_squared_error: 0.1579 - val_loss: 0.1865 - val_mean_squared_error: 0.1556\n",
      "Epoch 4/100\n",
      "30000/30000 [==============================] - 1s 37us/step - loss: 0.1685 - mean_squared_error: 0.1373 - val_loss: 0.1597 - val_mean_squared_error: 0.1282\n",
      "Epoch 5/100\n",
      "30000/30000 [==============================] - 1s 38us/step - loss: 0.1554 - mean_squared_error: 0.1238 - val_loss: 0.1514 - val_mean_squared_error: 0.1198\n",
      "Epoch 6/100\n",
      "30000/30000 [==============================] - 1s 34us/step - loss: 0.1464 - mean_squared_error: 0.1147 - val_loss: 0.1462 - val_mean_squared_error: 0.1146\n",
      "Epoch 7/100\n",
      "30000/30000 [==============================] - 1s 34us/step - loss: 0.1386 - mean_squared_error: 0.1070 - val_loss: 0.1362 - val_mean_squared_error: 0.1047\n",
      "Epoch 8/100\n",
      "30000/30000 [==============================] - 1s 33us/step - loss: 0.1331 - mean_squared_error: 0.1016 - val_loss: 0.1347 - val_mean_squared_error: 0.1033\n",
      "Epoch 9/100\n",
      "30000/30000 [==============================] - 1s 34us/step - loss: 0.1284 - mean_squared_error: 0.0972 - val_loss: 0.1310 - val_mean_squared_error: 0.0999\n",
      "Epoch 10/100\n",
      "30000/30000 [==============================] - 1s 35us/step - loss: 0.1247 - mean_squared_error: 0.0937 - val_loss: 0.1284 - val_mean_squared_error: 0.0975\n",
      "Epoch 11/100\n",
      "30000/30000 [==============================] - 1s 38us/step - loss: 0.1211 - mean_squared_error: 0.0903 - val_loss: 0.1238 - val_mean_squared_error: 0.0931\n",
      "Epoch 12/100\n",
      "30000/30000 [==============================] - 1s 40us/step - loss: 0.1179 - mean_squared_error: 0.0873 - val_loss: 0.1187 - val_mean_squared_error: 0.0883\n",
      "Epoch 13/100\n",
      "30000/30000 [==============================] - 1s 34us/step - loss: 0.1155 - mean_squared_error: 0.0852 - val_loss: 0.1184 - val_mean_squared_error: 0.0883\n",
      "Epoch 14/100\n",
      "30000/30000 [==============================] - 1s 36us/step - loss: 0.1128 - mean_squared_error: 0.0828 - val_loss: 0.1182 - val_mean_squared_error: 0.0883\n",
      "Epoch 15/100\n",
      "30000/30000 [==============================] - 1s 35us/step - loss: 0.1107 - mean_squared_error: 0.0810 - val_loss: 0.1216 - val_mean_squared_error: 0.0920\n",
      "Epoch 16/100\n",
      "30000/30000 [==============================] - 1s 37us/step - loss: 0.1087 - mean_squared_error: 0.0792 - val_loss: 0.1150 - val_mean_squared_error: 0.0856\n",
      "Epoch 17/100\n",
      "30000/30000 [==============================] - 1s 40us/step - loss: 0.1071 - mean_squared_error: 0.0778 - val_loss: 0.1089 - val_mean_squared_error: 0.0798\n",
      "Epoch 18/100\n",
      "30000/30000 [==============================] - 1s 35us/step - loss: 0.1050 - mean_squared_error: 0.0760 - val_loss: 0.1111 - val_mean_squared_error: 0.0823\n",
      "Epoch 19/100\n",
      "30000/30000 [==============================] - 1s 34us/step - loss: 0.1036 - mean_squared_error: 0.0749 - val_loss: 0.1088 - val_mean_squared_error: 0.0802\n",
      "Epoch 20/100\n",
      "30000/30000 [==============================] - 1s 35us/step - loss: 0.1027 - mean_squared_error: 0.0742 - val_loss: 0.1066 - val_mean_squared_error: 0.0783\n",
      "Epoch 21/100\n",
      "30000/30000 [==============================] - 1s 40us/step - loss: 0.1017 - mean_squared_error: 0.0734 - val_loss: 0.1093 - val_mean_squared_error: 0.0811\n",
      "Epoch 22/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000/30000 [==============================] - 1s 45us/step - loss: 0.0993 - mean_squared_error: 0.0713 - val_loss: 0.1054 - val_mean_squared_error: 0.0775\n",
      "Epoch 23/100\n",
      "30000/30000 [==============================] - 1s 35us/step - loss: 0.0980 - mean_squared_error: 0.0702 - val_loss: 0.1012 - val_mean_squared_error: 0.0735\n",
      "Epoch 24/100\n",
      "30000/30000 [==============================] - 1s 36us/step - loss: 0.0973 - mean_squared_error: 0.0697 - val_loss: 0.1021 - val_mean_squared_error: 0.0747\n",
      "Epoch 25/100\n",
      "30000/30000 [==============================] - 1s 38us/step - loss: 0.0963 - mean_squared_error: 0.0689 - val_loss: 0.1017 - val_mean_squared_error: 0.0744\n",
      "Epoch 26/100\n",
      "30000/30000 [==============================] - 1s 35us/step - loss: 0.0956 - mean_squared_error: 0.0684 - val_loss: 0.1001 - val_mean_squared_error: 0.0730\n",
      "Epoch 27/100\n",
      "30000/30000 [==============================] - 1s 36us/step - loss: 0.0939 - mean_squared_error: 0.0670 - val_loss: 0.0993 - val_mean_squared_error: 0.0724\n",
      "Epoch 28/100\n",
      "30000/30000 [==============================] - 1s 33us/step - loss: 0.0934 - mean_squared_error: 0.0666 - val_loss: 0.0991 - val_mean_squared_error: 0.0724\n",
      "Epoch 29/100\n",
      "30000/30000 [==============================] - 1s 35us/step - loss: 0.0928 - mean_squared_error: 0.0662 - val_loss: 0.0979 - val_mean_squared_error: 0.0713\n",
      "Epoch 30/100\n",
      "30000/30000 [==============================] - 1s 32us/step - loss: 0.0916 - mean_squared_error: 0.0652 - val_loss: 0.0964 - val_mean_squared_error: 0.0700\n",
      "Epoch 31/100\n",
      "30000/30000 [==============================] - 1s 32us/step - loss: 0.0912 - mean_squared_error: 0.0649 - val_loss: 0.1046 - val_mean_squared_error: 0.0784\n",
      "Epoch 32/100\n",
      "30000/30000 [==============================] - 1s 33us/step - loss: 0.0905 - mean_squared_error: 0.0643 - val_loss: 0.0961 - val_mean_squared_error: 0.0701\n",
      "Epoch 33/100\n",
      "30000/30000 [==============================] - 1s 31us/step - loss: 0.0898 - mean_squared_error: 0.0639 - val_loss: 0.0954 - val_mean_squared_error: 0.0694\n",
      "Epoch 34/100\n",
      "30000/30000 [==============================] - 1s 32us/step - loss: 0.0893 - mean_squared_error: 0.0635 - val_loss: 0.1039 - val_mean_squared_error: 0.0781\n",
      "Epoch 35/100\n",
      "30000/30000 [==============================] - 1s 32us/step - loss: 0.0882 - mean_squared_error: 0.0625 - val_loss: 0.0923 - val_mean_squared_error: 0.0666\n",
      "Epoch 36/100\n",
      "30000/30000 [==============================] - 1s 34us/step - loss: 0.0869 - mean_squared_error: 0.0613 - val_loss: 0.0932 - val_mean_squared_error: 0.0676\n",
      "Epoch 37/100\n",
      "30000/30000 [==============================] - 1s 39us/step - loss: 0.0869 - mean_squared_error: 0.0614 - val_loss: 0.0970 - val_mean_squared_error: 0.0716\n",
      "Epoch 38/100\n",
      "30000/30000 [==============================] - 1s 36us/step - loss: 0.0867 - mean_squared_error: 0.0613 - val_loss: 0.0936 - val_mean_squared_error: 0.0683\n",
      "Epoch 39/100\n",
      "30000/30000 [==============================] - 1s 32us/step - loss: 0.0858 - mean_squared_error: 0.0605 - val_loss: 0.0955 - val_mean_squared_error: 0.0703\n",
      "Epoch 40/100\n",
      "30000/30000 [==============================] - 1s 31us/step - loss: 0.0848 - mean_squared_error: 0.0596 - val_loss: 0.0932 - val_mean_squared_error: 0.0681\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 41/100\n",
      "30000/30000 [==============================] - 2s 54us/step - loss: 0.0789 - mean_squared_error: 0.0539 - val_loss: 0.0867 - val_mean_squared_error: 0.0617\n",
      "Epoch 42/100\n",
      "30000/30000 [==============================] - 1s 31us/step - loss: 0.0787 - mean_squared_error: 0.0537 - val_loss: 0.0859 - val_mean_squared_error: 0.0610\n",
      "Epoch 43/100\n",
      "30000/30000 [==============================] - 1s 34us/step - loss: 0.0784 - mean_squared_error: 0.0535 - val_loss: 0.0861 - val_mean_squared_error: 0.0612\n",
      "Epoch 44/100\n",
      "30000/30000 [==============================] - 1s 30us/step - loss: 0.0779 - mean_squared_error: 0.0531 - val_loss: 0.0855 - val_mean_squared_error: 0.0607\n",
      "Epoch 45/100\n",
      "30000/30000 [==============================] - 1s 31us/step - loss: 0.0778 - mean_squared_error: 0.0531 - val_loss: 0.0860 - val_mean_squared_error: 0.0613\n",
      "Epoch 46/100\n",
      "30000/30000 [==============================] - 1s 34us/step - loss: 0.0773 - mean_squared_error: 0.0527 - val_loss: 0.0848 - val_mean_squared_error: 0.0602\n",
      "Epoch 47/100\n",
      "30000/30000 [==============================] - 1s 35us/step - loss: 0.0772 - mean_squared_error: 0.0526 - val_loss: 0.0848 - val_mean_squared_error: 0.0603\n",
      "Epoch 48/100\n",
      "30000/30000 [==============================] - 1s 40us/step - loss: 0.0769 - mean_squared_error: 0.0524 - val_loss: 0.0832 - val_mean_squared_error: 0.0588\n",
      "Epoch 49/100\n",
      "30000/30000 [==============================] - 1s 39us/step - loss: 0.0761 - mean_squared_error: 0.0518 - val_loss: 0.0840 - val_mean_squared_error: 0.0597\n",
      "Epoch 50/100\n",
      "30000/30000 [==============================] - 1s 39us/step - loss: 0.0760 - mean_squared_error: 0.0517 - val_loss: 0.0844 - val_mean_squared_error: 0.0601\n",
      "Epoch 51/100\n",
      "30000/30000 [==============================] - 1s 42us/step - loss: 0.0757 - mean_squared_error: 0.0515 - val_loss: 0.0839 - val_mean_squared_error: 0.0598\n",
      "Epoch 52/100\n",
      "30000/30000 [==============================] - 1s 39us/step - loss: 0.0755 - mean_squared_error: 0.0513 - val_loss: 0.0833 - val_mean_squared_error: 0.0592\n",
      "Epoch 53/100\n",
      "30000/30000 [==============================] - 1s 37us/step - loss: 0.0752 - mean_squared_error: 0.0511 - val_loss: 0.0818 - val_mean_squared_error: 0.0578\n",
      "Epoch 54/100\n",
      "30000/30000 [==============================] - 1s 34us/step - loss: 0.0747 - mean_squared_error: 0.0507 - val_loss: 0.0828 - val_mean_squared_error: 0.0588\n",
      "Epoch 55/100\n",
      "30000/30000 [==============================] - 1s 34us/step - loss: 0.0745 - mean_squared_error: 0.0506 - val_loss: 0.0838 - val_mean_squared_error: 0.0600\n",
      "Epoch 56/100\n",
      "30000/30000 [==============================] - 1s 34us/step - loss: 0.0744 - mean_squared_error: 0.0506 - val_loss: 0.0827 - val_mean_squared_error: 0.0590\n",
      "Epoch 57/100\n",
      "30000/30000 [==============================] - 1s 35us/step - loss: 0.0743 - mean_squared_error: 0.0505 - val_loss: 0.0825 - val_mean_squared_error: 0.0588\n",
      "Epoch 58/100\n",
      "30000/30000 [==============================] - 1s 34us/step - loss: 0.0739 - mean_squared_error: 0.0502 - val_loss: 0.0831 - val_mean_squared_error: 0.0595\n",
      "\n",
      "Epoch 00058: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 59/100\n",
      "30000/30000 [==============================] - 1s 34us/step - loss: 0.0707 - mean_squared_error: 0.0471 - val_loss: 0.0797 - val_mean_squared_error: 0.0561\n",
      "Epoch 60/100\n",
      "30000/30000 [==============================] - 1s 32us/step - loss: 0.0705 - mean_squared_error: 0.0469 - val_loss: 0.0794 - val_mean_squared_error: 0.0558\n",
      "Epoch 61/100\n",
      "30000/30000 [==============================] - 1s 34us/step - loss: 0.0703 - mean_squared_error: 0.0468 - val_loss: 0.0787 - val_mean_squared_error: 0.0552\n",
      "Epoch 62/100\n",
      "30000/30000 [==============================] - 1s 36us/step - loss: 0.0701 - mean_squared_error: 0.0467 - val_loss: 0.0789 - val_mean_squared_error: 0.0554\n",
      "Epoch 63/100\n",
      "30000/30000 [==============================] - 1s 35us/step - loss: 0.0701 - mean_squared_error: 0.0467 - val_loss: 0.0790 - val_mean_squared_error: 0.0555\n",
      "Epoch 64/100\n",
      "30000/30000 [==============================] - 1s 32us/step - loss: 0.0699 - mean_squared_error: 0.0465 - val_loss: 0.0783 - val_mean_squared_error: 0.0549\n",
      "Epoch 65/100\n",
      "30000/30000 [==============================] - 1s 39us/step - loss: 0.0697 - mean_squared_error: 0.0463 - val_loss: 0.0783 - val_mean_squared_error: 0.0550\n",
      "Epoch 66/100\n",
      "30000/30000 [==============================] - 1s 39us/step - loss: 0.0697 - mean_squared_error: 0.0464 - val_loss: 0.0782 - val_mean_squared_error: 0.0549\n",
      "Epoch 67/100\n",
      "30000/30000 [==============================] - 1s 35us/step - loss: 0.0695 - mean_squared_error: 0.0462 - val_loss: 0.0782 - val_mean_squared_error: 0.0550\n",
      "Epoch 68/100\n",
      "30000/30000 [==============================] - 1s 30us/step - loss: 0.0693 - mean_squared_error: 0.0461 - val_loss: 0.0782 - val_mean_squared_error: 0.0551\n",
      "Epoch 69/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000/30000 [==============================] - 1s 34us/step - loss: 0.0692 - mean_squared_error: 0.0460 - val_loss: 0.0782 - val_mean_squared_error: 0.0551\n",
      "\n",
      "Epoch 00069: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 70/100\n",
      "30000/30000 [==============================] - 1s 32us/step - loss: 0.0674 - mean_squared_error: 0.0443 - val_loss: 0.0764 - val_mean_squared_error: 0.0533\n",
      "Epoch 71/100\n",
      "30000/30000 [==============================] - 1s 33us/step - loss: 0.0673 - mean_squared_error: 0.0442 - val_loss: 0.0772 - val_mean_squared_error: 0.0541\n",
      "Epoch 72/100\n",
      "30000/30000 [==============================] - 1s 32us/step - loss: 0.0673 - mean_squared_error: 0.0442 - val_loss: 0.0771 - val_mean_squared_error: 0.0540\n",
      "Epoch 73/100\n",
      "30000/30000 [==============================] - 1s 35us/step - loss: 0.0672 - mean_squared_error: 0.0441 - val_loss: 0.0762 - val_mean_squared_error: 0.0531\n",
      "Epoch 74/100\n",
      "30000/30000 [==============================] - 1s 36us/step - loss: 0.0672 - mean_squared_error: 0.0441 - val_loss: 0.0765 - val_mean_squared_error: 0.0535\n",
      "Epoch 75/100\n",
      "30000/30000 [==============================] - 1s 31us/step - loss: 0.0671 - mean_squared_error: 0.0440 - val_loss: 0.0765 - val_mean_squared_error: 0.0535\n",
      "Epoch 76/100\n",
      "30000/30000 [==============================] - 1s 32us/step - loss: 0.0669 - mean_squared_error: 0.0439 - val_loss: 0.0763 - val_mean_squared_error: 0.0534\n",
      "Epoch 77/100\n",
      "30000/30000 [==============================] - 1s 31us/step - loss: 0.0669 - mean_squared_error: 0.0439 - val_loss: 0.0766 - val_mean_squared_error: 0.0536\n",
      "Epoch 78/100\n",
      "30000/30000 [==============================] - 1s 32us/step - loss: 0.0668 - mean_squared_error: 0.0438 - val_loss: 0.0762 - val_mean_squared_error: 0.0533\n",
      "\n",
      "Epoch 00078: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 79/100\n",
      "30000/30000 [==============================] - 1s 40us/step - loss: 0.0659 - mean_squared_error: 0.0430 - val_loss: 0.0755 - val_mean_squared_error: 0.0526\n",
      "Epoch 80/100\n",
      "30000/30000 [==============================] - 1s 34us/step - loss: 0.0659 - mean_squared_error: 0.0430 - val_loss: 0.0753 - val_mean_squared_error: 0.0524\n",
      "Epoch 81/100\n",
      "30000/30000 [==============================] - 1s 37us/step - loss: 0.0658 - mean_squared_error: 0.0429 - val_loss: 0.0757 - val_mean_squared_error: 0.0528\n",
      "Epoch 82/100\n",
      "30000/30000 [==============================] - 1s 33us/step - loss: 0.0658 - mean_squared_error: 0.0429 - val_loss: 0.0754 - val_mean_squared_error: 0.0526\n",
      "Epoch 83/100\n",
      "30000/30000 [==============================] - 1s 33us/step - loss: 0.0657 - mean_squared_error: 0.0428 - val_loss: 0.0752 - val_mean_squared_error: 0.0523\n",
      "Epoch 84/100\n",
      "30000/30000 [==============================] - 1s 34us/step - loss: 0.0657 - mean_squared_error: 0.0428 - val_loss: 0.0757 - val_mean_squared_error: 0.0528\n",
      "Epoch 85/100\n",
      "30000/30000 [==============================] - 1s 33us/step - loss: 0.0656 - mean_squared_error: 0.0428 - val_loss: 0.0751 - val_mean_squared_error: 0.0523\n",
      "Epoch 86/100\n",
      "30000/30000 [==============================] - 1s 35us/step - loss: 0.0656 - mean_squared_error: 0.0427 - val_loss: 0.0752 - val_mean_squared_error: 0.0524\n",
      "Epoch 87/100\n",
      "30000/30000 [==============================] - 1s 35us/step - loss: 0.0656 - mean_squared_error: 0.0427 - val_loss: 0.0751 - val_mean_squared_error: 0.0522\n",
      "Epoch 88/100\n",
      "30000/30000 [==============================] - 1s 34us/step - loss: 0.0655 - mean_squared_error: 0.0427 - val_loss: 0.0752 - val_mean_squared_error: 0.0524\n",
      "\n",
      "Epoch 00088: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 89/100\n",
      "30000/30000 [==============================] - 1s 36us/step - loss: 0.0650 - mean_squared_error: 0.0422 - val_loss: 0.0748 - val_mean_squared_error: 0.0520\n",
      "Epoch 90/100\n",
      "30000/30000 [==============================] - 1s 39us/step - loss: 0.0650 - mean_squared_error: 0.0422 - val_loss: 0.0749 - val_mean_squared_error: 0.0521\n",
      "Epoch 91/100\n",
      "30000/30000 [==============================] - 1s 34us/step - loss: 0.0650 - mean_squared_error: 0.0422 - val_loss: 0.0747 - val_mean_squared_error: 0.0519\n",
      "Epoch 92/100\n",
      "30000/30000 [==============================] - 1s 34us/step - loss: 0.0650 - mean_squared_error: 0.0422 - val_loss: 0.0747 - val_mean_squared_error: 0.0519\n",
      "Epoch 93/100\n",
      "30000/30000 [==============================] - 1s 33us/step - loss: 0.0649 - mean_squared_error: 0.0421 - val_loss: 0.0748 - val_mean_squared_error: 0.0520\n",
      "Epoch 94/100\n",
      "30000/30000 [==============================] - 1s 33us/step - loss: 0.0649 - mean_squared_error: 0.0421 - val_loss: 0.0746 - val_mean_squared_error: 0.0518\n",
      "Epoch 95/100\n",
      "30000/30000 [==============================] - 1s 33us/step - loss: 0.0649 - mean_squared_error: 0.0422 - val_loss: 0.0747 - val_mean_squared_error: 0.0520\n",
      "Epoch 96/100\n",
      "30000/30000 [==============================] - 1s 34us/step - loss: 0.0649 - mean_squared_error: 0.0421 - val_loss: 0.0747 - val_mean_squared_error: 0.0519\n",
      "Epoch 97/100\n",
      "30000/30000 [==============================] - 1s 32us/step - loss: 0.0648 - mean_squared_error: 0.0421 - val_loss: 0.0747 - val_mean_squared_error: 0.0520\n",
      "Epoch 98/100\n",
      "30000/30000 [==============================] - 1s 32us/step - loss: 0.0648 - mean_squared_error: 0.0421 - val_loss: 0.0746 - val_mean_squared_error: 0.0518loss: 0.0648 - mean_squar\n",
      "Epoch 99/100\n",
      "30000/30000 [==============================] - 1s 31us/step - loss: 0.0648 - mean_squared_error: 0.0421 - val_loss: 0.0746 - val_mean_squared_error: 0.0518\n",
      "\n",
      "Epoch 00099: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 100/100\n",
      "30000/30000 [==============================] - 1s 32us/step - loss: 0.0646 - mean_squared_error: 0.0418 - val_loss: 0.0745 - val_mean_squared_error: 0.0517\n",
      "models/model_RoboschoolAnt-v1_training_set_size_30000_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm/base.hdf5\n",
      "Done training model for RoboschoolAnt-v1\n",
      "{'model_name': 'model_RoboschoolAnt-v1_training_set_size_30000_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm', 'model_path': 'models/model_RoboschoolAnt-v1_training_set_size_30000_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm', 'test_mse': 0.05173593830657201, 'train_mse': 0.04581423393434081, 'dataset_name': 'RoboschoolAnt-v1'}\n",
      "Training a FC ANN for env RoboschoolAnt-v1 over a training set size 40000 \n",
      "model_name='model_RoboschoolAnt-v1_training_set_size_40000_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm'\n",
      "model_path='models/model_RoboschoolAnt-v1_training_set_size_40000_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm'\n",
      "Train on 40000 samples, validate on 5000 samples\n",
      "Epoch 1/100\n",
      "40000/40000 [==============================] - 2s 49us/step - loss: 0.3872 - mean_squared_error: 0.3598 - val_loss: 0.2310 - val_mean_squared_error: 0.2011\n",
      "Epoch 2/100\n",
      "40000/40000 [==============================] - 1s 33us/step - loss: 0.2030 - mean_squared_error: 0.1721 - val_loss: 0.1796 - val_mean_squared_error: 0.1480\n",
      "Epoch 3/100\n",
      "40000/40000 [==============================] - 1s 33us/step - loss: 0.1700 - mean_squared_error: 0.1381 - val_loss: 0.1576 - val_mean_squared_error: 0.1254\n",
      "Epoch 4/100\n",
      "40000/40000 [==============================] - 1s 33us/step - loss: 0.1522 - mean_squared_error: 0.1198 - val_loss: 0.1432 - val_mean_squared_error: 0.1108\n",
      "Epoch 5/100\n",
      "40000/40000 [==============================] - 1s 33us/step - loss: 0.1407 - mean_squared_error: 0.1083 - val_loss: 0.1375 - val_mean_squared_error: 0.1053\n",
      "Epoch 6/100\n",
      "40000/40000 [==============================] - 2s 38us/step - loss: 0.1328 - mean_squared_error: 0.1007 - val_loss: 0.1315 - val_mean_squared_error: 0.0995\n",
      "Epoch 7/100\n",
      "40000/40000 [==============================] - 1s 34us/step - loss: 0.1269 - mean_squared_error: 0.0951 - val_loss: 0.1287 - val_mean_squared_error: 0.0972\n",
      "Epoch 8/100\n",
      "40000/40000 [==============================] - 1s 33us/step - loss: 0.1219 - mean_squared_error: 0.0905 - val_loss: 0.1188 - val_mean_squared_error: 0.0876\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/40000 [==============================] - 1s 34us/step - loss: 0.1179 - mean_squared_error: 0.0869 - val_loss: 0.1190 - val_mean_squared_error: 0.0881\n",
      "Epoch 10/100\n",
      "40000/40000 [==============================] - 1s 33us/step - loss: 0.1142 - mean_squared_error: 0.0836 - val_loss: 0.1125 - val_mean_squared_error: 0.0821\n",
      "Epoch 11/100\n",
      "40000/40000 [==============================] - 1s 33us/step - loss: 0.1114 - mean_squared_error: 0.0812 - val_loss: 0.1152 - val_mean_squared_error: 0.0851\n",
      "Epoch 12/100\n",
      "40000/40000 [==============================] - 1s 32us/step - loss: 0.1084 - mean_squared_error: 0.0785 - val_loss: 0.1096 - val_mean_squared_error: 0.0799\n",
      "Epoch 13/100\n",
      "40000/40000 [==============================] - 1s 33us/step - loss: 0.1065 - mean_squared_error: 0.0770 - val_loss: 0.1073 - val_mean_squared_error: 0.0779\n",
      "Epoch 14/100\n",
      "40000/40000 [==============================] - 1s 33us/step - loss: 0.1036 - mean_squared_error: 0.0745 - val_loss: 0.1084 - val_mean_squared_error: 0.0794\n",
      "Epoch 15/100\n",
      "40000/40000 [==============================] - 1s 31us/step - loss: 0.1022 - mean_squared_error: 0.0733 - val_loss: 0.1060 - val_mean_squared_error: 0.0773\n",
      "Epoch 16/100\n",
      "40000/40000 [==============================] - 1s 32us/step - loss: 0.1002 - mean_squared_error: 0.0717 - val_loss: 0.1018 - val_mean_squared_error: 0.0734\n",
      "Epoch 17/100\n",
      "40000/40000 [==============================] - 1s 31us/step - loss: 0.0985 - mean_squared_error: 0.0703 - val_loss: 0.1029 - val_mean_squared_error: 0.0749\n",
      "Epoch 18/100\n",
      "40000/40000 [==============================] - 1s 34us/step - loss: 0.0973 - mean_squared_error: 0.0694 - val_loss: 0.0998 - val_mean_squared_error: 0.0721\n",
      "Epoch 19/100\n",
      "40000/40000 [==============================] - 1s 32us/step - loss: 0.0959 - mean_squared_error: 0.0682 - val_loss: 0.0991 - val_mean_squared_error: 0.0716\n",
      "Epoch 20/100\n",
      "40000/40000 [==============================] - 1s 33us/step - loss: 0.0947 - mean_squared_error: 0.0674 - val_loss: 0.0983 - val_mean_squared_error: 0.0711\n",
      "Epoch 21/100\n",
      "40000/40000 [==============================] - 1s 36us/step - loss: 0.0936 - mean_squared_error: 0.0665 - val_loss: 0.0968 - val_mean_squared_error: 0.0698\n",
      "Epoch 22/100\n",
      "40000/40000 [==============================] - 2s 40us/step - loss: 0.0921 - mean_squared_error: 0.0653 - val_loss: 0.0946 - val_mean_squared_error: 0.0678\n",
      "Epoch 23/100\n",
      "40000/40000 [==============================] - 2s 48us/step - loss: 0.0909 - mean_squared_error: 0.0643 - val_loss: 0.0982 - val_mean_squared_error: 0.0716\n",
      "Epoch 24/100\n",
      "40000/40000 [==============================] - 1s 36us/step - loss: 0.0898 - mean_squared_error: 0.0634 - val_loss: 0.0935 - val_mean_squared_error: 0.0672\n",
      "Epoch 25/100\n",
      "40000/40000 [==============================] - 2s 41us/step - loss: 0.0894 - mean_squared_error: 0.0632 - val_loss: 0.0927 - val_mean_squared_error: 0.0665\n",
      "Epoch 26/100\n",
      "40000/40000 [==============================] - 2s 38us/step - loss: 0.0882 - mean_squared_error: 0.0622 - val_loss: 0.0915 - val_mean_squared_error: 0.0656\n",
      "Epoch 27/100\n",
      "40000/40000 [==============================] - 1s 35us/step - loss: 0.0874 - mean_squared_error: 0.0615 - val_loss: 0.0916 - val_mean_squared_error: 0.0659\n",
      "Epoch 28/100\n",
      "40000/40000 [==============================] - 1s 35us/step - loss: 0.0864 - mean_squared_error: 0.0608 - val_loss: 0.0933 - val_mean_squared_error: 0.0678\n",
      "Epoch 29/100\n",
      "40000/40000 [==============================] - 1s 34us/step - loss: 0.0860 - mean_squared_error: 0.0606 - val_loss: 0.0899 - val_mean_squared_error: 0.0646\n",
      "Epoch 30/100\n",
      "40000/40000 [==============================] - 1s 33us/step - loss: 0.0850 - mean_squared_error: 0.0598 - val_loss: 0.0876 - val_mean_squared_error: 0.0624\n",
      "Epoch 31/100\n",
      "40000/40000 [==============================] - 1s 34us/step - loss: 0.0844 - mean_squared_error: 0.0593 - val_loss: 0.0873 - val_mean_squared_error: 0.0623\n",
      "Epoch 32/100\n",
      "40000/40000 [==============================] - 1s 34us/step - loss: 0.0833 - mean_squared_error: 0.0583 - val_loss: 0.0868 - val_mean_squared_error: 0.0620\n",
      "Epoch 33/100\n",
      "40000/40000 [==============================] - 1s 35us/step - loss: 0.0830 - mean_squared_error: 0.0582 - val_loss: 0.0884 - val_mean_squared_error: 0.0637\n",
      "Epoch 34/100\n",
      "40000/40000 [==============================] - 2s 49us/step - loss: 0.0825 - mean_squared_error: 0.0579 - val_loss: 0.0861 - val_mean_squared_error: 0.0615\n",
      "Epoch 35/100\n",
      "40000/40000 [==============================] - 2s 40us/step - loss: 0.0815 - mean_squared_error: 0.0571 - val_loss: 0.0850 - val_mean_squared_error: 0.0606\n",
      "Epoch 36/100\n",
      "40000/40000 [==============================] - 1s 33us/step - loss: 0.0815 - mean_squared_error: 0.0571 - val_loss: 0.0861 - val_mean_squared_error: 0.0619\n",
      "Epoch 37/100\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 0.0805 - mean_squared_error: 0.0562 - val_loss: 0.0847 - val_mean_squared_error: 0.0606\n",
      "Epoch 38/100\n",
      "40000/40000 [==============================] - 1s 33us/step - loss: 0.0798 - mean_squared_error: 0.0558 - val_loss: 0.0858 - val_mean_squared_error: 0.0618\n",
      "Epoch 39/100\n",
      "40000/40000 [==============================] - 2s 57us/step - loss: 0.0798 - mean_squared_error: 0.0559 - val_loss: 0.0835 - val_mean_squared_error: 0.0597\n",
      "Epoch 40/100\n",
      "40000/40000 [==============================] - 2s 42us/step - loss: 0.0792 - mean_squared_error: 0.0554 - val_loss: 0.0838 - val_mean_squared_error: 0.0601\n",
      "Epoch 41/100\n",
      "40000/40000 [==============================] - 1s 32us/step - loss: 0.0781 - mean_squared_error: 0.0544 - val_loss: 0.0833 - val_mean_squared_error: 0.0597\n",
      "Epoch 42/100\n",
      "40000/40000 [==============================] - 1s 32us/step - loss: 0.0784 - mean_squared_error: 0.0549 - val_loss: 0.0825 - val_mean_squared_error: 0.0590\n",
      "Epoch 43/100\n",
      "40000/40000 [==============================] - 1s 31us/step - loss: 0.0778 - mean_squared_error: 0.0543 - val_loss: 0.0826 - val_mean_squared_error: 0.0593\n",
      "Epoch 44/100\n",
      "40000/40000 [==============================] - 1s 31us/step - loss: 0.0777 - mean_squared_error: 0.0544 - val_loss: 0.0842 - val_mean_squared_error: 0.0609\n",
      "Epoch 45/100\n",
      "40000/40000 [==============================] - 1s 31us/step - loss: 0.0772 - mean_squared_error: 0.0540 - val_loss: 0.0839 - val_mean_squared_error: 0.0607\n",
      "Epoch 46/100\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 0.0765 - mean_squared_error: 0.0534 - val_loss: 0.0835 - val_mean_squared_error: 0.0604\n",
      "Epoch 47/100\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 0.0767 - mean_squared_error: 0.0536 - val_loss: 0.0827 - val_mean_squared_error: 0.0597\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 48/100\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 0.0701 - mean_squared_error: 0.0472 - val_loss: 0.0753 - val_mean_squared_error: 0.0524\n",
      "Epoch 49/100\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 0.0699 - mean_squared_error: 0.0471 - val_loss: 0.0772 - val_mean_squared_error: 0.0544\n",
      "Epoch 50/100\n",
      "40000/40000 [==============================] - 1s 31us/step - loss: 0.0695 - mean_squared_error: 0.0468 - val_loss: 0.0751 - val_mean_squared_error: 0.0524\n",
      "Epoch 51/100\n",
      "40000/40000 [==============================] - 1s 33us/step - loss: 0.0694 - mean_squared_error: 0.0468 - val_loss: 0.0748 - val_mean_squared_error: 0.0522\n",
      "Epoch 52/100\n",
      "40000/40000 [==============================] - 1s 32us/step - loss: 0.0692 - mean_squared_error: 0.0466 - val_loss: 0.0750 - val_mean_squared_error: 0.0525\n",
      "Epoch 53/100\n",
      "40000/40000 [==============================] - 1s 31us/step - loss: 0.0691 - mean_squared_error: 0.0466 - val_loss: 0.0739 - val_mean_squared_error: 0.0515\n",
      "Epoch 54/100\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 0.0687 - mean_squared_error: 0.0463 - val_loss: 0.0785 - val_mean_squared_error: 0.0561\n",
      "Epoch 55/100\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 0.0684 - mean_squared_error: 0.0461 - val_loss: 0.0763 - val_mean_squared_error: 0.0540\n",
      "Epoch 56/100\n",
      "40000/40000 [==============================] - 1s 32us/step - loss: 0.0682 - mean_squared_error: 0.0460 - val_loss: 0.0734 - val_mean_squared_error: 0.0512\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/40000 [==============================] - 1s 31us/step - loss: 0.0680 - mean_squared_error: 0.0458 - val_loss: 0.0742 - val_mean_squared_error: 0.0521\n",
      "Epoch 58/100\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 0.0678 - mean_squared_error: 0.0458 - val_loss: 0.0737 - val_mean_squared_error: 0.0516\n",
      "Epoch 59/100\n",
      "40000/40000 [==============================] - 1s 31us/step - loss: 0.0676 - mean_squared_error: 0.0456 - val_loss: 0.0741 - val_mean_squared_error: 0.0522\n",
      "Epoch 60/100\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 0.0672 - mean_squared_error: 0.0453 - val_loss: 0.0728 - val_mean_squared_error: 0.0509\n",
      "Epoch 61/100\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 0.0671 - mean_squared_error: 0.0452 - val_loss: 0.0744 - val_mean_squared_error: 0.0526\n",
      "Epoch 62/100\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 0.0670 - mean_squared_error: 0.0453 - val_loss: 0.0735 - val_mean_squared_error: 0.0518\n",
      "Epoch 63/100\n",
      "40000/40000 [==============================] - 1s 32us/step - loss: 0.0667 - mean_squared_error: 0.0450 - val_loss: 0.0731 - val_mean_squared_error: 0.0514\n",
      "Epoch 64/100\n",
      "40000/40000 [==============================] - 1s 33us/step - loss: 0.0664 - mean_squared_error: 0.0448 - val_loss: 0.0727 - val_mean_squared_error: 0.0511\n",
      "Epoch 65/100\n",
      "40000/40000 [==============================] - 1s 31us/step - loss: 0.0666 - mean_squared_error: 0.0450 - val_loss: 0.0766 - val_mean_squared_error: 0.0551\n",
      "\n",
      "Epoch 00065: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 66/100\n",
      "40000/40000 [==============================] - 1s 33us/step - loss: 0.0631 - mean_squared_error: 0.0417 - val_loss: 0.0694 - val_mean_squared_error: 0.0480\n",
      "Epoch 67/100\n",
      "40000/40000 [==============================] - 1s 36us/step - loss: 0.0629 - mean_squared_error: 0.0415 - val_loss: 0.0697 - val_mean_squared_error: 0.0483\n",
      "Epoch 68/100\n",
      "40000/40000 [==============================] - 2s 40us/step - loss: 0.0628 - mean_squared_error: 0.0414 - val_loss: 0.0701 - val_mean_squared_error: 0.0488\n",
      "Epoch 69/100\n",
      "40000/40000 [==============================] - 2s 40us/step - loss: 0.0628 - mean_squared_error: 0.0415 - val_loss: 0.0700 - val_mean_squared_error: 0.0487\n",
      "Epoch 70/100\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 0.0627 - mean_squared_error: 0.0414 - val_loss: 0.0691 - val_mean_squared_error: 0.0479\n",
      "Epoch 71/100\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 0.0625 - mean_squared_error: 0.0413 - val_loss: 0.0698 - val_mean_squared_error: 0.0486\n",
      "Epoch 72/100\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 0.0624 - mean_squared_error: 0.0412 - val_loss: 0.0687 - val_mean_squared_error: 0.0475\n",
      "Epoch 73/100\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 0.0623 - mean_squared_error: 0.0412 - val_loss: 0.0702 - val_mean_squared_error: 0.0490\n",
      "Epoch 74/100\n",
      "40000/40000 [==============================] - 1s 31us/step - loss: 0.0622 - mean_squared_error: 0.0411 - val_loss: 0.0690 - val_mean_squared_error: 0.0480\n",
      "Epoch 75/100\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 0.0621 - mean_squared_error: 0.0411 - val_loss: 0.0688 - val_mean_squared_error: 0.0478\n",
      "Epoch 76/100\n",
      "40000/40000 [==============================] - 1s 31us/step - loss: 0.0619 - mean_squared_error: 0.0409 - val_loss: 0.0688 - val_mean_squared_error: 0.0479\n",
      "Epoch 77/100\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 0.0617 - mean_squared_error: 0.0407 - val_loss: 0.0687 - val_mean_squared_error: 0.0478\n",
      "\n",
      "Epoch 00077: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 78/100\n",
      "40000/40000 [==============================] - 1s 31us/step - loss: 0.0602 - mean_squared_error: 0.0392 - val_loss: 0.0672 - val_mean_squared_error: 0.0463\n",
      "Epoch 79/100\n",
      "40000/40000 [==============================] - 1s 29us/step - loss: 0.0601 - mean_squared_error: 0.0392 - val_loss: 0.0676 - val_mean_squared_error: 0.0467\n",
      "Epoch 80/100\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 0.0600 - mean_squared_error: 0.0391 - val_loss: 0.0676 - val_mean_squared_error: 0.0467\n",
      "Epoch 81/100\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 0.0599 - mean_squared_error: 0.0391 - val_loss: 0.0672 - val_mean_squared_error: 0.0464\n",
      "Epoch 82/100\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 0.0599 - mean_squared_error: 0.0391 - val_loss: 0.0676 - val_mean_squared_error: 0.0468\n",
      "Epoch 83/100\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 0.0599 - mean_squared_error: 0.0391 - val_loss: 0.0674 - val_mean_squared_error: 0.0466\n",
      "\n",
      "Epoch 00083: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 84/100\n",
      "40000/40000 [==============================] - 1s 30us/step - loss: 0.0590 - mean_squared_error: 0.0382 - val_loss: 0.0667 - val_mean_squared_error: 0.0459\n",
      "Epoch 85/100\n",
      "40000/40000 [==============================] - 1s 32us/step - loss: 0.0589 - mean_squared_error: 0.0381 - val_loss: 0.0665 - val_mean_squared_error: 0.0457\n",
      "Epoch 86/100\n",
      "40000/40000 [==============================] - 1s 33us/step - loss: 0.0588 - mean_squared_error: 0.0381 - val_loss: 0.0663 - val_mean_squared_error: 0.0456\n",
      "Epoch 87/100\n",
      "40000/40000 [==============================] - 1s 32us/step - loss: 0.0588 - mean_squared_error: 0.0381 - val_loss: 0.0664 - val_mean_squared_error: 0.0457\n",
      "Epoch 88/100\n",
      "40000/40000 [==============================] - 1s 31us/step - loss: 0.0588 - mean_squared_error: 0.0380 - val_loss: 0.0666 - val_mean_squared_error: 0.0458\n",
      "Epoch 89/100\n",
      "40000/40000 [==============================] - 1s 32us/step - loss: 0.0587 - mean_squared_error: 0.0380 - val_loss: 0.0663 - val_mean_squared_error: 0.0456\n",
      "Epoch 90/100\n",
      "40000/40000 [==============================] - 1s 32us/step - loss: 0.0587 - mean_squared_error: 0.0380 - val_loss: 0.0663 - val_mean_squared_error: 0.0456\n",
      "Epoch 91/100\n",
      "40000/40000 [==============================] - 1s 32us/step - loss: 0.0587 - mean_squared_error: 0.0380 - val_loss: 0.0667 - val_mean_squared_error: 0.0460\n",
      "\n",
      "Epoch 00091: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 92/100\n",
      "40000/40000 [==============================] - 1s 32us/step - loss: 0.0582 - mean_squared_error: 0.0375 - val_loss: 0.0660 - val_mean_squared_error: 0.0453\n",
      "Epoch 93/100\n",
      "40000/40000 [==============================] - 1s 31us/step - loss: 0.0581 - mean_squared_error: 0.0375 - val_loss: 0.0659 - val_mean_squared_error: 0.0452\n",
      "Epoch 94/100\n",
      "40000/40000 [==============================] - 2s 39us/step - loss: 0.0581 - mean_squared_error: 0.0375 - val_loss: 0.0659 - val_mean_squared_error: 0.0452\n",
      "Epoch 95/100\n",
      "40000/40000 [==============================] - 2s 43us/step - loss: 0.0581 - mean_squared_error: 0.0375 - val_loss: 0.0660 - val_mean_squared_error: 0.0453\n",
      "Epoch 96/100\n",
      "40000/40000 [==============================] - 2s 38us/step - loss: 0.0581 - mean_squared_error: 0.0375 - val_loss: 0.0658 - val_mean_squared_error: 0.0452\n",
      "Epoch 97/100\n",
      "40000/40000 [==============================] - 1s 35us/step - loss: 0.0581 - mean_squared_error: 0.0374 - val_loss: 0.0659 - val_mean_squared_error: 0.0453\n",
      "Epoch 98/100\n",
      "40000/40000 [==============================] - 1s 34us/step - loss: 0.0581 - mean_squared_error: 0.0374 - val_loss: 0.0659 - val_mean_squared_error: 0.0453\n",
      "\n",
      "Epoch 00098: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 99/100\n",
      "40000/40000 [==============================] - 1s 34us/step - loss: 0.0578 - mean_squared_error: 0.0372 - val_loss: 0.0658 - val_mean_squared_error: 0.0452\n",
      "Epoch 100/100\n",
      "40000/40000 [==============================] - 1s 34us/step - loss: 0.0578 - mean_squared_error: 0.0372 - val_loss: 0.0659 - val_mean_squared_error: 0.0452\n",
      "models/model_RoboschoolAnt-v1_training_set_size_40000_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm/base.hdf5\n",
      "Done training model for RoboschoolAnt-v1\n",
      "{'model_name': 'model_RoboschoolAnt-v1_training_set_size_40000_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm', 'model_path': 'models/model_RoboschoolAnt-v1_training_set_size_40000_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm', 'test_mse': 0.045233902375793056, 'train_mse': 0.03819933666391306, 'dataset_name': 'RoboschoolAnt-v1'}\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.callbacks import Callback, LambdaCallback, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "from manage_datasets import get_datasets\n",
    "from keras_helpers.model_helper import create_model, get_model_name, calc_mse\n",
    "from keras_helpers.keras_train_stats import KerasTrainStats\n",
    "\n",
    "\n",
    "ANT_SAVED_MODELS_DIR = 'ant_models'\n",
    "MODEL_FILE_NAME = \"base.hdf5\"\n",
    "create_dir_if_not_exists(SAVED_MODELS_DIR)\n",
    "\n",
    "model_mse = []\n",
    "envname = 'RoboschoolAnt-v1'\n",
    "# Load the datasets once\n",
    "X_train, X_test, y_train, y_test = get_datasets(dataset_name=envname, dataset_dir=EXPERT_DATA_DIR)\n",
    "\n",
    "for train_size in range(10000, 50000, 10000):  # [10000, 20000, 30000, 40000]\n",
    "    print(\"Training a FC ANN for env %s over a training set size %d \" % (envname, train_size))\n",
    "    \n",
    "    # Define the model params   \n",
    "    config_dict = dict(\n",
    "        input_dim=len(X_train[1, :]),\n",
    "        output_dim=len(y_train[1, :]),\n",
    "        units=100,\n",
    "        layers = 3,\n",
    "        l2_reg = 1e-04,\n",
    "        optimizer_cls=keras.optimizers.Adam,\n",
    "        lr = 1e-03,\n",
    "        dropout=None,\n",
    "        use_batchnorm=False)\n",
    "    \n",
    "    # Create the model\n",
    "    model = create_model(**config_dict)\n",
    "    \n",
    "    # Compile the model\n",
    "    optimizer = config_dict['optimizer_cls'](lr=config_dict['lr'])\n",
    "    model.compile(loss='mse', optimizer=optimizer, metrics=['mse'])\n",
    "    \n",
    "    # Set a unique name/directory for the model\n",
    "    base_name = \"%s_training_set_size_%d\" % (envname, train_size)\n",
    "    model_name = get_model_name(base_name=base_name, **config_dict)\n",
    "    model_path = os.path.join(SAVED_MODELS_DIR, model_name)\n",
    "    print(\"model_name='%s'\" % model_name)\n",
    "    print(\"model_path='%s'\" % model_path)\n",
    "    create_dir_if_not_exists(model_path)\n",
    "    model_filename = os.path.join(model_path, MODEL_FILE_NAME)\n",
    "    \n",
    "    \n",
    "    # Train the model\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_mean_squared_error', factor=0.5, patience=5, verbose=1)\n",
    "    tf_board = TensorBoard()\n",
    "\n",
    "    # Define a train_stats object\n",
    "    train_stats = KerasTrainStats(model_name=model_name, history_dir=model_path)\n",
    "\n",
    "\n",
    "    # Train the model\n",
    "    batch_size = 64\n",
    "    epochs = 100\n",
    "\n",
    "    _ = model.fit([X_train[0:train_size]], y_train[0:train_size],\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        callbacks=[train_stats.print_callback, reduce_lr, tf_board],\n",
    "        validation_data=([X_test], y_test)\n",
    "        )\n",
    "    \n",
    "    # Save the model\n",
    "    model.save(model_filename)\n",
    "    print(model_filename)\n",
    "    \n",
    "    # Calculate the MSE for each \n",
    "    res = calc_mse(model, envname, X_train, X_test, y_train, y_test)\n",
    "    res['model_name'] = model_name\n",
    "    res['model_path'] = model_path\n",
    "    model_mse.append(res)\n",
    "    print(\"Done training model for %s\" % envname)\n",
    "    #print(\"KerasTrainStats(model_name=model_name, history_dir=model_path).plt_history(start_epoch=5, metric_str='mean_squared_error', title='%s MSE')\" % dataset_name)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get the test MSE error for each of the models\n",
    "roboschool_ant_df = pd.DataFrame(model_mse)\n",
    "test_mse = roboschool_ant_df['test_mse'].tolist()\n",
    "\n",
    "# Add the test MSE error we previously got for the training set size of 45K\n",
    "test_mse_45k = train_df_stats[train_df_stats['dataset_name']=='RoboschoolAnt-v1'].test_mse.iloc[0]\n",
    "#test_mse_45k = 0.046053225442220666\n",
    "test_mse.append(test_mse_45k)\n",
    "\n",
    "# Scatter plot\n",
    "plt.figure(figsize=(20,10))\n",
    "scatter(x=[1e+4, 2e+4, 3e+4, 4e+4, 45000], y=test_mse)\n",
    "_ = plt.title(\"Test MSE vs training set size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a significant improvement in terms of MSE when increasing the dataset size upto 40K. On the contrary, there seems to be a very subtle performance improvement between 40K and 45K (0.0452 MSE vs 0.045 respectively). We will also calculate the 40K model mean return vs the expert return. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the trajectories using the 40K model and compare the results\n",
    "This is done in order to remove the advantage in number of samples the RoboschoolAnt-v1 model had over thee RoboschoolHumanoid-v1 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env description: Supervised model policy for module RoboschoolAnt-v1 with a smaller training set of 40K examples\n",
      "mean return 1819.7226450017135\n",
      "std of return 359.9117185487721\n",
      "CPU times: user 2min 43s, sys: 1min 16s, total: 3min 59s\n",
      "Wall time: 3min 53s\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from run_expert import run_policy\n",
    "\n",
    "envname = 'RoboschoolAnt-v1'\n",
    "ant_40k_model_filename = 'models/model_RoboschoolAnt-v1_training_set_size_40000_layers_100_3_neurons_l2_0.0001_Adam_optimizer_0.001_lr_None_dropout_without_batchnorm/base.hdf5'\n",
    "policy = SupervisedModelPolicy(envname, model_filename=ant_40k_model_filename)\n",
    "env = gym.make(envname)\n",
    "description = \"Supervised model policy for module %s with a smaller training set of 40K examples\" % envname\n",
    "%time supervised_modeled_data = run_policy(env=env, policy=policy, num_rollouts=50, \\\n",
    "                                           description=description, verbose=False)\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1819.7226450017135, 359.9117185487721)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The calculated returns for the 40K model\n",
    "returns = supervised_modeled_data['returns']\n",
    "returns.mean(), returns.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1818.8292998516504, 381.8678179633842)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The calculated returns for the 45K model\n",
    "t = stats_df[stats_df['envname'] == 'RoboschoolAnt-v1'][['expert_mean', 'expert_std']].values[0]\n",
    "prev_mean, prev_std = tuple(t)\n",
    "prev_mean, prev_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no difference in the returns between the 40K and 45K datasets models. This shows that as suspected, the\n",
    "RoboschoolAnt-v1 model advantage over the RoboschoolHumanoid-v1 model was not caused by the slight different in trainset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
